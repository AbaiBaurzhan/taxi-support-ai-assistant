#!/usr/bin/env python3
"""
‚ö° –°–£–ü–ï–†-–ë–´–°–¢–†–ê–Ø LLM –ú–û–î–ï–õ–¨ APARU
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –±—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤
"""

import requests
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UltraFastLLMClient:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.model_name = "aparu-senior-ai"
        self.ollama_available = False
        self._check_ollama()
        
        # –£–õ–¨–¢–†–ê-–ë–´–°–¢–†–´–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
        self.ultra_fast_params = {
            "temperature": 0.05,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
            "num_predict": 50,    # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã
            "num_ctx": 128,       # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            "repeat_penalty": 1.0,
            "top_k": 5,           # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä
            "top_p": 0.7,         # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            "stop": ["\n", ".", "!", "?", "–û—Ç–≤–µ—Ç:"]  # –†–∞–Ω–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        }
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        self.minimal_prompt = """APARU —Ç–∞–∫—Å–∏. –û—Ç–≤–µ—á–∞–π –ö–†–ê–¢–ö–û:
- –ù–∞—Ü–µ–Ω–∫–∞ = –¥–æ–ø–ª–∞—Ç–∞ –∑–∞ —Å–ø—Ä–æ—Å
- –î–æ—Å—Ç–∞–≤–∫–∞ = –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ‚Üí –î–æ—Å—Ç–∞–≤–∫–∞ ‚Üí –∞–¥—Ä–µ—Å
- –ë–∞–ª–∞–Ω—Å = –ü—Ä–æ—Ñ–∏–ª—å ‚Üí –ü–æ–ø–æ–ª–Ω–∏—Ç—å
- –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ = –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫ + –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ

–ú–∞–∫—Å–∏–º—É–º 1 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å = "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É" """

    def _check_ollama(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=3)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if any(m["name"].startswith(self.model_name) for m in models):
                    self.ollama_available = True
                    logger.info(f"‚úÖ Ollama –∏ –º–æ–¥–µ–ª—å '{self.model_name}' –¥–æ—Å—Ç—É–ø–Ω—ã")
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            else:
                logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")

    def get_ultra_fast_llm_response(self, question: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM –º–æ–¥–µ–ª–∏"""
        if not self.ollama_available:
            logger.error("‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None

        try:
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
            prompt = f"{self.minimal_prompt}\n\n{question}:"
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": self.ultra_fast_params
            }
            
            logger.info(f"‚ö° –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM...")
            start_time = datetime.now()
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=15  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', '').strip()
                
                if answer:
                    logger.info(f"‚ö° LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {processing_time:.2f}—Å")
                    return {
                        "answer": answer,
                        "category": "llm_generated",
                        "confidence": 0.95,
                        "source": "ultra_fast_llm",
                        "processing_time": processing_time
                    }
            
            logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {response.status_code}")
            return None
            
        except requests.exceptions.Timeout:
            logger.error(f"‚ùå LLM —Ç–∞–π–º–∞—É—Ç (>15—Å)")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
            return None

    def get_hybrid_response(self, question: str) -> Dict[str, Any]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç: –£–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π LLM + fallback"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—ã–π LLM –æ—Ç–≤–µ—Ç
        llm_result = self.get_ultra_fast_llm_response(question)
        
        if llm_result:
            return llm_result
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
        logger.info("üîÑ Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É...")
        return self._simple_search(question)
    
    def _simple_search(self, question: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (fallback)"""
        question_lower = question.lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
        simple_kb = {
            "–Ω–∞—Ü–µ–Ω–∫–∞": "–ù–∞—Ü–µ–Ω–∫–∞ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç–∞ –∑–∞ –≤—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å. –ü–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏–≤–ª–µ—á—å –±–æ–ª—å—à–µ –≤–æ–¥–∏—Ç–µ–ª–µ–π.",
            "–¥–æ—Å—Ç–∞–≤–∫–∞": "–î–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏: –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ‚Üí –î–æ—Å—Ç–∞–≤–∫–∞ ‚Üí —É–∫–∞–∂–∏—Ç–µ –∞–¥—Ä–µ—Å–∞ ‚Üí –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –∑–∞–∫–∞–∑.",
            "–±–∞–ª–∞–Ω—Å": "–ü–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å: –ü—Ä–æ—Ñ–∏–ª—å ‚Üí –ü–æ–ø–æ–ª–Ω–∏—Ç—å ‚Üí –≤—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã.",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç? –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –æ–±–Ω–æ–≤–∏—Ç–µ –≤–µ—Ä—Å–∏—é."
        }
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword, answer in simple_kb.items():
            if keyword in question_lower:
                return {
                    "answer": answer,
                    "category": keyword,
                    "confidence": 0.8,
                    "source": "simple_search",
                    "processing_time": 0.1
                }
        
        # Fallback
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
            "category": "unknown",
            "confidence": 0.0,
            "source": "fallback",
            "processing_time": 0.1
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
ultra_fast_llm_client = UltraFastLLMClient()

def test_ultra_fast_llm():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–ª—å—Ç—Ä–∞-–±—ã—Å—Ç—Ä—É—é LLM –º–æ–¥–µ–ª—å"""
    print("‚ö° –¢–ï–°–¢–ò–†–£–Æ –£–õ–¨–¢–†–ê-–ë–´–°–¢–†–£–Æ LLM –ú–û–î–ï–õ–¨")
    print("=" * 60)
    
    test_questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞?",
        "–ö–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É?",
        "–ö–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å?",
        "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}: {question}")
        
        result = ultra_fast_llm_client.get_hybrid_response(question)
        
        print(f"‚úÖ –û—Ç–≤–µ—Ç: {result['answer']}")
        print(f"üìä –ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['processing_time']:.2f}—Å")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")

if __name__ == "__main__":
    test_ultra_fast_llm()
