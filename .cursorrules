Ты — Senior Python/JS разработчик.
Нужно сделать демо-проект ИИ-ассистента службы поддержки такси в виде Telegram Mini App (WebApp), который будет развёрнут через Railway и храниться в GitHub.
Функционал демо:
Пользователь пишет в чат (бот или мини-приложение).
Ассистент отвечает по простым сценариям:
«Жду такси, где водитель?» → возвращает статус поездки (моки).
«Пришлите чек» → отправляет чек (моки).
«Какие у меня карты / сделай карту основной» → работа с картами (моки).
«С меня списали дважды» → эскалация к оператору (создаётся тикет).
«Как считается цена / где вводить промокод» → отвечает из FAQ.
Использовать OpenAI Chat Completions + function calling.
Все данные — моковые (fixtures.json).
Интерфейс чата в мини-приложении: поле ввода, сообщения, быстрые кнопки (CTA).
Требования по стеку:
Backend: Python + FastAPI (эндпоинт /chat, функции-моки, отдача статики).
Bot: Python + aiogram, выдаёт кнопку «Открыть поддержку» → открывает WebApp.
Frontend (WebApp): простой HTML/JS с подключением Telegram WebApp SDK.
Хостинг: Railway (деплой из GitHub).
когда пишешь код будь на 100% уверен что не ловаемшь не чего существующего
Сделай демо-проект на FastAPI, который работает с локальной моделью LLaMA. Нужен REST API сервер с эндпоинтом /chat.
Он принимает текст, user_id и locale, выполняет предобработку текста (убирает эмодзи и спецсимволы), определяет язык (RU/KZ/EN), классифицирует запрос (FAQ, статус поездки, чек, жалоба). Если это FAQ, то отвечает из базы знаний kb.json.
Если это не FAQ, то обращается к локальной LLaMA (через Ollama API или библиотеку transformers) и возвращает ответ.
Также нужно логировать intent, confidence и источник ответа (kb или llm).
Добавь минимальные моки для такси: get_ride_status, send_receipt, list_cards, escalate_to_human с данными из fixtures.json.
Структура проекта должна быть такой: backend/main.py, backend/llm_client.py, backend/kb.json, backend/fixtures.json, backend/requirements.txt и README.md.
Цель — показать работу с локальной LLaMA через REST API для текстовой поддержки такси.