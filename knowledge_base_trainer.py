#!/usr/bin/env python3
"""
üß† APARU Knowledge Base Training System
–û–±—É—á–µ–Ω–∏–µ LLM –Ω–∞ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏
"""

import json
import sqlite3
import pandas as pd
import numpy as np
from typing import List, Dict, Any
import logging
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import os

class TaxiKnowledgeBase:
    def __init__(self, db_path: str = None):
        self.db_path = db_path
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.index = None
        self.knowledge_base = []
        self.logger = logging.getLogger(__name__)
        
    def load_from_database(self, db_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {db_path}")
        
        try:
            # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
            conn = sqlite3.connect(db_path)
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã
            cursor = conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
            tables = cursor.fetchall()
            
            self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã
            for table_name in tables:
                table_name = table_name[0]
                self.logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–∞–±–ª–∏—Ü—É: {table_name}")
                
                # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–∞–±–ª–∏—Ü—ã
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = cursor.fetchall()
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                self._process_table_data(table_name, df, columns)
            
            conn.close()
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
            raise
    
    def load_from_csv(self, csv_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ CSV —Ñ–∞–π–ª–∞"""
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {csv_path}")
        
        try:
            df = pd.read_csv(csv_path)
            
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É: question, answer, category, etc.
            for _, row in df.iterrows():
                knowledge_item = {
                    'question': str(row.get('question', '')),
                    'answer': str(row.get('answer', '')),
                    'category': str(row.get('category', 'general')),
                    'keywords': str(row.get('keywords', '')).split(',') if row.get('keywords') else [],
                    'source': 'csv',
                    'id': len(self.knowledge_base)
                }
                self.knowledge_base.append(knowledge_item)
            
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π –∏–∑ CSV")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ CSV: {e}")
            raise
    
    def load_from_json(self, json_path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {json_path}")
        
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if isinstance(data, list):
                for item in data:
                    self.knowledge_base.append(self._normalize_knowledge_item(item))
            elif isinstance(data, dict):
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å –∫–ª—é—á–∞–º–∏
                for key, value in data.items():
                    if isinstance(value, list):
                        for item in value:
                            item['category'] = key
                            self.knowledge_base.append(self._normalize_knowledge_item(item))
            
            self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π –∏–∑ JSON")
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ JSON: {e}")
            raise
    
    def _process_table_data(self, table_name: str, df: pd.DataFrame, columns: List):
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
        question_col = None
        answer_col = None
        category_col = None
        
        for col in columns:
            col_name = col[1].lower()
            if 'question' in col_name or '–≤–æ–ø—Ä–æ—Å' in col_name:
                question_col = col[1]
            elif 'answer' in col_name or '–æ—Ç–≤–µ—Ç' in col_name or 'reply' in col_name:
                answer_col = col[1]
            elif 'category' in col_name or '–∫–∞—Ç–µ–≥–æ—Ä–∏—è' in col_name or 'type' in col_name:
                category_col = col[1]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
        for _, row in df.iterrows():
            knowledge_item = {
                'question': str(row.get(question_col, '')),
                'answer': str(row.get(answer_col, '')),
                'category': str(row.get(category_col, table_name)),
                'keywords': [],
                'source': f'db_{table_name}',
                'id': len(self.knowledge_base)
            }
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
            if knowledge_item['question']:
                keywords = self._extract_keywords(knowledge_item['question'])
                knowledge_item['keywords'] = keywords
            
            self.knowledge_base.append(knowledge_item)
    
    def _normalize_knowledge_item(self, item: Dict) -> Dict:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —ç–ª–µ–º–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        normalized = {
            'question': str(item.get('question', '')),
            'answer': str(item.get('answer', '')),
            'category': str(item.get('category', 'general')),
            'keywords': item.get('keywords', []),
            'source': str(item.get('source', 'json')),
            'id': len(self.knowledge_base)
        }
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if not normalized['keywords'] and normalized['question']:
            normalized['keywords'] = self._extract_keywords(normalized['question'])
        
        return normalized
    
    def _extract_keywords(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        import re
        
        # –£–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        words = text.split()
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–æ—Ç', '–¥–æ', '–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É'}
        keywords = [word for word in words if len(word) > 2 and word not in stop_words]
        
        return keywords[:10]  # –ú–∞–∫—Å–∏–º—É–º 10 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def build_embeddings_index(self):
        """–°—Ç—Ä–æ–∏—Ç –∏–Ω–¥–µ–∫—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        self.logger.info("–°—Ç—Ä–æ—é –∏–Ω–¥–µ–∫—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
        
        if not self.knowledge_base:
            raise ValueError("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        texts = []
        for item in self.knowledge_base:
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç –∏ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            text = f"{item['question']} {item['answer']} {' '.join(item['keywords'])}"
            texts.append(text)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self.logger.info("–ì–µ–Ω–µ—Ä–∏—Ä—É—é —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        embeddings = self.embeddings_model.encode(texts)
        
        # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        
        self.logger.info(f"–ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω: {self.index.ntotal} –≤–µ–∫—Ç–æ—Ä–æ–≤")
    
    def search_similar(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        if self.index is None:
            raise ValueError("–ò–Ω–¥–µ–∫—Å –Ω–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embeddings_model.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –≤–µ–∫—Ç–æ—Ä—ã
        scores, indices = self.index.search(query_embedding, top_k)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.knowledge_base):
                result = self.knowledge_base[idx].copy()
                result['similarity_score'] = float(score)
                results.append(result)
        
        return results
    
    def save_index(self, path: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        self.logger.info(f"–°–æ—Ö—Ä–∞–Ω—è—é –∏–Ω–¥–µ–∫—Å –≤ {path}")
        
        data = {
            'knowledge_base': self.knowledge_base,
            'index': self.index
        }
        
        with open(path, 'wb') as f:
            pickle.dump(data, f)
        
        self.logger.info("–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
    
    def load_index(self, path: str):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –∏ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –∏–Ω–¥–µ–∫—Å –∏–∑ {path}")
        
        with open(path, 'rb') as f:
            data = pickle.load(f)
        
        self.knowledge_base = data['knowledge_base']
        self.index = data['index']
        
        self.logger.info(f"–ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
    
    def generate_training_data(self, output_path: str):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LLM"""
        self.logger.info("–ì–µ–Ω–µ—Ä–∏—Ä—É—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        
        training_data = []
        
        for item in self.knowledge_base:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            prompt = f"""–¢—ã - –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏-—Å–µ—Ä–≤–∏—Å–∞.

–í–æ–ø—Ä–æ—Å: {item['question']}
–û—Ç–≤–µ—Ç: {item['answer']}

–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {item['category']}
–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(item['keywords'])}

–ü—Ä–∞–≤–∏–ª–∞ –æ—Ç–≤–µ—Ç–∞:
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
- –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º"""
            
            training_data.append({
                'prompt': prompt,
                'question': item['question'],
                'answer': item['answer'],
                'category': item['category'],
                'keywords': item['keywords']
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_path}")
        return training_data
    
    def get_statistics(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        if not self.knowledge_base:
            return {}
        
        categories = {}
        total_questions = len(self.knowledge_base)
        
        for item in self.knowledge_base:
            category = item['category']
            categories[category] = categories.get(category, 0) + 1
        
        return {
            'total_questions': total_questions,
            'categories': categories,
            'avg_keywords_per_item': np.mean([len(item['keywords']) for item in self.knowledge_base]),
            'sources': list(set([item['source'] for item in self.knowledge_base]))
        }

def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    logging.basicConfig(level=logging.INFO)
    
    # –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä
    kb = TaxiKnowledgeBase()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–≤—ã–±–µ—Ä–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ —Å–ø–æ—Å–æ–±–æ–≤)
    # kb.load_from_database('support_database.db')
    # kb.load_from_csv('support_data.csv')
    # kb.load_from_json('support_data.json')
    
    # –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å
    kb.build_embeddings_index()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    kb.save_index('taxi_knowledge_index.pkl')
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    kb.generate_training_data('training_data.json')
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = kb.get_statistics()
    print("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:")
    print(json.dumps(stats, indent=2, ensure_ascii=False))
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    query = "–ì–¥–µ –º–æ–π –≤–æ–¥–∏—Ç–µ–ª—å?"
    results = kb.search_similar(query, top_k=3)
    
    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}':")
    for result in results:
        print(f"Score: {result['similarity_score']:.3f}")
        print(f"Question: {result['question']}")
        print(f"Answer: {result['answer']}")
        print("-" * 50)

if __name__ == "__main__":
    main()
