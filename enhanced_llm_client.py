#!/usr/bin/env python3
"""
üß† APARU Enhanced LLM Client
–ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å LLM
"""

import json
import logging
import requests
import os
import pickle
from typing import Dict, Any, Optional, List
from knowledge_base_trainer import TaxiKnowledgeBase

logger = logging.getLogger(__name__)

class EnhancedLLMClient:
    def __init__(self, model_name: str = "llama2", use_ollama: bool = True):
        self.use_ollama = use_ollama
        self.model_name = model_name
        self.knowledge_base = None
        
        if use_ollama:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –≤–Ω–µ—à–Ω–µ–≥–æ LLM
            self.llm_url = os.getenv("LLM_URL", "http://localhost:11434")
            self.ollama_url = f"{self.llm_url}/api/generate"
            self.llm_enabled = os.getenv("LLM_ENABLED", "true").lower() == "true"
            
            if not self.llm_enabled:
                logger.warning("LLM –æ—Ç–∫–ª—é—á–µ–Ω —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è")
                self.use_ollama = False
        else:
            logger.warning("Heavy ML libraries not available, using fallback responses")
    
    def load_knowledge_base(self, index_path: str = "taxi_knowledge_index.pkl"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            self.knowledge_base = TaxiKnowledgeBase()
            self.knowledge_base.load_index(index_path)
            logger.info(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.knowledge_base.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return False
    
    def generate_response(self, prompt: str, max_length: int = 200) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            if self.use_ollama:
                return self._generate_with_knowledge_base(prompt, max_length)
            else:
                return self._generate_fallback(prompt)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
    
    def _generate_with_knowledge_base(self, prompt: str, max_length: int) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ï—Å–ª–∏ –µ—Å—Ç—å –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π, –∏—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if self.knowledge_base:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–æ–ø—Ä–æ—Å –∏–∑ –ø—Ä–æ–º–ø—Ç–∞
                user_question = self._extract_question_from_prompt(prompt)
                
                # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
                similar_items = self.knowledge_base.search_similar(user_question, top_k=3)
                
                if similar_items and similar_items[0]['similarity_score'] > 0.7:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
                    context = self._build_context_from_knowledge(similar_items)
                    enhanced_prompt = self._enhance_prompt_with_context(prompt, context)
                else:
                    enhanced_prompt = prompt
            else:
                enhanced_prompt = prompt
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM
            payload = {
                "model": self.model_name,
                "prompt": enhanced_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "max_tokens": max_length
                }
            }
            
            response = requests.post(self.ollama_url, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            return result.get("response", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏.")
            
        except requests.exceptions.RequestException as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ Ollama: {e}")
            return self._generate_fallback(prompt)
    
    def _extract_question_from_prompt(self, prompt: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –ø—Ä–æ–º–ø—Ç–∞"""
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫—É "–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:"
        if "–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:" in prompt:
            return prompt.split("–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:")[-1].strip()
        return prompt
    
    def _build_context_from_knowledge(self, similar_items: List[Dict]) -> str:
        """–°—Ç—Ä–æ–∏—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        context_parts = []
        
        for item in similar_items:
            context_parts.append(f"–í–æ–ø—Ä–æ—Å: {item['question']}")
            context_parts.append(f"–û—Ç–≤–µ—Ç: {item['answer']}")
            if item['keywords']:
                context_parts.append(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join(item['keywords'])}")
            context_parts.append("---")
        
        return "\n".join(context_parts)
    
    def _enhance_prompt_with_context(self, original_prompt: str, context: str) -> str:
        """–£–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        enhanced_prompt = f"""–¢—ã - –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏-—Å–µ—Ä–≤–∏—Å–∞.

–ò—Å–ø–æ–ª—å–∑—É–π —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞:

{context}

–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
- –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤—ã—à–µ
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º
- –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ

{original_prompt}

–û—Ç–≤–µ—Ç:"""
        
        return enhanced_prompt
    
    def _generate_fallback(self, prompt: str) -> str:
        """–ü—Ä–æ—Å—Ç–æ–π fallback –±–µ–∑ ML –±–∏–±–ª–∏–æ—Ç–µ–∫"""
        prompt_lower = prompt.lower()
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –¥–ª—è fallback
        if self.knowledge_base:
            user_question = self._extract_question_from_prompt(prompt)
            similar_items = self.knowledge_base.search_similar(user_question, top_k=1)
            
            if similar_items and similar_items[0]['similarity_score'] > 0.8:
                return similar_items[0]['answer']
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ fallback –æ—Ç–≤–µ—Ç—ã
        if "—Ü–µ–Ω–∞" in prompt_lower or "—Å—Ç–æ–∏–º–æ—Å—Ç—å" in prompt_lower:
            return "–¶–µ–Ω–∞ –ø–æ–µ–∑–¥–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –∏ –≤—Ä–µ–º–µ–Ω–∏. –ë–∞–∑–æ–≤—ã–π —Ç–∞—Ä–∏—Ñ 200 —Ç–µ–Ω–≥–µ + 50 —Ç–µ–Ω–≥–µ –∑–∞ –∫–º."
        elif "–ø—Ä–æ–º–æ–∫–æ–¥" in prompt_lower:
            return "–ü—Ä–æ–º–æ–∫–æ–¥ –º–æ–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ –ø—Ä–∏ –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–∏ –∑–∞–∫–∞–∑–∞ –≤ –ø–æ–ª–µ '–ü—Ä–æ–º–æ–∫–æ–¥'."
        elif "–æ—Ç–º–µ–Ω–∏—Ç—å" in prompt_lower:
            return "–ü–æ–µ–∑–¥–∫—É –º–æ–∂–Ω–æ –æ—Ç–º–µ–Ω–∏—Ç—å –≤ —Ç–µ—á–µ–Ω–∏–µ 2 –º–∏–Ω—É—Ç –ø–æ—Å–ª–µ –∑–∞–∫–∞–∑–∞ –±–µ–∑ —à—Ç—Ä–∞—Ñ–∞."
        elif "–≤–æ–¥–∏—Ç–µ–ª—å" in prompt_lower:
            return "–í—ã –º–æ–∂–µ—Ç–µ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –≤–æ–¥–∏—Ç–µ–ª–µ–º —á–µ—Ä–µ–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –∏–ª–∏ –ø–æ–∑–≤–æ–Ω–∏—Ç—å –ø–æ –Ω–æ–º–µ—Ä—É –≤ –¥–µ—Ç–∞–ª—è—Ö –ø–æ–µ–∑–¥–∫–∏."
        else:
            return "–°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—â–µ–Ω–∏–µ! –ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, —è –ø–æ–º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç."
    
    def create_taxi_context_prompt(self, user_message: str, intent: str, locale: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ç–∞–∫—Å–∏-—Å–µ—Ä–≤–∏—Å–∞ —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π"""
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π, –∏—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        context_info = ""
        if self.knowledge_base:
            similar_items = self.knowledge_base.search_similar(user_message, top_k=2)
            if similar_items and similar_items[0]['similarity_score'] > 0.6:
                context_info = f"\n\n–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:\n"
                for item in similar_items:
                    context_info += f"–í–æ–ø—Ä–æ—Å: {item['question']}\n"
                    context_info += f"–û—Ç–≤–µ—Ç: {item['answer']}\n\n"
        
        system_prompt = f"""–¢—ã - –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏-—Å–µ—Ä–≤–∏—Å–∞. 
–û—Ç–≤–µ—á–∞–π –Ω–∞ —è–∑—ã–∫–µ: {locale}
–¢–∏–ø –∑–∞–ø—Ä–æ—Å–∞: {intent}
{context_info}
–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É
- –ë—É–¥—å –≤–µ–∂–ª–∏–≤—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º
- –ò—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –≤—ã—à–µ
- –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, –ø—Ä–µ–¥–ª–æ–∂–∏ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º
- –î–ª—è –∂–∞–ª–æ–± –≤—Å–µ–≥–¥–∞ –ø—Ä–µ–¥–ª–∞–≥–∞–π —ç—Å–∫–∞–ª–∞—Ü–∏—é –∫ —á–µ–ª–æ–≤–µ–∫—É
- –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π, –∫–∞–∑–∞—Ö—Å–∫–∏–π –∏–ª–∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç locale

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_message}

–û—Ç–≤–µ—Ç:"""

        return system_prompt
    
    def get_knowledge_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        if self.knowledge_base:
            return self.knowledge_base.get_statistics()
        return {"error": "–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∏–µ–Ω—Ç–∞
enhanced_llm_client = EnhancedLLMClient()

def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ enhanced LLM client"""
    logging.basicConfig(level=logging.INFO)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    if enhanced_llm_client.load_knowledge_base():
        print("‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
        test_questions = [
            "–ì–¥–µ –º–æ–π –≤–æ–¥–∏—Ç–µ–ª—å?",
            "–ö–∞–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ü–µ–Ω–∞ –ø–æ–µ–∑–¥–∫–∏?",
            "–ú–æ–∂–Ω–æ –ª–∏ –æ—Ç–º–µ–Ω–∏—Ç—å –ø–æ–µ–∑–¥–∫—É?",
            "–ö–∞–∫ –≤–≤–µ—Å—Ç–∏ –ø—Ä–æ–º–æ–∫–æ–¥?"
        ]
        
        for question in test_questions:
            print(f"\nüìù –¢–µ—Å—Ç: {question}")
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç
            prompt = enhanced_llm_client.create_taxi_context_prompt(question, "faq", "RU")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = enhanced_llm_client.generate_response(prompt)
            print(f"‚úÖ –û—Ç–≤–µ—Ç: {response[:100]}...")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            stats = enhanced_llm_client.get_knowledge_stats()
            print(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π")

if __name__ == "__main__":
    main()
