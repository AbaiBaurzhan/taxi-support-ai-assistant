#!/usr/bin/env python3
"""
üéØ LLM –°–ò–°–¢–ï–ú–ê –í–´–ë–û–†–ê –ì–û–¢–û–í–´–• –û–¢–í–ï–¢–û–í
LLM —Ç–æ–ª—å–∫–æ –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã, –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π
"""

import json
import logging
import requests
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="APARU Answer Selection LLM", version="8.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–¥–µ–ª–∏
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str
    timestamp: str
    suggestions: List[str] = []

class HealthResponse(BaseModel):
    status: str
    architecture: str = "answer_selection_llm"
    timestamp: str
    llm_available: bool = False

class AnswerSelectionLLMClient:
    def __init__(self):
        self.ollama_url = os.environ.get("OLLAMA_URL", "http://localhost:11434")
        self.model_name = "aparu-senior-ai"
        self.ollama_available = False
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        self.knowledge_base = self._load_knowledge_base()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        self._check_ollama_model()
    
    def _load_knowledge_base(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            with open("BZ.txt", "r", encoding="utf-8") as f:
                data = json.load(f)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: BZ.txt ({len(data)} –æ—Ç–≤–µ—Ç–æ–≤)")
                return data
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _check_ollama_model(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –∏ –º–æ–¥–µ–ª–∏"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if any(m["name"].startswith(self.model_name) for m in models):
                    self.ollama_available = True
                    logger.info(f"‚úÖ Ollama –∏ –º–æ–¥–µ–ª—å '{self.model_name}' –¥–æ—Å—Ç—É–ø–Ω—ã")
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama")
            else:
                logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")
    
    def find_best_answer(self, question: str) -> Dict[str, Any]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è LLM –¥–ª—è –≤—ã–±–æ—Ä–∞"""
        start_time = datetime.now()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –≤—ã–±–æ—Ä–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        if self.ollama_available:
            try:
                logger.info("üéØ –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –¥–ª—è –≤—ã–±–æ—Ä–∞ –≥–æ—Ç–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞...")
                result = self._llm_answer_selection(question)
                if result:
                    processing_time = (datetime.now() - start_time).total_seconds()
                    logger.info(f"‚úÖ LLM –≤—ã–±–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time:.2f}—Å")
                    return result
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –≤—ã–±–æ—Ä–∞: {e}")
        
        # Fallback –∫ –ø–æ–∏—Å–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        logger.info("üîÑ Fallback –∫ –ø–æ–∏—Å–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")
        return self._keyword_search(question)
    
    def _llm_answer_selection(self, question: str) -> Dict[str, Any]:
        """LLM –≤—ã–±–∏—Ä–∞–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ—Ç–≤–µ—Ç–∞
            selection_prompt = self._create_selection_prompt(question)
            
            payload = {
                "model": self.model_name,
                "prompt": selection_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.0,   # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    "num_predict": 3,      # –¢–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä –æ—Ç–≤–µ—Ç–∞
                    "num_ctx": 512,       # –î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    "repeat_penalty": 1.0,
                    "top_k": 1,           # –¢–æ–ª—å–∫–æ –ª—É—á—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç
                    "top_p": 0.1,         # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                    "stop": ["\n", ".", "!", "?", "–û—Ç–≤–µ—Ç:", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:"]  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
                }
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=10  # –ö–æ—Ä–æ—Ç–∫–∏–π —Ç–∞–π–º–∞—É—Ç
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', '').strip()
                
                # –ü–∞—Ä—Å–∏–º –Ω–æ–º–µ—Ä –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                selected_num = self._parse_selected_answer(answer)
                if selected_num:
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≥–æ—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã
                    if 1 <= selected_num <= len(self.knowledge_base):
                        kb_item = self.knowledge_base[selected_num - 1]
                        return {
                            "answer": kb_item.get("answer", "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"),
                            "category": f"–û—Ç–≤–µ—Ç {selected_num}",
                            "confidence": 0.95,
                            "source": "llm_answer_selection"
                        }
                
                logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: '{answer}'")
                return None
            
            logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.status_code}")
            return None
            
        except requests.exceptions.Timeout:
            logger.error(f"‚ùå LLM –≤—ã–±–æ—Ä —Ç–∞–π–º–∞—É—Ç (>10—Å)")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –≤—ã–±–æ—Ä–∞: {e}")
            return None
    
    def _create_selection_prompt(self, question: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        prompt = f"""–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{question}"

–î–æ—Å—Ç—É–ø–Ω—ã–µ –≥–æ—Ç–æ–≤—ã–µ –æ—Ç–≤–µ—Ç—ã:

"""
        
        for i, item in enumerate(self.knowledge_base, 1):
            variations = item.get("question_variations", [])
            keywords = item.get("keywords", [])
            answer_preview = item.get("answer", "")[:100] + "..."
            
            prompt += f"{i}. –í–ê–†–ò–ê–¶–ò–ò –í–û–ü–†–û–°–û–í:\n"
            for variation in variations[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –≤–∞—Ä–∏–∞—Ü–∏–∏
                prompt += f"   - {variation}\n"
            
            prompt += f"   –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {', '.join(keywords[:5])}\n"
            prompt += f"   –û–¢–í–ï–¢: {answer_preview}\n\n"
        
        prompt += f"""–ó–∞–¥–∞—á–∞: –í—ã–±–µ—Ä–∏ –Ω–æ–º–µ—Ä –æ—Ç–≤–µ—Ç–∞ (1-{len(self.knowledge_base)}), –∫–æ—Ç–æ—Ä—ã–π –ª—É—á—à–µ –≤—Å–µ–≥–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: "{question}"

–£—á–∏—Ç—ã–≤–∞–π:
- –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
- –ü–æ—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
- –°–º—ã—Å–ª–æ–≤—É—é –±–ª–∏–∑–æ—Å—Ç—å

–ù–æ–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:"""

        return prompt
    
    def _parse_selected_answer(self, answer: str) -> Optional[int]:
        """–ü–∞—Ä—Å–∏—Ç –Ω–æ–º–µ—Ä –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
        try:
            # –ò—â–µ–º —á–∏—Å–ª–æ –≤ –æ—Ç–≤–µ—Ç–µ
            import re
            numbers = re.findall(r'\d+', answer)
            if numbers:
                num = int(numbers[0])
                if 1 <= num <= len(self.knowledge_base):
                    return num
            return None
        except:
            return None
    
    def _keyword_search(self, question: str) -> Dict[str, Any]:
        """Fallback –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        question_lower = question.lower()
        
        best_match = None
        best_score = 0
        
        for item in self.knowledge_base:
            keywords = item.get("keywords", [])
            variations = item.get("question_variations", [])
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_matches = 0
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    keyword_matches += 1
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π
            variation_matches = 0
            for variation in variations:
                if variation.lower() in question_lower:
                    variation_matches += 1
            
            # –û–±—â–∏–π —Å—á–µ—Ç
            total_score = keyword_matches + variation_matches
            
            if total_score > best_score:
                best_score = total_score
                best_match = item
        
        if best_match and best_score > 0:
            return {
                "answer": best_match.get("answer", "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"),
                "category": "keyword_match",
                "confidence": min(0.9, best_score * 0.2),
                "source": "keyword_search"
            }
        
        # Fallback
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
            "category": "unknown",
            "confidence": 0.0,
            "source": "fallback"
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
answer_selection_client = AnswerSelectionLLMClient()

@app.get("/")
async def root():
    return {
        "message": "APARU Answer Selection LLM", 
        "status": "running", 
        "version": "8.0.0",
        "architecture": "answer_selection_llm",
        "llm_available": answer_selection_client.ollama_available
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(
        status="healthy",
        architecture="answer_selection_llm",
        timestamp=datetime.now().isoformat(),
        llm_available=answer_selection_client.ollama_available
    )

@app.get("/webapp", response_class=HTMLResponse)
async def webapp():
    """Telegram WebApp –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    try:
        with open("webapp.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>WebApp –Ω–µ –Ω–∞–π–¥–µ–Ω</h1><p>–§–∞–π–ª webapp.html –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç</p>",
            status_code=404
        )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞"""
    try:
        result = answer_selection_client.find_best_answer(request.text)
        
        return ChatResponse(
            response=result["answer"],
            intent=result["category"],
            confidence=result["confidence"],
            source=result["source"],
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ /chat: {e}")
        return ChatResponse(
            response="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.",
            intent="error",
            confidence=0.0,
            source="error",
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )

if __name__ == "__main__":
    import uvicorn
    
    # Railway –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é PORT
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
