"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ —è–∑—ã–∫–æ–≤
–°–ø–µ—Ü–∏–∞–ª—å–Ω–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π BZ.txt
"""

import re
import json
import logging
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

logger = logging.getLogger(__name__)

class EnhancedMorphologicalAnalyzer:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è BZ.txt"""
    
    def __init__(self):
        self.rules = self._load_morphological_rules()
        self.stop_words = self._load_stop_words()
        self.synonyms = self._load_synonyms()
        self.patterns = self._load_patterns()
        
    def _load_morphological_rules(self) -> Dict[str, Dict[str, List[str]]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ –∏ –∫–∞–∑–∞—Ö—Å–∫–æ–≥–æ"""
        return {
            'ru': {
                'endings': {
                    '–∏—è': ['–∏–∏', '–∏—é', '–∏–µ', '–∏–π'],
                    '—Ü–∏—è': ['—Ü–∏–∏', '—Ü–∏—é', '—Ü–∏–π'],
                    '–∞—Ü–∏—è': ['–∞—Ü–∏–∏', '–∞—Ü–∏—é', '–∞—Ü–∏–π'],
                    '–æ—Å—Ç—å': ['–æ—Å—Ç–∏', '–æ—Å—Ç—å—é'],
                    '–µ–Ω–∏–µ': ['–µ–Ω–∏—è', '–µ–Ω–∏—é', '–µ–Ω–∏–µ–º'],
                    '–∞–Ω–∏–µ': ['–∞–Ω–∏—è', '–∞–Ω–∏—é', '–∞–Ω–∏–µ–º'],
                    '–µ–Ω–∏–µ': ['–µ–Ω–∏—è', '–µ–Ω–∏—é', '–µ–Ω–∏–µ–º'],
                    '–æ—Å—Ç—å': ['–æ—Å—Ç–∏', '–æ—Å—Ç—å—é'],
                    '–∫–∞': ['–∫–∏', '–∫—É', '–∫–æ–π', '–∫–µ'],
                    '–∏–∫': ['–∏–∫–∞', '–∏–∫—É', '–∏–∫–æ–º', '–∏–∫–µ'],
                    '–Ω–∏–∫': ['–Ω–∏–∫–∞', '–Ω–∏–∫—É', '–Ω–∏–∫–æ–º', '–Ω–∏–∫–µ'],
                    '—Ç–µ–ª—å': ['—Ç–µ–ª—è', '—Ç–µ–ª—é', '—Ç–µ–ª–µ–º', '—Ç–µ–ª–µ'],
                    '–æ—Å—Ç—å': ['–æ—Å—Ç–∏', '–æ—Å—Ç—å—é'],
                    '—Å—Ç–≤–æ': ['—Å—Ç–≤–∞', '—Å—Ç–≤—É', '—Å—Ç–≤–æ–º', '—Å—Ç–≤–µ'],
                    '–µ–Ω–∏–µ': ['–µ–Ω–∏—è', '–µ–Ω–∏—é', '–µ–Ω–∏–µ–º'],
                    '–∞–Ω–∏–µ': ['–∞–Ω–∏—è', '–∞–Ω–∏—é', '–∞–Ω–∏–µ–º'],
                    '–∏–º–æ—Å—Ç—å': ['–∏–º–æ—Å—Ç–∏', '–∏–º–æ—Å—Ç—å—é'],
                    '–Ω–æ—Å—Ç—å': ['–Ω–æ—Å—Ç–∏', '–Ω–æ—Å—Ç—å—é'],
                    '–µ–ª—å': ['–µ–ª—è', '–µ–ª—é', '–µ–ª–µ–º', '–µ–ª–µ'],
                    '–∞—Ä—å': ['–∞—Ä—è', '–∞—Ä—é', '–∞—Ä–µ–º', '–∞—Ä–µ'],
                    '–µ—Ä': ['–µ—Ä–∞', '–µ—Ä—É', '–µ—Ä–æ–º', '–µ—Ä–µ'],
                    '–æ—Ä': ['–æ—Ä–∞', '–æ—Ä—É', '–æ—Ä–æ–º', '–æ—Ä–µ'],
                    '–µ—Ü': ['—Ü–∞', '—Ü—É', '—Ü–æ–º', '—Ü–µ'],
                    '–∏–∫': ['–∏–∫–∞', '–∏–∫—É', '–∏–∫–æ–º', '–∏–∫–µ'],
                    '–æ–∫': ['–∫–∞', '–∫—É', '–∫–æ–º', '–∫–µ'],
                    '–µ–∫': ['–∫–∞', '–∫—É', '–∫–æ–º', '–∫–µ'],
                    '–∏—Ü–∞': ['–∏—Ü—ã', '–∏—Ü–µ', '–∏—Ü–µ–π'],
                    '–∏—Ü–∞': ['–∏—Ü—ã', '–∏—Ü–µ', '–∏—Ü–µ–π'],
                    '–∞': ['—ã', '–µ', '–æ–π', '—É'],
                    '—è': ['–∏', '–µ', '–µ–π', '—é'],
                    '—å': ['–∏', '–µ', '—å—é', '—é'],
                    '–π': ['—è', '—é', '–µ–º', '–µ'],
                    '–æ': ['–∞', '—É', '–æ–º', '–µ'],
                    '–µ': ['—è', '—é', '–µ–º', '–µ'],
                    '—ã': ['', '–µ', '–æ–π', '—É'],
                    '–∏': ['–µ–π', '—è–º', '—è–º–∏', '—è—Ö'],
                    '—É': ['–∞', '—É', '–æ–º', '–µ'],
                    '–∞': ['—ã', '–µ', '–æ–π', '—É']
                },
                'suffixes': {
                    '—Ç–µ–ª—å': ['—Ç–µ–ª—å', '—Ç–µ–ª—è', '—Ç–µ–ª—é', '—Ç–µ–ª–µ–º'],
                    '–Ω–∏–∫': ['–Ω–∏–∫', '–Ω–∏–∫–∞', '–Ω–∏–∫—É', '–Ω–∏–∫–æ–º'],
                    '–∏–∫': ['–∏–∫', '–∏–∫–∞', '–∏–∫—É', '–∏–∫–æ–º'],
                    '–æ–∫': ['–æ–∫', '–∫–∞', '–∫—É', '–∫–æ–º'],
                    '–µ–∫': ['–µ–∫', '–∫–∞', '–∫—É', '–∫–æ–º'],
                    '–∏—Ü–∞': ['–∏—Ü–∞', '–∏—Ü—ã', '–∏—Ü–µ', '–∏—Ü–µ–π'],
                    '–æ—Å—Ç—å': ['–æ—Å—Ç—å', '–æ—Å—Ç–∏', '–æ—Å—Ç—å—é'],
                    '–Ω–æ—Å—Ç—å': ['–Ω–æ—Å—Ç—å', '–Ω–æ—Å—Ç–∏', '–Ω–æ—Å—Ç—å—é'],
                    '–∏–º–æ—Å—Ç—å': ['–∏–º–æ—Å—Ç—å', '–∏–º–æ—Å—Ç–∏', '–∏–º–æ—Å—Ç—å—é'],
                    '—Å—Ç–≤–æ': ['—Å—Ç–≤–æ', '—Å—Ç–≤–∞', '—Å—Ç–≤—É', '—Å—Ç–≤–æ–º'],
                    '–µ–Ω–∏–µ': ['–µ–Ω–∏–µ', '–µ–Ω–∏—è', '–µ–Ω–∏—é', '–µ–Ω–∏–µ–º'],
                    '–∞–Ω–∏–µ': ['–∞–Ω–∏–µ', '–∞–Ω–∏—è', '–∞–Ω–∏—é', '–∞–Ω–∏–µ–º']
                }
            },
            'kz': {
                'endings': {
                    '–ª–∞—Ä': ['–ª–∞—Ä—ã', '–ª–∞—Ä—ã–Ω', '–ª–∞—Ä—ã–Ω–∞', '–ª–∞—Ä—ã–Ω–¥–∞'],
                    '–ª–µ—Ä': ['–ª–µ—Ä—ñ', '–ª–µ—Ä—ñ–Ω', '–ª–µ—Ä—ñ–Ω–µ', '–ª–µ—Ä—ñ–Ω–¥–µ'],
                    '–¥–∞—Ä': ['–¥–∞—Ä—ã', '–¥–∞—Ä—ã–Ω', '–¥–∞—Ä—ã–Ω–∞', '–¥–∞—Ä—ã–Ω–¥–∞'],
                    '–¥–µ—Ä': ['–¥–µ—Ä—ñ', '–¥–µ—Ä—ñ–Ω', '–¥–µ—Ä—ñ–Ω–µ', '–¥–µ—Ä—ñ–Ω–¥–µ'],
                    '—Ç–∞—Ä': ['—Ç–∞—Ä—ã', '—Ç–∞—Ä—ã–Ω', '—Ç–∞—Ä—ã–Ω–∞', '—Ç–∞—Ä—ã–Ω–¥–∞'],
                    '—Ç–µ—Ä': ['—Ç–µ—Ä—ñ', '—Ç–µ—Ä—ñ–Ω', '—Ç–µ—Ä—ñ–Ω–µ', '—Ç–µ—Ä—ñ–Ω–¥–µ'],
                    '–∞': ['–∞–Ω—ã', '–∞“ì–∞', '–∞–¥–∞', '–∞–¥–∞–Ω'],
                    '–µ': ['–µ—Å—ñ', '–µ–≥–µ', '–µ–¥–µ', '–µ–¥–µ–Ω'],
                    '—ã': ['—ã–Ω—ã', '—ã“ì–∞', '—ã–¥–∞', '—ã–¥–∞–Ω'],
                    '—ñ': ['—ñ–Ω—ñ', '—ñ–≥–µ', '—ñ–¥–µ', '—ñ–¥–µ–Ω'],
                    '–æ': ['–æ–Ω—ã', '–æ“ì–∞', '–æ–¥–∞', '–æ–¥–∞–Ω'],
                    '”©': ['”©–Ω—ñ', '”©–≥–µ', '”©–¥–µ', '”©–¥–µ–Ω'],
                    '“±': ['“±–Ω—ã', '“±“ì–∞', '“±–¥–∞', '“±–¥–∞–Ω'],
                    '“Ø': ['“Ø–Ω—ñ', '“Ø–≥–µ', '“Ø–¥–µ', '“Ø–¥–µ–Ω']
                }
            }
        }
    
    def _load_stop_words(self) -> Dict[str, set]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏"""
        return {
            'ru': {
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
            },
            'kz': {
                '–∂”ô–Ω–µ', '–º–µ–Ω', '–±–µ–Ω', '–ø–µ–Ω', '“Ø—à—ñ–Ω', '—Ç—É—Ä–∞–ª—ã', '“õ–∞—Ä–∞“ì–∞–Ω–¥–∞', '—Å–∏—è“õ—Ç—ã', '–¥–µ–ø', '–¥–µ–π—ñ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ', '–¥–µ–π—ñ–Ω–¥–µ', '–¥–µ–π—ñ–Ω–Ω–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ–Ω—ñ“£', '–¥–µ–π—ñ–Ω–≥—ñ–≥–µ', '–¥–µ–π—ñ–Ω–≥—ñ–¥–µ', '–¥–µ–π—ñ–Ω–≥—ñ–¥–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ–º–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–µ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–¥–µ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–º–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–µ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–¥–µ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–º–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–µ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–¥–µ', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–Ω–µ–Ω', '–¥–µ–π—ñ–Ω–≥—ñ—Å—ñ–º–µ–Ω'
            }
        }
    
    def _load_synonyms(self) -> Dict[str, Dict[str, List[str]]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞"""
        return {
            'ru': {
                '—Ç–∞—Ä–∏—Ñ': ['—Å—Ç–∞–≤–∫–∞', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä–∞—Å—Ü–µ–Ω–∫–∞'],
                '–∫–æ–º—Ñ–æ—Ä—Ç': ['—É–¥–æ–±—Å—Ç–≤–æ', '–ø—Ä–µ–º–∏—É–º', '–ª—é–∫—Å'],
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–ø–µ—Ä–µ–≤–æ–∑–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞', '–≥—Ä—É–∑', '–≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∞ —Ç–æ–≤–∞—Ä–æ–≤', '–¥–æ—Å—Ç–∞–≤–∫–∞ –≥—Ä—É–∑–∞'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': ['—à–æ—Ñ–µ—Ä', '—Ç–∞–∫—Å–∏—Å—Ç', '–ø–µ—Ä–µ–≤–æ–∑—á–∏–∫'],
                '–±–∞–ª–∞–Ω—Å': ['—Å—á–µ—Ç', '—Å—Ä–µ–¥—Å—Ç–≤–∞', '–¥–µ–Ω—å–≥–∏'],
                '–º–æ—Ç–æ—á–∞—Å—ã': ['–≤—Ä–µ–º—è', '–º–∏–Ω—É—Ç—ã', '–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å'],
                '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä': ['—Å—á–µ—Ç—á–∏–∫', '–∏–∑–º–µ—Ä–∏—Ç–µ–ª—å'],
                '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': ['–ø—Ä–æ–≥—Ä–∞–º–º–∞', '—Å–æ—Ñ—Ç', '–∞–ø–ø']
            },
            'kz': {
                '—Ç–∞—Ä–∏—Ñ': ['–±–∞“ì–∞', '“õ“±–Ω', '–∞“õ—ã'],
                '–∫–æ–º—Ñ–æ—Ä—Ç': ['—ã“£“ì–∞–π–ª—ã–ª—ã“õ', '–ø—Ä–µ–º–∏—É–º'],
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–∂–µ—Ç–∫—ñ–∑—É', '—Ç–∞—Å—ã–º–∞–ª–¥–∞—É'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': ['–∂“Ø—Ä–≥—ñ–∑—É—à—ñ', '—Ç–∞–∫—Å–∏—Å—Ç'],
                '–±–∞–ª–∞–Ω—Å': ['—Ç–µ“£–≥–µ—Ä—ñ–º', '–∞“õ—à–∞', '“õ–∞—Ä–∞–∂–∞—Ç'],
                '–º–æ—Ç–æ—á–∞—Å—ã': ['—É–∞“õ—ã—Ç', '–º–∏–Ω—É—Ç'],
                '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä': ['–µ—Å–µ–ø—Ç–µ–≥—ñ—à'],
                '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': ['“õ–æ—Å—ã–º—à–∞', '–±–∞“ì–¥–∞—Ä–ª–∞–º–∞']
            }
        }
    
    def _load_patterns(self) -> Dict[str, List[str]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        return {
            'question_patterns': [
                r'–∫–∞–∫\s+(.+?)\?',
                r'—á—Ç–æ\s+(.+?)\?',
                r'–≥–¥–µ\s+(.+?)\?',
                r'–ø–æ—á–µ–º—É\s+(.+?)\?',
                r'–∑–∞—á–µ–º\s+(.+?)\?',
                r'–∫–æ–≥–¥–∞\s+(.+?)\?',
                r'—Å–∫–æ–ª—å–∫–æ\s+(.+?)\?',
                r'–º–æ–∂–Ω–æ\s+–ª–∏\s+(.+?)\?',
                r'–µ—Å—Ç—å\s+–ª–∏\s+(.+?)\?',
                r'—á—Ç–æ\s+—Ç–∞–∫–æ–µ\s+(.+?)\?',
                r'—á—Ç–æ\s+–æ–∑–Ω–∞—á–∞–µ—Ç\s+(.+?)\?',
                r'—á—Ç–æ\s+–∑–Ω–∞—á–∏—Ç\s+(.+?)\?',
                r'–∫–∞–∫–∏–º\s+–æ–±—Ä–∞–∑–æ–º\s+(.+?)\?',
                r'–∫–∞–∫–∏–º\s+—Å–ø–æ—Å–æ–±–æ–º\s+(.+?)\?',
                r'–≤\s+—á–µ–º\s+—Ä–∞–∑–Ω–∏—Ü–∞\s+(.+?)\?',
                r'—á–µ–º\s+–æ—Ç–ª–∏—á–∞–µ—Ç—Å—è\s+(.+?)\?',
                r'–∫–∞–∫–∏–µ\s+(.+?)\?',
                r'–∫–∞–∫–æ–π\s+(.+?)\?',
                r'–∫–∞–∫–∞—è\s+(.+?)\?',
                r'–∫–∞–∫–æ–µ\s+(.+?)\?'
            ],
            'intent_patterns': {
                '—Ç–∞—Ä–∏—Ñ': [r'—Ç–∞—Ä–∏—Ñ', r'–∫–æ–º—Ñ–æ—Ä—Ç', r'–∫–ª–∞—Å—Å', r'–º–∞—à–∏–Ω', r'–ø—Ä–µ–º–∏—É–º'],
                '—Ä–∞—Å—Ü–µ–Ω–∫–∞': [r'—Ä–∞—Å—Ü–µ–Ω–∫', r'—Å—Ç–æ–∏–º–æ—Å—Ç', r'—Ü–µ–Ω', r'—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', r'–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä'],
                '–ø—Ä–µ–¥–∑–∞–∫–∞–∑': [r'–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω', r'–∑–∞—Ä–∞–Ω–µ', r'–ø—Ä–µ–¥–∑–∞–∫–∞–∑', r'–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä'],
                '–¥–æ—Å—Ç–∞–≤–∫–∞': [r'–¥–æ—Å—Ç–∞–≤–∫', r'–∫—É—Ä—å–µ—Ä', r'–ø–æ—Å—ã–ª–∫', r'–ø–µ—Ä–µ–≤–æ–∑–∫', r'–≥—Ä—É–∑', r'–≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–∫', r'—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': [r'–≤–æ–¥–∏—Ç–µ–ª', r'—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü', r'–∑–∞–∫–∞–∑', r'–±–∞–ª–∞–Ω—Å'],
                '–±–∞–ª–∞–Ω—Å': [r'–±–∞–ª–∞–Ω—Å', r'–ø–æ–ø–æ–ª–Ω–µ–Ω', r'qiwi', r'kaspi', r'–∫–∞—Ä—Ç'],
                '–º–æ—Ç–æ—á–∞—Å—ã': [r'–º–æ—Ç–æ—á–∞—Å', r'–º–∏–Ω—É—Ç', r'–≤—Ä–µ–º–µ–Ω', r'–¥–ª–∏—Ç–µ–ª—å–Ω'],
                '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä': [r'—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', r'–æ–∂–∏–¥–∞–Ω–∏', r'–ø–æ–µ—Ö–∞–ª', r'–æ—Å—Ç–∞–Ω–æ–≤'],
                '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': [r'–ø—Ä–∏–ª–æ–∂–µ–Ω–∏', r'–Ω–µ\s+—Ä–∞–±–æ—Ç–∞–µ—Ç', r'–æ–±–Ω–æ–≤–ª–µ–Ω', r'gps']
            }
        }
    
    def normalize_text(self, text: str, language: str = 'ru') -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"""
        if not text:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower().strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = text.split()
        filtered_words = [word for word in words if word not in self.stop_words.get(language, set())]
        
        return ' '.join(filtered_words)
    
    def get_word_stem(self, word: str, language: str = 'ru') -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Å–Ω–æ–≤—É —Å–ª–æ–≤–∞"""
        if not word:
            return ""
        
        word = word.lower().strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
        if language not in self.rules:
            return word
        
        rules = self.rules[language]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è
        for ending, variations in rules.get('endings', {}).items():
            if word.endswith(ending):
                # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ
                stem = word[:-len(ending)]
                return stem
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—Ñ—Ñ–∏–∫—Å—ã
        for suffix, variations in rules.get('suffixes', {}).items():
            if word.endswith(suffix):
                # –£–±–∏—Ä–∞–µ–º —Å—É—Ñ—Ñ–∏–∫—Å
                stem = word[:-len(suffix)]
                return stem
        
        return word
    
    def expand_query(self, query: str, language: str = 'ru') -> List[str]:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–º–æ—â—å—é —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏"""
        if not query:
            return []
        
        expanded = set()
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        normalized = self.normalize_text(query, language)
        expanded.add(normalized)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª–æ–≤–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏
        words = normalized.split()
        for word in words:
            stem = self.get_word_stem(word, language)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤—É
            expanded.add(stem)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏
            if language in self.rules:
                for ending in self.rules[language].get('endings', {}).keys():
                    if not word.endswith(ending):
                        expanded.add(stem + ending)
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            if language in self.synonyms:
                for base_word, synonyms in self.synonyms[language].items():
                    if stem == base_word or word == base_word or base_word in word or word in base_word:
                        for synonym in synonyms:
                            expanded.add(synonym)
                            expanded.add(self.get_word_stem(synonym, language))
                            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏ —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
                            if ' ' in synonym:
                                for part in synonym.split():
                                    expanded.add(part)
                                    expanded.add(self.get_word_stem(part, language))
        
        return list(expanded)
    
    def extract_keywords(self, text: str, language: str = 'ru') -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        normalized = self.normalize_text(text, language)
        words = normalized.split()
        
        keywords = []
        for word in words:
            stem = self.get_word_stem(word, language)
            keywords.append(stem)
            keywords.append(word)
        
        return list(set(keywords))
    
    def match_keywords(self, query_keywords: List[str], text_keywords: List[str]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–µ–ø–µ–Ω—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"""
        if not query_keywords or not text_keywords:
            return 0.0
        
        query_set = set(query_keywords)
        text_set = set(text_keywords)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        intersection = query_set.intersection(text_set)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–ª—é —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        return len(intersection) / max(len(query_set), len(text_set))
    
    def detect_language(self, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return 'ru'
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        kz_chars = set('”ô“ì“õ“£”©“±“Ø—ñ“ª')
        text_chars = set(text.lower())
        
        if kz_chars.intersection(text_chars):
            return 'kz'
        
        return 'ru'
    
    def analyze_intent(self, query: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞"""
        if not query:
            return {'intent': 'unknown', 'confidence': 0.0, 'language': 'ru'}
        
        language = self.detect_language(query)
        normalized_query = self.normalize_text(query, language)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        intent_scores = {}
        
        for intent, patterns in self.patterns['intent_patterns'].items():
            score = 0.0
            for pattern in patterns:
                if re.search(pattern, normalized_query, re.IGNORECASE):
                    score += 1.0
            
            if score > 0:
                intent_scores[intent] = score / len(patterns)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ª—É—á—à–µ–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–µ
        if intent_scores:
            best_intent = max(intent_scores.items(), key=lambda x: x[1])
            return {
                'intent': best_intent[0],
                'confidence': best_intent[1],
                'language': language,
                'all_scores': intent_scores
            }
        
        return {'intent': 'unknown', 'confidence': 0.0, 'language': language}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
enhanced_analyzer = EnhancedMorphologicalAnalyzer()

def enhance_classification_with_morphology(query: str, kb_data: Dict[str, Any]) -> Dict[str, Any]:
    """–£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    if not query or not kb_data:
        return {'intent': 'unknown', 'confidence': 0.0, 'language': 'ru'}
    
    try:
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        analysis = enhanced_analyzer.analyze_intent(query)
        language = analysis['language']
        
        # –ü–æ–ª—É—á–∞–µ–º FAQ —ç–ª–µ–º–µ–Ω—Ç—ã
        faq_items = kb_data.get('faq', [])
        if not faq_items:
            return analysis
        
        best_match = None
        best_score = 0.0
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        query_keywords = enhanced_analyzer.extract_keywords(query, language)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –û–¢–ö–õ–Æ–ß–ï–ù–û
        # –≠—Ç–æ –ø—Ä–∏–≤–æ–¥–∏–ª–æ –∫ –ª–æ–∂–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º —Å FAQ, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ "—á—Ç–æ —Ç–∞–∫–æ–µ"
        original_query = query
        # if len(query.strip().split()) == 1 and len(query.strip()) > 3:
        #     # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º "—á—Ç–æ —Ç–∞–∫–æ–µ"
        #     if not any(word in query.lower() for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º']):
        #         query = f"—á—Ç–æ —Ç–∞–∫–æ–µ {query}"
        #         logger.info(f"üîÑ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: '{original_query}' ‚Üí '{query}'")
        
        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        if len(query.strip()) < 20:  # –ö–æ—Ä–æ—Ç–∫–∏–µ –∑–∞–ø—Ä–æ—Å—ã
            query_words = query.lower().strip().split()
            for word in query_words:
                if len(word) > 3:  # –¢–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
                    query_keywords.append(word)
                    query_keywords.append(enhanced_analyzer.get_word_stem(word, language))
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
                    if language in enhanced_analyzer.synonyms:
                        for base_word, synonyms in enhanced_analyzer.synonyms[language].items():
                            if word == base_word or enhanced_analyzer.get_word_stem(word, language) == base_word:
                                for synonym in synonyms:
                                    query_keywords.append(synonym)
                                    query_keywords.append(enhanced_analyzer.get_word_stem(synonym, language))
        
        # –ò—â–µ–º –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        logger.info(f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: query_keywords = {query_keywords}")
        for item in faq_items:
            score = 0.0
            question = item.get('question', '')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = item.get('keywords', [])
            if keywords:
                keyword_score = enhanced_analyzer.match_keywords(query_keywords, keywords)
                score += keyword_score * 0.4
                logger.info(f"üîç FAQ: '{question}' keywords={keywords}, score={keyword_score:.2f}")
                if keyword_score > 0:
                    logger.info(f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: '{original_query}' ‚Üí '{question}' (keywords: {keyword_score:.2f})")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
            variations = item.get('question_variations', [])
            if variations:
                max_variation_score = 0.0
                for variation in variations:
                    variation_keywords = enhanced_analyzer.extract_keywords(variation, language)
                    variation_score = enhanced_analyzer.match_keywords(query_keywords, variation_keywords)
                    max_variation_score = max(max_variation_score, variation_score)
                score += max_variation_score * 0.4
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å
            main_question = item.get('question', '')
            if main_question:
                main_keywords = enhanced_analyzer.extract_keywords(main_question, language)
                main_score = enhanced_analyzer.match_keywords(query_keywords, main_keywords)
                score += main_score * 0.2
            
            # –ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è
            question_text = item.get('question', '').lower()
            context_bonus = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in query.lower() and '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in question_text:
                context_bonus += 0.3
            elif '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in query.lower() and '—á—Ç–æ —Ç–∞–∫–æ–µ' in question_text:
                context_bonus -= 0.2  # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            
            if '—á—Ç–æ —Ç–∞–∫–æ–µ' in query.lower() and '—á—Ç–æ —Ç–∞–∫–æ–µ' in question_text:
                context_bonus += 0.3
            elif '—á—Ç–æ —Ç–∞–∫–æ–µ' in query.lower() and '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in question_text:
                context_bonus -= 0.2  # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å
            final_score = score + context_bonus
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if final_score > best_score:
                best_score = final_score
                best_match = item
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if best_match and best_score > 0.1:  # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
            return {
                'intent': 'faq',
                'confidence': min(best_score, 1.0),
                'language': language,
                'matched_item': best_match,
                'match_score': best_score
            }
        
        return analysis
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}")
        return {'intent': 'unknown', 'confidence': 0.0, 'language': 'ru'}

__all__ = ['EnhancedMorphologicalAnalyzer', 'enhanced_analyzer', 'enhance_classification_with_morphology']
