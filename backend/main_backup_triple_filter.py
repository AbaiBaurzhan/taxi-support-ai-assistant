import json
import re
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException

# –ò–º–ø–æ—Ä—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    from enhanced_morphological_analyzer import enhance_classification_with_morphology, enhanced_analyzer
    MORPHOLOGY_AVAILABLE = True
    print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except ImportError:
    MORPHOLOGY_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Taxi Support AI Assistant", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="."), name="static")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str  # "kb" –∏–ª–∏ "llm"
    timestamp: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_json_file(filename: str) -> Dict[str, Any]:
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return {}
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ {filename}: {e}")
        return {}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
fixtures = load_json_file("fixtures.json")
kb_data = load_json_file("kb.json")

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã"""
    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    
    text = emoji_pattern.sub('', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[^\w\s\-.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
    try:
        lang = detect(text)
        if lang in ['ru', 'kk']:
            return 'ru'  # –†—É—Å—Å–∫–∏–π/–∫–∞–∑–∞—Ö—Å–∫–∏–π
        elif lang == 'en':
            return 'en'
        else:
            return 'ru'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π
    except LangDetectException:
        return 'ru'

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
def classify_intent(text: str) -> tuple[str, float]:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    text_lower = text.lower()
    
    # FAQ –∏–Ω—Ç–µ–Ω—Ç—ã
    faq_keywords = ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—á–µ—Ç', '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç', 
                   '–ø—Ä–æ–º–æ–∫–æ–¥', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ', '–∫–æ–¥', '–≤–≤–µ—Å—Ç–∏',
                   '–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑',
                   '—Å–≤—è–∑–∞—Ç—å—Å—è', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–≤–æ–¥–∏—Ç–µ–ª—å', '–∫–æ–Ω—Ç–∞–∫—Ç',
                   '–Ω–µ –ø—Ä–∏–µ—Ö–∞–ª', '–æ–ø–æ–∑–¥–∞–ª', '–∂–¥–∞—Ç—å', '–ø—Ä–æ–±–ª–µ–º–∞']
    
    # –°—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏
    ride_status_keywords = ['–≥–¥–µ –≤–æ–¥–∏—Ç–µ–ª—å', '—Å—Ç–∞—Ç—É—Å', '–ø–æ–µ–∑–¥–∫–∞', '–∑–∞–∫–∞–∑', '–æ–∂–∏–¥–∞–Ω–∏–µ']
    
    # –ß–µ–∫
    receipt_keywords = ['—á–µ–∫', '–∫–≤–∏—Ç–∞–Ω—Ü–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–ø—Ä–∞–≤–∫–∞']
    
    # –ö–∞—Ä—Ç—ã
    cards_keywords = ['–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–æ—Å–Ω–æ–≤–Ω–∞—è', '–ø–ª–∞—Ç–µ–∂', '–æ–ø–ª–∞—Ç–∞']
    
    # –ñ–∞–ª–æ–±—ã
    complaint_keywords = ['—Å–ø–∏—Å–∞–ª–∏ –¥–≤–∞–∂–¥—ã', '–¥–≤–æ–π–Ω–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ', '–∂–∞–ª–æ–±–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ']
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    faq_score = sum(1 for keyword in faq_keywords if keyword in text_lower)
    ride_score = sum(1 for keyword in ride_status_keywords if keyword in text_lower)
    receipt_score = sum(1 for keyword in receipt_keywords if keyword in text_lower)
    cards_score = sum(1 for keyword in cards_keywords if keyword in text_lower)
    complaint_score = sum(1 for keyword in complaint_keywords if keyword in text_lower)
    
    scores = {
        'faq': faq_score,
        'ride_status': ride_score,
        'receipt': receipt_score,
        'cards': cards_score,
        'complaint': complaint_score
    }
    
    max_intent = max(scores, key=scores.get)
    max_score = scores[max_intent]
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —á–µ—Ç–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞, —Å—á–∏—Ç–∞–µ–º FAQ
    if max_score == 0:
        return 'faq', 0.5
    
    confidence = min(max_score / 3.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 1.0
    return max_intent, confidence

# –ú–æ–∫–∏ –¥–ª—è —Ç–∞–∫—Å–∏
def get_ride_status(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_rides = fixtures.get("rides", {}).get(user_id, {})
    if not user_rides:
        return {"status": "no_rides", "message": "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"}
    
    return user_rides

def send_receipt(user_id: str) -> Dict[str, Any]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–µ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
    user_receipts = fixtures.get("receipts", {}).get(user_id, [])
    if not user_receipts:
        return {"status": "no_receipts", "message": "–£ –≤–∞—Å –Ω–µ—Ç —á–µ–∫–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"}
    
    latest_receipt = user_receipts[-1]
    return {
        "status": "sent",
        "receipt": latest_receipt,
        "message": f"–ß–µ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ email. –°—É–º–º–∞: {latest_receipt['amount']} —Ç–µ–Ω–≥–µ"
    }

def list_cards(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_cards = fixtures.get("cards", {}).get(user_id, [])
    if not user_cards:
        return {"status": "no_cards", "message": "–£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç"}
    
    return {"status": "success", "cards": user_cards}

def escalate_to_human(user_id: str, description: str) -> Dict[str, Any]:
    """–≠—Å–∫–∞–ª–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    tickets = fixtures.get("tickets", {})
    next_id = tickets.get("next_id", 1001)
    
    new_ticket = {
        "ticket_id": f"TKT_{next_id}",
        "user_id": user_id,
        "subject": "–≠—Å–∫–∞–ª–∞—Ü–∏—è –æ—Ç –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
        "description": description,
        "status": "open",
        "created_at": datetime.now().isoformat(),
        "priority": "medium"
    }
    
    tickets["tickets"].append(new_ticket)
    tickets["next_id"] = next_id + 1
    
    return {
        "status": "escalated",
        "ticket_id": new_ticket["ticket_id"],
        "message": f"–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–¥–∞–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É. –ù–æ–º–µ—Ä —Ç–∏–∫–µ—Ç–∞: {new_ticket['ticket_id']}"
    }

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ FAQ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
def search_with_three_filters(query: str, faq_items: List[Dict]) -> List[tuple]:
    """
    –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏:
    1. question_variations (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.5)
    2. keywords (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.3) 
    3. answer content (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.2)
    """
    if not query or not faq_items:
        return []
    
    results = []
    query_lower = query.lower().strip()
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
    query_words = re.sub(r'[^\w\s]', ' ', query_lower).split()
    query_words = [word for word in query_words if len(word) > 2]
    
    for item in faq_items:
        # Filter 1: Question Variations (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.5)
        variations_score = search_question_variations(query_lower, item.get("question_variations", []))
        
        # Filter 2: Keywords (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.3)
        keywords_score = search_keywords(query_words, item.get("keywords", []))
        
        # Filter 3: Answer Content (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.2)
        answer_score = search_answer_content(query_words, item.get("answer", ""))
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        total_score = (
            variations_score * 0.5 +    # 50% - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            keywords_score * 0.3 +      # 30% - —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            answer_score * 0.2          # 20% - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        )
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è
        if total_score >= 0.3:  # 30% –º–∏–Ω–∏–º—É–º
            results.append((item, total_score, {
                'variations': variations_score,
                'keywords': keywords_score, 
                'answer': answer_score
            }))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞
    return sorted(results, key=lambda x: x[1], reverse=True)


def search_question_variations(query: str, variations: List[str]) -> float:
    """Filter 1: –ü–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –≤–æ–ø—Ä–æ—Å–æ–≤ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"""
    if not variations:
        return 0.0
    
    best_score = 0.0
    
    for variation in variations:
        variation_lower = variation.lower()
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if query == variation_lower:
            return 1.0
        
        # –û—á–µ–Ω—å –±–ª–∏–∑–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (90%+)
        similarity = calculate_text_similarity(query, variation_lower)
        if similarity > 0.9:
            best_score = max(best_score, similarity)
        
        # –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (80%+)
        elif similarity > 0.8:
            best_score = max(best_score, similarity * 0.9)
        
        # –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (70%+)
        elif similarity > 0.7:
            best_score = max(best_score, similarity * 0.7)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
        if query in variation_lower or variation_lower in query:
            partial_score = min(len(query), len(variation_lower)) / max(len(query), len(variation_lower))
            best_score = max(best_score, partial_score * 0.6)
    
    return best_score


def search_keywords(query_words: List[str], keywords: List[str]) -> float:
    """Filter 2: –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–µ–π"""
    if not query_words or not keywords:
        return 0.0
    
    total_score = 0.0
    max_possible = len(keywords) * 8  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    
    for keyword in keywords:
        keyword_lower = keyword.lower()
        keyword_root = extract_word_root(keyword_lower)
        max_similarity = 0
        
        for word in query_words:
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
            similarity = calculate_word_similarity(word, keyword_lower)
            max_similarity = max(max_similarity, similarity)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–æ—Ä–Ω–µ–º
            word_root = extract_word_root(word)
            root_similarity = calculate_word_similarity(word_root, keyword_root)
            max_similarity = max(max_similarity, root_similarity)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –≤ –±–∞–ª–ª—ã
        if max_similarity >= 1.0:      # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            total_score += 8
        elif max_similarity >= 0.9:    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
            total_score += 7
        elif max_similarity >= 0.8:    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
            total_score += 6
        elif max_similarity >= 0.7:    # –•–æ—Ä–æ—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += 5
        elif max_similarity >= 0.6:    # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += 3
        elif max_similarity >= 0.5:    # –°–ª–∞–±–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += 2
        elif max_similarity >= 0.4:    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += 1
    
    return total_score / max_possible if max_possible > 0 else 0.0


def search_answer_content(query_words: List[str], answer: str) -> float:
    """Filter 3: –ü–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –æ—Ç–≤–µ—Ç–æ–≤"""
    if not query_words or not answer:
        return 0.0
    
    answer_lower = answer.lower()
    answer_words = re.sub(r'[^\w\s]', ' ', answer_lower).split()
    answer_words = [word for word in answer_words if len(word) > 2]
    
    if not answer_words:
        return 0.0
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç
    common_words = set(query_words).intersection(set(answer_words))
    if not common_words:
        return 0.0
    
    # –ü—Ä–æ—Å—Ç–æ–π TF-IDF
    tf_score = len(common_words) / len(answer_words)
    idf_score = 1 + (len(common_words) / len(query_words))
    tfidf_score = tf_score * idf_score
    
    # –ë–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞
    position_bonus = 0
    for i, word in enumerate(answer_words[:10]):  # –ü–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤
        if word in query_words:
            position_bonus += (10 - i) / 10 * 0.1
    
    # –ë–æ–Ω—É—Å –∑–∞ —Ñ—Ä–∞–∑–æ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    phrase_bonus = 0
    for i in range(len(query_words) - 1):
        phrase = f"{query_words[i]} {query_words[i+1]}"
        if phrase in answer_lower:
            phrase_bonus += 0.2
    
    return min(1.0, tfidf_score + position_bonus + phrase_bonus)


def calculate_text_similarity(text1: str, text2: str) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
    if not text1 or not text2:
        return 0.0
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if text1 == text2:
        return 1.0
    
    # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    distance = levenshtein_distance(text1, text2)
    max_len = max(len(text1), len(text2))
    similarity = 1 - (distance / max_len)
    
    return max(0.0, similarity)


def search_faq(text: str) -> Optional[Dict[str, Any]]:
    """–¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π FAQ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""
    if not text or not kb_data:
        return None
    
    faq_items = kb_data.get("faq", [])
    if not faq_items:
        return None
    
    logger.info(f"üîç –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è: '{text}'")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞
    results = search_with_three_filters(text, faq_items)
    
    if results:
        best_match, best_score, filter_scores = results[0]
        
        logger.info(f"üéØ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_score:.3f}")
        logger.info(f"   üìã Filter 1 (variations): {filter_scores['variations']:.3f}")
        logger.info(f"   üîë Filter 2 (keywords): {filter_scores['keywords']:.3f}")
        logger.info(f"   üìÑ Filter 3 (answer): {filter_scores['answer']:.3f}")
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
        if best_score >= 0.7:  # –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            logger.info(f"‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {best_score:.3f}")
            return best_match
        elif best_score >= 0.5:  # –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            logger.info(f"‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {best_score:.3f}")
            return best_match
        elif best_score >= 0.3:  # –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            logger.info(f"‚ö†Ô∏è –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {best_score:.3f}")
            return best_match
    
    # Fallback –∫ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –µ—Å–ª–∏ —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if MORPHOLOGY_AVAILABLE:
        try:
            result = enhance_classification_with_morphology(text, kb_data)
            confidence = result.get('confidence', 0)
            logger.info(f"üîç Fallback –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: confidence={confidence:.2f}")
            
            if result.get('matched_item') and confidence > 0.1:
                logger.info(f"‚úÖ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π fallback –Ω–∞–π–¥–µ–Ω: {confidence:.2f}")
                return result['matched_item']
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ fallback: {e}")
    
    logger.info(f"‚ùå –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    return None

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    if len(text_cleaned.split()) == 1 and len(text_cleaned) > 3:
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞, –¥–æ–±–∞–≤–ª—è–µ–º "—á—Ç–æ —Ç–∞–∫–æ–µ"
        if not any(word in text_cleaned for word in ['–∫–∞–∫', '—á—Ç–æ', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É', '–∑–∞—á–µ–º']):
            text_cleaned = f"—á—Ç–æ —Ç–∞–∫–æ–µ {text_cleaned}"
            logger.info(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ: '{text}' ‚Üí '—á—Ç–æ —Ç–∞–∫–æ–µ {text}'")
            # –û–±–Ω–æ–≤–ª—è–µ–º processed_text —Ç–æ–∂–µ
            processed_text = f"—á—Ç–æ —Ç–∞–∫–æ–µ {text_lower}"
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    slang_replacements = {
        '—á–µ': '—á—Ç–æ',
        '—Ç–∞–º': '',
        '–ø–æ': '',
        '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç': '–∫–∞–∫',
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': '—Ä–∞–±–æ—Ç–∞',
        '–∫–∞–∫': '–∫–∞–∫',
        '–≥–¥–µ': '–≥–¥–µ',
        '—á—Ç–æ': '—á—Ç–æ',
        '–∫–∞–∫–∏–µ': '–∫–∞–∫–∏–µ',
        '–º–æ–∂–Ω–æ': '–º–æ–∂–Ω–æ',
        '–Ω—É–∂–Ω–æ': '–Ω—É–∂–Ω–æ',
        '–µ—Å—Ç—å': '–µ—Å—Ç—å',
        '–±—ã—Ç—å': '–±—ã—Ç—å',
        '–≤—Ä–µ–º—è': '–≤—Ä–µ–º—è',
        '–¥–µ–Ω—å–≥–∏': '–¥–µ–Ω—å–≥–∏',
        '—Ü–µ–Ω–∞': '—Ü–µ–Ω–∞',
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': '—Å—Ç–æ–∏–º–æ—Å—Ç—å'
    }
    
    processed_text = text_lower
    for slang, replacement in slang_replacements.items():
        processed_text = processed_text.replace(slang, replacement)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    processed_text = ' '.join(processed_text.split())
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    def calculate_tfidf_score(query_words, doc_words):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not query_words or not doc_words:
            return 0
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        common_words = set(query_words).intersection(set(doc_words))
        if not common_words:
            return 0
        
        # –ü—Ä–æ—Å—Ç–æ–π TF-IDF: log(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤ / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        tf_score = len(common_words) / len(doc_words)
        idf_score = 1 + (len(common_words) / len(query_words))
        
        return tf_score * idf_score
    
    best_match = None
    best_score = 0
    
    for item in faq_items:
        score = 0
        
        # –£–õ–¨–¢–†–ê-–ì–ò–ë–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —É–º–Ω—ã–º stemming
        keywords = item.get("keywords", [])
        keyword_score = 0
        
        def calculate_word_similarity(word1, word2):
            """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
            if not word1 or not word2:
                return 0
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–∞
            w1, w2 = word1.lower(), word2.lower()
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if w1 == w2:
                return 1.0
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ—Ä–Ω–∏
            root1, root2 = extract_word_root(w1), extract_word_root(w2)
            if root1 == root2:
                return 0.9
            
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
            if len(root1) >= 4 and len(root2) >= 4:
                common_len = 0
                min_len = min(len(root1), len(root2))
                for i in range(min_len):
                    if root1[i] == root2[i]:
                        common_len += 1
                    else:
                        break
                
                if common_len >= 4:
                    return 0.7 + (common_len / min_len) * 0.2
            
            # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫
            def levenshtein_distance(s1, s2):
                if len(s1) < len(s2):
                    return levenshtein_distance(s2, s1)
                if len(s2) == 0:
                    return len(s1)
                
                previous_row = list(range(len(s2) + 1))
                for i, c1 in enumerate(s1):
                    current_row = [i + 1]
                    for j, c2 in enumerate(s2):
                        insertions = previous_row[j + 1] + 1
                        deletions = current_row[j] + 1
                        substitutions = previous_row[j] + (c1 != c2)
                        current_row.append(min(insertions, deletions, substitutions))
                    previous_row = current_row
                
                return previous_row[-1]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏
            if len(w1) >= 3 and len(w2) >= 3:
                distance = levenshtein_distance(w1, w2)
                max_len = max(len(w1), len(w2))
                similarity = 1 - (distance / max_len)
                
                if similarity > 0.6:  # 60% —Å—Ö–æ–∂–µ—Å—Ç–∏
                    return similarity * 0.5
            
            # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            phonetic_map = {
                '–∫': ['–≥', '—Ö', '–∫'], '–ø': ['–±', '–ø'], '—Ç': ['–¥', '—Ç'],
                '—Å': ['–∑', '—Ü', '—Å'], '—Ñ': ['–≤', '—Ñ'], '—à': ['—â', '–∂', '—à'],
                '—á': ['—â', '—á'], '—Ä': ['—Ä', '–ª'], '–ª': ['—Ä', '–ª']
            }
            
            if len(w1) >= 3 and len(w2) >= 3:
                phonetic_score = 0
                min_len = min(len(w1), len(w2))
                
                for i in range(min_len):
                    if w1[i] == w2[i]:
                        phonetic_score += 1
                    elif w1[i] in phonetic_map.get(w2[i], []) or w2[i] in phonetic_map.get(w1[i], []):
                        phonetic_score += 0.7
                
                phonetic_similarity = phonetic_score / min_len
                if phonetic_similarity > 0.7:
                    return phonetic_similarity * 0.4
            
            return 0
        
        def extract_word_root(word):
            """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –æ–∫–æ–Ω—á–∞–Ω–∏—è, —Å—É—Ñ—Ñ–∏–∫—Å—ã –∏ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏"""
            if len(word) < 4:
                return word
            
            root = word.lower()
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤
            special_cases = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–∏': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫—É': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–µ': '–¥–æ—Å—Ç–∞–≤',
                '–¥–æ—Å—Ç–∞–≤—â–∏–∫': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å': '–¥–æ—Å—Ç–∞–≤',
                '–ø–µ—Ä–µ–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–ø–æ–¥–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤',
                '–≤–æ–¥–∏—Ç–µ–ª—å': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª–∏': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç—å': '–≤–æ–¥–∏',
                '–≤–æ–¥–∏—Ç': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–æ': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∏': '–≤–æ–¥–∏',
                '–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—á–∏–∫': '–∑–∞–∫–∞–∑',
                '–∑–∞–∫–∞–∑—á–∏—Ü–∞': '–∑–∞–∫–∞–∑', '–ø–µ—Ä–µ–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–ø–æ–¥–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑',
                '—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': '—Ü–µ–Ω', '–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω', '–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω',
                '–∫–∞—Ä—Ç–æ—á–∫–∞': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ': '–∫–∞—Ä—Ç',
                '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞': '–±–∞–ª–∞–Ω—Å', '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å': '–±–∞–ª–∞–Ω—Å',
                '—Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è': '—Ç–∞—Ä–∏—Ñ', '—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å': '—Ç–∞—Ä–∏—Ñ'
            }
            
            if root in special_cases:
                return special_cases[root]
            
            # –†—É—Å—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            russian_endings = [
                '–æ–≤–∞—Ç—å—Å—è', '–∏–≤–∞—Ç—å—Å—è', '–µ–≤–∞—Ç—å—Å—è', '–∞—Ç—å—Å—è', '–∏—Ç—å—Å—è', '–µ—Ç—å—Å—è', '—É—Ç—å—Å—è',
                '–æ–≤–∞–Ω–∏–µ', '–∏—Ä–æ–≤–∞–Ω–∏–µ', '–µ–≤–∞–Ω–∏–µ', '–∞–Ω–∏–µ', '–µ–Ω–∏–µ', '–µ–Ω–∏–µ',
                '—Å–∫–∏–π', '—Å–∫–∞—è', '—Å–∫–æ–µ', '—Å–∫–∏–µ', '—Å–∫–æ–≥–æ', '—Å–∫–æ–π', '—Å–∫—É—é', '—Å–∫–∏–º', '—Å–∫–æ–º', '—Å–∫–∏—Ö', '—Å–∫–∏–º–∏',
                '–æ—Å—Ç—å', '–µ—Å—Ç—å', '—Å—Ç–≤–æ', '—Ç–µ–ª—å', '—Ç–µ–ª—å–Ω–∏—Ü–∞',
                '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–≥–æ', '–æ–π', '—É—é', '—ã–º', '–æ–º', '–∏—Ö', '—ã–º–∏',
                '–∏—è', '–∏–π', '–∏–µ', '–∏—é', '–∏–µ–º', '–∏–∏', '–∏—è–º–∏', '–∏—è—Ö',
                '–∏–∫', '–∏—Ü', '–∏—á', '–∏—â', '–Ω–∏–∫', '–Ω–∏—Ü', '—â–∏–∫', '—â–∏—Ü',
                '–∞', '–æ', '—É', '—ã', '–∏', '–µ', '—é', '–µ–º', '–æ–º', '–∞—Ö', '—è–º–∏', '—è—Ö',
                '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏', '–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã'
            ]
            
            # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏ (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            prefixes = [
                '–ø–µ—Ä–µ', '–ø—Ä–µ–¥', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—Ä–∞–∑', '—Ä–∞—Å', '–∏–∑', '–∏—Å',
                '–æ—Ç', '–æ–±', '–≤', '–≤–æ', '–∑–∞', '–Ω–∞', '–¥–æ', '–ø–æ', '—Å–æ', '–≤—ã', '—É'
            ]
            
            # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            for ending in russian_endings:
                if root.endswith(ending) and len(root) > len(ending) + 2:
                    root = root[:-len(ending)]
                    break
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            for prefix in prefixes:
                if root.startswith(prefix) and len(root) > len(prefix) + 3:
                    root = root[len(prefix):]
                    break
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ—Ä–Ω—è
            return root if len(root) >= 3 else word.lower()
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            keyword_root = extract_word_root(keyword_lower)
            max_similarity = 0
            
            # –£–õ–¨–¢–†–ê-–ü–†–û–î–í–ò–ù–£–¢–´–ô –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–ª–æ–≤–∞–º –∑–∞–ø—Ä–æ—Å–∞
            for word in text_cleaned.split():
                if len(word) < 3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
                similarity = calculate_word_similarity(word, keyword_lower)
                max_similarity = max(max_similarity, similarity)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–æ—Ä–Ω–µ–º
                word_root = extract_word_root(word)
                root_similarity = calculate_word_similarity(word_root, keyword_root)
                max_similarity = max(max_similarity, root_similarity)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –≤ –±–∞–ª–ª—ã
            if max_similarity >= 1.0:      # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                keyword_score += 8
            elif max_similarity >= 0.9:    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
                keyword_score += 7
            elif max_similarity >= 0.8:    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
                keyword_score += 6
            elif max_similarity >= 0.7:    # –•–æ—Ä–æ—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 5
            elif max_similarity >= 0.6:    # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 3
            elif max_similarity >= 0.5:    # –°–ª–∞–±–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 2
            elif max_similarity >= 0.4:    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 1
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if max_similarity > 0:
                # –ë–æ–Ω—É—Å –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                word_count = text_cleaned.count(keyword_lower)
                if word_count > 1:
                    keyword_score += word_count * 0.5
                
                # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ (–Ω–∞—á–∞–ª–æ –≤–∞–∂–Ω–µ–µ)
                words = text_cleaned.split()
                for i, word in enumerate(words):
                    if calculate_word_similarity(word, keyword_lower) > 0.7:
                        position_bonus = 1.0 - (i / len(words)) * 0.5
                        keyword_score += position_bonus * 0.5
                        break
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ñ–æ—Ä–º–∞–º —Å–ª–æ–≤–∞ (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏)
            keyword_forms = [
                keyword_lower,
                keyword_lower + '–∞', keyword_lower + '–æ', keyword_lower + '—É', keyword_lower + '—ã', keyword_lower + '–∏',
                keyword_lower + '–∞–º', keyword_lower + '–∞–º–∏', keyword_lower + '–∞—Ö',
                keyword_lower + '–æ–π', keyword_lower + '—É—é', keyword_lower + '—ã–º', keyword_lower + '–æ–º',
                keyword_lower + '—ã–π', keyword_lower + '–∞—è', keyword_lower + '–æ–µ', keyword_lower + '—ã–µ',
                keyword_lower + '–æ–≥–æ', keyword_lower + '–æ–π', keyword_lower + '–∏—Ö', keyword_lower + '—ã–º–∏',
                keyword_lower[:-1] if keyword_lower.endswith('–∞') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞'
                keyword_lower[:-1] if keyword_lower.endswith('–æ') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–æ'
                keyword_lower[:-1] if keyword_lower.endswith('–∏') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∏'
                keyword_lower[:-2] if keyword_lower.endswith('—ã–π') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '—ã–π'
                keyword_lower[:-2] if keyword_lower.endswith('–∞—è') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞—è'
            ]
            
            for form in keyword_forms:
                if form in text_cleaned:
                    keyword_score += 3
                    found_by_root = True
                    break
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
            
            # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Å–∏–Ω–æ–Ω–∏–º–∞–º
            extended_synonyms = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∏', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å', '–¥–æ—Å—Ç–∞–≤—â–∏–∫', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞', 
                           '–∫—É—Ä—å–µ—Ä', '–∫—É—Ä—å–µ—Ä—Å–∫–∞—è', '–ø–æ—Å—ã–ª–∫–∞', '–ø–µ—Ä–µ–≤–æ–∑–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞',
                           '–≥—Ä—É–∑', '–≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–∫–∞', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–æ—Ç–ø—Ä–∞–≤–∫–∞', '–ø–æ–ª—É—á–µ–Ω–∏–µ'],
                '—Ü–µ–Ω–∞': ['—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–ø—Ä–∞–π—Å', '–ø—Ä–µ–π—Å–∫—É—Ä–∞–Ω—Ç',
                        '—Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Å—Ç–æ–∏—Ç', '—Å—Ç–æ–∏—Ç—å', '–ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞', '–¥–µ–Ω—å–≥–∏'],
                '–∫–∞—Ä—Ç–∞': ['–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–∫–∞—Ä—Ç–æ—á–∫–∞', '–ø–ª–∞—Å—Ç–∏–∫', '–ø–ª–∞—Ç–µ–∂', '–±–∞–Ω–∫–æ–≤—Å–∫–∞—è',
                         '–∫—Ä–µ–¥–∏—Ç–∫–∞', '–¥–µ–±–µ—Ç–∫–∞', '—Å–±–µ—Ä–±–∞–Ω–∫', 'visa', 'mastercard'],
                '–±–∞–ª–∞–Ω—Å': ['–±–∞–ª–∞–Ω—Å', '—Å—á–µ—Ç', '—Å—á–µ—Ç–∞', '–¥–µ–Ω—å–≥–∏', '—Å—Ä–µ–¥—Å—Ç–≤–∞', '–∫–∞–ø–∏—Ç–∞–ª', '—Ñ–∏–Ω–∞–Ω—Å—ã',
                          '–Ω–∞–ª–∏—á–∫–∞', '–Ω–∞–ª–∏—á–Ω—ã–µ', '–∫–æ–ø–µ–π–∫–∏', '—Ä—É–±–ª–∏', '—Ç–µ–Ω–≥–µ'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': ['–≤–æ–¥–∏—Ç–µ–ª—å', '–≤–æ–¥–∏—Ç–µ–ª–∏', '—à–æ—Ñ–µ—Ä', '—Ç–∞–∫—Å–∏—Å—Ç', '–ø–µ—Ä–µ–≤–æ–∑—á–∏–∫', '–º–∞—à–∏–Ω–∏—Å—Ç',
                            '—É–ø—Ä–∞–≤–ª—è—é—â–∏–π', '–∫–æ–Ω–¥—É–∫—Ç–æ—Ä', '–ø–∏–ª–æ—Ç'],
                '–∑–∞–∫–∞–∑': ['–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–ø–æ–∫—É–ø–∫–∞', '–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ',
                         '—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ', '–ø—Ä–æ—Å—å–±–∞', '–∑–∞—è–≤–∫–∞']
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            if keyword_lower in extended_synonyms:
                for synonym in extended_synonyms[keyword_lower]:
                    for word in text_cleaned.split():
                        if len(word) >= 3:
                            synonym_similarity = calculate_word_similarity(word, synonym)
                            if synonym_similarity > 0.7:
                                keyword_score += synonym_similarity * 3
                                break
            
            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
            related_concepts = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–∞–¥—Ä–µ—Å', '–ø–æ–ª—É—á–∞—Ç–µ–ª—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å', '–ø–æ—Å—ã–ª–∫–∞', '–ø–∞–∫–µ—Ç', '—Ç–æ–≤–∞—Ä'],
                '—Ü–µ–Ω–∞': ['–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ–∫–æ–¥', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞'],
                '–∫–∞—Ä—Ç–∞': ['–ø—Ä–∏–≤—è–∑–∞—Ç—å', '–ø—Ä–∏–≤—è–∑–∫–∞', '–æ–ø–ª–∞—Ç–∞', '—Å–ø–∏—Å–∞–Ω–∏–µ', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ'],
                '–±–∞–ª–∞–Ω—Å': ['–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ', '—Å–ø–∏—Å–∞—Ç—å', '—Å–ø–∏—Å–∞–Ω–∏–µ', '–∏—Å—Ç–æ—Ä–∏—è'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': ['–º–∞—à–∏–Ω–∞', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '—Ç–∞–∫—Å–∏', '–ø–æ–µ–∑–¥–∫–∞', '–º–∞—Ä—à—Ä—É—Ç'],
                '–∑–∞–∫–∞–∑': ['—Å–¥–µ–ª–∞—Ç—å', '–æ—Ñ–æ—Ä–º–∏—Ç—å', '–æ—Ç–º–µ–Ω–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '—Å—Ç–∞—Ç—É—Å']
            }
            
            if keyword_lower in related_concepts:
                for concept in related_concepts[keyword_lower]:
                    for word in text_cleaned.split():
                        if len(word) >= 3:
                            concept_similarity = calculate_word_similarity(word, concept)
                            if concept_similarity > 0.8:
                                keyword_score += concept_similarity * 2
                                break
        
        # –ì–õ–£–ë–û–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤
        variations = item.get("question_variations", [])
        variation_score = 0
        query_words = text_cleaned.split()
        
        for variation in variations:
            variation_words = variation.lower().split()
            
            # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
            tfidf_score = calculate_tfidf_score(query_words, variation_words)
            variation_score += tfidf_score
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º (–¥–≤–∞ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            query_bigrams = set()
            for i in range(len(query_words) - 1):
                query_bigrams.add(f"{query_words[i]} {query_words[i+1]}")
            
            variation_bigrams = set()
            for i in range(len(variation_words) - 1):
                variation_bigrams.add(f"{variation_words[i]} {variation_words[i+1]}")
            
            bigram_matches = len(query_bigrams.intersection(variation_bigrams))
            variation_score += bigram_matches * 0.5
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ —Ç—Ä–∏–≥—Ä–∞–º–º–∞–º (—Ç—Ä–∏ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            if len(query_words) >= 3 and len(variation_words) >= 3:
                query_trigrams = set()
                for i in range(len(query_words) - 2):
                    query_trigrams.add(f"{query_words[i]} {query_words[i+1]} {query_words[i+2]}")
                
                variation_trigrams = set()
                for i in range(len(variation_words) - 2):
                    variation_trigrams.add(f"{variation_words[i]} {variation_words[i+1]} {variation_words[i+2]}")
                
                trigram_matches = len(query_trigrams.intersection(variation_trigrams))
                variation_score += trigram_matches * 1.0
        
        # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É
        main_question = item.get("question", "").lower()
        main_question_words = main_question.split()
        
        # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
        main_score = calculate_tfidf_score(query_words, main_question_words) * 2
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
        question_phrases = [
            "–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º",
            "–º–æ–∂–Ω–æ", "–Ω—É–∂–Ω–æ", "–µ—Å—Ç—å", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–¥–µ–ª–∞—Ç—å"
        ]
        
        for phrase in question_phrases:
            if phrase in text_cleaned and phrase in main_question:
                main_score += 0.5
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å–ª–æ–≤ (–≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ—Ä—è–¥–∫–∞
        ordered_matches = 0
        for i, word in enumerate(query_words):
            if word in main_question_words:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤–æ–ø—Ä–æ—Å–µ
                for j, q_word in enumerate(main_question_words):
                    if word == q_word:
                        # –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –±–∞–ª–ª
                        position_bonus = 1.0 - (abs(i - j) * 0.1)
                        ordered_matches += max(0.1, position_bonus)
                        break
        
        main_score += ordered_matches * 0.3
        
        # 4. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        important_words = ["–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ç–∞—Ä–∏—Ñ", "–∫–∞—Ä—Ç–∞", "–±–∞–ª–∞–Ω—Å", "–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–∫–∞–∑"]
        for word in important_words:
            if word in text_cleaned and word in main_question:
                main_score += 1.0  # –í—ã—Å–æ–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        
        # –£–õ–¨–¢–†–ê-–ü–†–û–î–í–ò–ù–£–¢–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–ª–æ–≤
        max_possible_keywords = len(keywords) * 8  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        normalized_keyword_score = keyword_score / max_possible_keywords if max_possible_keywords > 0 else 0
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        score = (
            normalized_keyword_score * 10.0 +   # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
            variation_score * 2.0 +             # –í–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            main_score * 4.0                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        )
        
        # –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –±–æ–Ω—É—Å—ã –∑–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        bonus_multiplier = 1.0
        
        if keyword_score > 0 and variation_score > 0 and main_score > 0:
            bonus_multiplier *= 1.3  # 30% –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        
        if keyword_score > len(keywords) * 5:  # –í—ã—Å–æ–∫–∏–π –±–∞–ª–ª –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            bonus_multiplier *= 1.2  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π 20% –±–æ–Ω—É—Å
        
        if variation_score > 2.0:  # –•–æ—Ä–æ—à–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
            bonus_multiplier *= 1.1  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π 10% –±–æ–Ω—É—Å
        
        score *= bonus_multiplier
        
        # –§–ò–ù–ê–õ–¨–ù–´–ô –±–æ–Ω—É—Å –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if score > 5.0:  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –æ–±—â–∏–π –±–∞–ª–ª
            score += 1.0  # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –±–æ–Ω—É—Å
        
        if score > best_score:
            best_score = score
            best_match = item
    
        # –ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–ª–∏—á–µ–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
        if best_match and best_score >= 0.3:  # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥, –Ω–æ –¥–æ–±–∞–≤–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            question_text = best_match.get('question', '').lower()
            context_bonus = 0
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
            if '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in text_cleaned and '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in question_text:
                context_bonus += 3.0
                logger.info("üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å: '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            elif '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in text_cleaned and '—Ä–∞–±–æ—Ç–∞–µ—Ç' in question_text:
                context_bonus += 1.5
                logger.info("üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å: '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' —á–∞—Å—Ç–∏—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            
            if '—á—Ç–æ —Ç–∞–∫–æ–µ' in text_cleaned and '—á—Ç–æ —Ç–∞–∫–æ–µ' in question_text:
                context_bonus += 3.0
                logger.info("üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å: '—á—Ç–æ —Ç–∞–∫–æ–µ' —Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            
            if '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π' in text_cleaned and '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π' in question_text:
                context_bonus += 2.0
                logger.info("üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å: '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π' —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            
            if '—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å' in text_cleaned and '—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å' in question_text:
                context_bonus += 2.0
                logger.info("üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å: '—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å' —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            
            if '–∑–∞–∫–∞–∑' in text_cleaned and '–∑–∞–∫–∞–∑' in question_text:
                context_bonus += 1.0
                logger.info("üéØ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å: '–∑–∞–∫–∞–∑' —Å–æ–≤–ø–∞–¥–∞–µ—Ç")
            
            # –®—Ç—Ä–∞—Ñ –∑–∞ –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            if '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in text_cleaned and '—á—Ç–æ —Ç–∞–∫–æ–µ' in question_text:
                context_bonus -= 2.0
                logger.info("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —à—Ç—Ä–∞—Ñ: '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' vs '—á—Ç–æ —Ç–∞–∫–æ–µ'")
            
            if '—á—Ç–æ —Ç–∞–∫–æ–µ' in text_cleaned and '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç' in question_text:
                context_bonus -= 2.0
                logger.info("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π —à—Ç—Ä–∞—Ñ: '—á—Ç–æ —Ç–∞–∫–æ–µ' vs '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç'")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –±–æ–Ω—É—Å
            final_score = best_score + context_bonus
            
            if final_score >= 0.5:
                logger.info(f"‚úÖ Fallback –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω: {best_score:.3f} + {context_bonus:.1f} = {final_score:.3f}")
                if best_match:
                    logger.info(f"üìã –ù–∞–π–¥–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {best_match.get('question', '')[:50]}...")
                return best_match
            else:
                logger.info(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è: {final_score:.3f} < 0.5, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
        
        logger.info(f"‚ùå Fallback –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–ª—É—á—à–∏–π –±–∞–ª–ª: {best_score:.3f})")
        return None

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞ —Å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º"""
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    processed_text = preprocess_text(request.text)
    if not processed_text:
        raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
    detected_lang = detect_language(processed_text)
    final_locale = request.locale if request.locale in ['ru', 'kz', 'en'] else detected_lang
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞
    intent, confidence = classify_intent(processed_text)
    
    logger.info(f"User: {request.user_id}, Intent: {intent}, Confidence: {confidence}, Locale: {final_locale}")
    
    response_text = ""
    source = "llm"
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É
    if intent == "faq":
        faq_result = search_faq(processed_text)
        if faq_result:
            response_text = faq_result["answer"]
            source = "kb"
            confidence = 0.9
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ FAQ, –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
            # LLM –æ—Ç–∫–ª—é—á–µ–Ω
            response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
    
    elif intent == "ride_status":
        ride_data = get_ride_status(request.user_id)
        if ride_data.get("status") == "no_rides":
            response_text = "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"
        else:
            driver = ride_data.get("driver", {})
            response_text = f"–í–∞—à–∞ –ø–æ–µ–∑–¥–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ. –í–æ–¥–∏—Ç–µ–ª—å: {driver.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –º–∞—à–∏–Ω–∞: {driver.get('car', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –Ω–æ–º–µ—Ä: {driver.get('plate', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}. –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏–±—ã—Ç–∏—è: {ride_data.get('estimated_arrival', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
        source = "kb"
        confidence = 0.95
    
    elif intent == "receipt":
        receipt_data = send_receipt(request.user_id)
        response_text = receipt_data["message"]
        source = "kb"
        confidence = 0.95
    
    elif intent == "cards":
        cards_data = list_cards(request.user_id)
        if cards_data["status"] == "success":
            cards = cards_data["cards"]
            primary_card = next((card for card in cards if card.get("is_primary")), None)
            response_text = f"–í–∞—à–∏ –∫–∞—Ä—Ç—ã: {', '.join([f'{card['type']} ****{card['last_four']}' for card in cards])}. –û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ä—Ç–∞: {primary_card['type']} ****{primary_card['last_four'] if primary_card else '–ù–µ –≤—ã–±—Ä–∞–Ω–∞'}"
        else:
            response_text = cards_data["message"]
        source = "kb"
        confidence = 0.95
    
    elif intent == "complaint":
        escalation_data = escalate_to_human(request.user_id, processed_text)
        response_text = escalation_data["message"]
        source = "kb"
        confidence = 0.95
    
    else:
        # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
        # LLM –æ—Ç–∫–ª—é—á–µ–Ω
        response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info(f"Response: {response_text[:100]}..., Source: {source}, Intent: {intent}, Confidence: {confidence}")
    
    return ChatResponse(
        response=response_text,
        intent=intent,
        confidence=confidence,
        source=source,
        timestamp=datetime.now().isoformat()
    )

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –º–æ–∫–æ–≤
@app.get("/ride-status/{user_id}")
async def get_ride_status_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏"""
    return get_ride_status(user_id)

@app.post("/send-receipt/{user_id}")
async def send_receipt_endpoint(user_id: str):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —á–µ–∫"""
    return send_receipt(user_id)

@app.get("/cards/{user_id}")
async def list_cards_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç"""
    return list_cards(user_id)

@app.post("/escalate/{user_id}")
async def escalate_endpoint(user_id: str, description: str):
    """–≠—Å–∫–∞–ª–∏—Ä–æ–≤–∞—Ç—å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    return escalate_to_human(user_id, description)

@app.get("/webapp")
async def webapp():
    """–û—Ç–¥–∞—á–∞ WebApp –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    return FileResponse("webapp.html")

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
