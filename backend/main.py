import json
import re
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException

# –ò–º–ø–æ—Ä—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    from enhanced_morphological_analyzer import enhance_classification_with_morphology, enhanced_analyzer
    MORPHOLOGY_AVAILABLE = True
    print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except ImportError:
    MORPHOLOGY_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Taxi Support AI Assistant", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="."), name="static")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str  # "kb" –∏–ª–∏ "llm"
    timestamp: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_json_file(filename: str) -> Dict[str, Any]:
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return {}
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ {filename}: {e}")
        return {}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
fixtures = load_json_file("fixtures.json")
kb_data = load_json_file("kb.json")

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã"""
    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    
    text = emoji_pattern.sub('', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[^\w\s\-.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
    try:
        lang = detect(text)
        if lang in ['ru', 'kk']:
            return 'ru'  # –†—É—Å—Å–∫–∏–π/–∫–∞–∑–∞—Ö—Å–∫–∏–π
        elif lang == 'en':
            return 'en'
        else:
            return 'ru'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π
    except LangDetectException:
        return 'ru'

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
def classify_intent(text: str) -> tuple[str, float]:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    text_lower = text.lower()
    
    # FAQ –∏–Ω—Ç–µ–Ω—Ç—ã
    faq_keywords = ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—á–µ—Ç', '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç', 
                   '–ø—Ä–æ–º–æ–∫–æ–¥', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ', '–∫–æ–¥', '–≤–≤–µ—Å—Ç–∏',
                   '–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑',
                   '—Å–≤—è–∑–∞—Ç—å—Å—è', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–≤–æ–¥–∏—Ç–µ–ª—å', '–∫–æ–Ω—Ç–∞–∫—Ç',
                   '–Ω–µ –ø—Ä–∏–µ—Ö–∞–ª', '–æ–ø–æ–∑–¥–∞–ª', '–∂–¥–∞—Ç—å', '–ø—Ä–æ–±–ª–µ–º–∞']
    
    # –°—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏
    ride_status_keywords = ['–≥–¥–µ –≤–æ–¥–∏—Ç–µ–ª—å', '—Å—Ç–∞—Ç—É—Å', '–ø–æ–µ–∑–¥–∫–∞', '–∑–∞–∫–∞–∑', '–æ–∂–∏–¥–∞–Ω–∏–µ']
    
    # –ß–µ–∫
    receipt_keywords = ['—á–µ–∫', '–∫–≤–∏—Ç–∞–Ω—Ü–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–ø—Ä–∞–≤–∫–∞']
    
    # –ö–∞—Ä—Ç—ã
    cards_keywords = ['–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–æ—Å–Ω–æ–≤–Ω–∞—è', '–ø–ª–∞—Ç–µ–∂', '–æ–ø–ª–∞—Ç–∞']
    
    # –ñ–∞–ª–æ–±—ã
    complaint_keywords = ['—Å–ø–∏—Å–∞–ª–∏ –¥–≤–∞–∂–¥—ã', '–¥–≤–æ–π–Ω–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ', '–∂–∞–ª–æ–±–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ']
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    faq_score = sum(1 for keyword in faq_keywords if keyword in text_lower)
    ride_score = sum(1 for keyword in ride_status_keywords if keyword in text_lower)
    receipt_score = sum(1 for keyword in receipt_keywords if keyword in text_lower)
    cards_score = sum(1 for keyword in cards_keywords if keyword in text_lower)
    complaint_score = sum(1 for keyword in complaint_keywords if keyword in text_lower)
    
    scores = {
        'faq': faq_score,
        'ride_status': ride_score,
        'receipt': receipt_score,
        'cards': cards_score,
        'complaint': complaint_score
    }
    
    max_intent = max(scores, key=scores.get)
    max_score = scores[max_intent]
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —á–µ—Ç–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞, —Å—á–∏—Ç–∞–µ–º FAQ
    if max_score == 0:
        return 'faq', 0.5
    
    confidence = min(max_score / 3.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 1.0
    return max_intent, confidence

# –ú–æ–∫–∏ –¥–ª—è —Ç–∞–∫—Å–∏
def get_ride_status(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_rides = fixtures.get("rides", {}).get(user_id, {})
    if not user_rides:
        return {"status": "no_rides", "message": "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"}
    
    return user_rides

def send_receipt(user_id: str) -> Dict[str, Any]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–µ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
    user_receipts = fixtures.get("receipts", {}).get(user_id, [])
    if not user_receipts:
        return {"status": "no_receipts", "message": "–£ –≤–∞—Å –Ω–µ—Ç —á–µ–∫–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"}
    
    latest_receipt = user_receipts[-1]
    return {
        "status": "sent",
        "receipt": latest_receipt,
        "message": f"–ß–µ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ email. –°—É–º–º–∞: {latest_receipt['amount']} —Ç–µ–Ω–≥–µ"
    }

def list_cards(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_cards = fixtures.get("cards", {}).get(user_id, [])
    if not user_cards:
        return {"status": "no_cards", "message": "–£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç"}
    
    return {"status": "success", "cards": user_cards}

def escalate_to_human(user_id: str, description: str) -> Dict[str, Any]:
    """–≠—Å–∫–∞–ª–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    tickets = fixtures.get("tickets", {})
    next_id = tickets.get("next_id", 1001)
    
    new_ticket = {
        "ticket_id": f"TKT_{next_id}",
        "user_id": user_id,
        "subject": "–≠—Å–∫–∞–ª–∞—Ü–∏—è –æ—Ç –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
        "description": description,
        "status": "open",
        "created_at": datetime.now().isoformat(),
        "priority": "medium"
    }
    
    tickets["tickets"].append(new_ticket)
    tickets["next_id"] = next_id + 1
    
    return {
        "status": "escalated",
        "ticket_id": new_ticket["ticket_id"],
        "message": f"–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–¥–∞–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É. –ù–æ–º–µ—Ä —Ç–∏–∫–µ—Ç–∞: {new_ticket['ticket_id']}"
    }

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ FAQ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
def search_faq(text: str) -> Optional[Dict[str, Any]]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π FAQ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    if not text or not kb_data:
        return None
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    if MORPHOLOGY_AVAILABLE:
        try:
            result = enhance_classification_with_morphology(text, kb_data)
            confidence = result.get('confidence', 0)
            logger.info(f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: confidence={confidence:.2f}, intent={result.get('intent', 'unknown')}")
            
            # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–æ –º–∏–Ω–∏–º—É–º–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            if result.get('matched_item') and confidence > 0.05:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                logger.info(f"‚úÖ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω: {confidence:.2f}")
                return result['matched_item']
            elif result.get('matched_item'):
                logger.info(f"‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            traceback.print_exc()
    else:
        logger.warning("‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
    
    # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
    faq_items = kb_data.get("faq", [])
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    slang_replacements = {
        '—á–µ': '—á—Ç–æ',
        '—Ç–∞–º': '',
        '–ø–æ': '',
        '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç': '–∫–∞–∫',
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': '—Ä–∞–±–æ—Ç–∞',
        '–∫–∞–∫': '–∫–∞–∫',
        '–≥–¥–µ': '–≥–¥–µ',
        '—á—Ç–æ': '—á—Ç–æ',
        '–∫–∞–∫–∏–µ': '–∫–∞–∫–∏–µ',
        '–º–æ–∂–Ω–æ': '–º–æ–∂–Ω–æ',
        '–Ω—É–∂–Ω–æ': '–Ω—É–∂–Ω–æ',
        '–µ—Å—Ç—å': '–µ—Å—Ç—å',
        '–±—ã—Ç—å': '–±—ã—Ç—å',
        '–≤—Ä–µ–º—è': '–≤—Ä–µ–º—è',
        '–¥–µ–Ω—å–≥–∏': '–¥–µ–Ω—å–≥–∏',
        '—Ü–µ–Ω–∞': '—Ü–µ–Ω–∞',
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': '—Å—Ç–æ–∏–º–æ—Å—Ç—å'
    }
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    import re
    text_cleaned = re.sub(r'[^\w\s]', ' ', text_lower)
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    
    processed_text = text_lower
    for slang, replacement in slang_replacements.items():
        processed_text = processed_text.replace(slang, replacement)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    processed_text = ' '.join(processed_text.split())
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    def calculate_tfidf_score(query_words, doc_words):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not query_words or not doc_words:
            return 0
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        common_words = set(query_words).intersection(set(doc_words))
        if not common_words:
            return 0
        
        # –ü—Ä–æ—Å—Ç–æ–π TF-IDF: log(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤ / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        tf_score = len(common_words) / len(doc_words)
        idf_score = 1 + (len(common_words) / len(query_words))
        
        return tf_score * idf_score
    
    best_match = None
    best_score = 0
    
    for item in faq_items:
        score = 0
        
        # –£–õ–¨–¢–†–ê-–ì–ò–ë–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —É–º–Ω—ã–º stemming
        keywords = item.get("keywords", [])
        keyword_score = 0
        
        def extract_word_root(word):
            """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –æ–∫–æ–Ω—á–∞–Ω–∏—è, —Å—É—Ñ—Ñ–∏–∫—Å—ã –∏ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏"""
            if len(word) < 4:
                return word
            
            root = word.lower()
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤
            special_cases = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–∏': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫—É': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–µ': '–¥–æ—Å—Ç–∞–≤',
                '–¥–æ—Å—Ç–∞–≤—â–∏–∫': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å': '–¥–æ—Å—Ç–∞–≤',
                '–ø–µ—Ä–µ–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–ø–æ–¥–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤',
                '–≤–æ–¥–∏—Ç–µ–ª—å': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª–∏': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç—å': '–≤–æ–¥–∏',
                '–≤–æ–¥–∏—Ç': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–æ': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∏': '–≤–æ–¥–∏',
                '–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—á–∏–∫': '–∑–∞–∫–∞–∑',
                '–∑–∞–∫–∞–∑—á–∏—Ü–∞': '–∑–∞–∫–∞–∑', '–ø–µ—Ä–µ–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–ø–æ–¥–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑',
                '—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': '—Ü–µ–Ω', '–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω', '–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω',
                '–∫–∞—Ä—Ç–æ—á–∫–∞': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ': '–∫–∞—Ä—Ç',
                '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞': '–±–∞–ª–∞–Ω—Å', '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å': '–±–∞–ª–∞–Ω—Å',
                '—Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è': '—Ç–∞—Ä–∏—Ñ', '—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å': '—Ç–∞—Ä–∏—Ñ'
            }
            
            if root in special_cases:
                return special_cases[root]
            
            # –†—É—Å—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            russian_endings = [
                '–æ–≤–∞—Ç—å—Å—è', '–∏–≤–∞—Ç—å—Å—è', '–µ–≤–∞—Ç—å—Å—è', '–∞—Ç—å—Å—è', '–∏—Ç—å—Å—è', '–µ—Ç—å—Å—è', '—É—Ç—å—Å—è',
                '–æ–≤–∞–Ω–∏–µ', '–∏—Ä–æ–≤–∞–Ω–∏–µ', '–µ–≤–∞–Ω–∏–µ', '–∞–Ω–∏–µ', '–µ–Ω–∏–µ', '–µ–Ω–∏–µ',
                '—Å–∫–∏–π', '—Å–∫–∞—è', '—Å–∫–æ–µ', '—Å–∫–∏–µ', '—Å–∫–æ–≥–æ', '—Å–∫–æ–π', '—Å–∫—É—é', '—Å–∫–∏–º', '—Å–∫–æ–º', '—Å–∫–∏—Ö', '—Å–∫–∏–º–∏',
                '–æ—Å—Ç—å', '–µ—Å—Ç—å', '—Å—Ç–≤–æ', '—Ç–µ–ª—å', '—Ç–µ–ª—å–Ω–∏—Ü–∞',
                '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–≥–æ', '–æ–π', '—É—é', '—ã–º', '–æ–º', '–∏—Ö', '—ã–º–∏',
                '–∏—è', '–∏–π', '–∏–µ', '–∏—é', '–∏–µ–º', '–∏–∏', '–∏—è–º–∏', '–∏—è—Ö',
                '–∏–∫', '–∏—Ü', '–∏—á', '–∏—â', '–Ω–∏–∫', '–Ω–∏—Ü', '—â–∏–∫', '—â–∏—Ü',
                '–∞', '–æ', '—É', '—ã', '–∏', '–µ', '—é', '–µ–º', '–æ–º', '–∞—Ö', '—è–º–∏', '—è—Ö',
                '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏', '–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã'
            ]
            
            # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏ (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            prefixes = [
                '–ø–µ—Ä–µ', '–ø—Ä–µ–¥', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—Ä–∞–∑', '—Ä–∞—Å', '–∏–∑', '–∏—Å',
                '–æ—Ç', '–æ–±', '–≤', '–≤–æ', '–∑–∞', '–Ω–∞', '–¥–æ', '–ø–æ', '—Å–æ', '–≤—ã', '—É'
            ]
            
            # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            for ending in russian_endings:
                if root.endswith(ending) and len(root) > len(ending) + 2:
                    root = root[:-len(ending)]
                    break
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            for prefix in prefixes:
                if root.startswith(prefix) and len(root) > len(prefix) + 3:
                    root = root[len(prefix):]
                    break
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ—Ä–Ω—è
            return root if len(root) >= 3 else word.lower()
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            keyword_root = extract_word_root(keyword_lower)
            
            # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–≤—ã—Å—à–∏–π –±–∞–ª–ª)
            if keyword_lower in text_cleaned:
                keyword_score += 6
                continue
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–æ—Ä–Ω—é —Å–ª–æ–≤–∞ (stemming) - –ì–õ–ê–í–ù–´–ô –ê–õ–ì–û–†–ò–¢–ú
            found_by_root = False
            for word in text_cleaned.split():
                word_root = extract_word_root(word)
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
                if keyword_root == word_root:
                    keyword_score += 5
                    found_by_root = True
                    break
                
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π (–º–∏–Ω–∏–º—É–º 4 —Å–∏–º–≤–æ–ª–∞)
                if len(keyword_root) >= 4 and len(word_root) >= 4:
                    min_len = min(len(keyword_root), len(word_root))
                    if keyword_root[:min_len] == word_root[:min_len] and min_len >= 4:
                        keyword_score += 4
                        found_by_root = True
                        break
            
            if found_by_root:
                continue
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ñ–æ—Ä–º–∞–º —Å–ª–æ–≤–∞ (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏)
            keyword_forms = [
                keyword_lower,
                keyword_lower + '–∞', keyword_lower + '–æ', keyword_lower + '—É', keyword_lower + '—ã', keyword_lower + '–∏',
                keyword_lower + '–∞–º', keyword_lower + '–∞–º–∏', keyword_lower + '–∞—Ö',
                keyword_lower + '–æ–π', keyword_lower + '—É—é', keyword_lower + '—ã–º', keyword_lower + '–æ–º',
                keyword_lower + '—ã–π', keyword_lower + '–∞—è', keyword_lower + '–æ–µ', keyword_lower + '—ã–µ',
                keyword_lower + '–æ–≥–æ', keyword_lower + '–æ–π', keyword_lower + '–∏—Ö', keyword_lower + '—ã–º–∏',
                keyword_lower[:-1] if keyword_lower.endswith('–∞') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞'
                keyword_lower[:-1] if keyword_lower.endswith('–æ') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–æ'
                keyword_lower[:-1] if keyword_lower.endswith('–∏') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∏'
                keyword_lower[:-2] if keyword_lower.endswith('—ã–π') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '—ã–π'
                keyword_lower[:-2] if keyword_lower.endswith('–∞—è') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞—è'
            ]
            
            for form in keyword_forms:
                if form in text_cleaned:
                    keyword_score += 3
                    found_by_root = True
                    break
            
            if found_by_root:
                continue
            
            # 4. –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)
            if keyword_lower in text_lower or keyword_lower in processed_text:
                keyword_score += 2
                continue
            
            # 5. –û–±—Ä–∞—Ç–Ω–æ–µ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å –≤ –∫–ª—é—á–µ–≤–æ–º —Å–ª–æ–≤–µ)
            for word in text_cleaned.split():
                if len(word) > 3 and word in keyword_lower:
                    keyword_score += 2
                    break
            
            # 6. –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–ø–æ—Ö–æ–∂–∏–µ –∑–≤—É–∫–∏)
            phonetic_map = {
                '–∫': ['–≥', '—Ö'],
                '–ø': ['–±'],
                '—Ç': ['–¥'],
                '—Å': ['–∑', '—Ü'],
                '—Ñ': ['–≤'],
                '—à': ['—â', '–∂'],
                '—á': ['—â']
            }
            
            for word in text_cleaned.split():
                word_root = extract_word_root(word)
                if len(word_root) >= 4 and len(keyword_root) >= 4:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∫–æ—Ä–Ω–µ–π
                    similarity_count = 0
                    min_len = min(len(word_root), len(keyword_root))
                    
                    for i in range(min_len):
                        if word_root[i] == keyword_root[i]:
                            similarity_count += 1
                        elif word_root[i] in phonetic_map.get(keyword_root[i], []):
                            similarity_count += 0.5
                    
                    if similarity_count >= min_len * 0.7:  # 70% —Å—Ö–æ–¥—Å—Ç–≤–∞
                        keyword_score += 1.5
                        break
        
        # –ì–õ–£–ë–û–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤
        variations = item.get("question_variations", [])
        variation_score = 0
        query_words = text_cleaned.split()
        
        for variation in variations:
            variation_words = variation.lower().split()
            
            # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
            tfidf_score = calculate_tfidf_score(query_words, variation_words)
            variation_score += tfidf_score
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º (–¥–≤–∞ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            query_bigrams = set()
            for i in range(len(query_words) - 1):
                query_bigrams.add(f"{query_words[i]} {query_words[i+1]}")
            
            variation_bigrams = set()
            for i in range(len(variation_words) - 1):
                variation_bigrams.add(f"{variation_words[i]} {variation_words[i+1]}")
            
            bigram_matches = len(query_bigrams.intersection(variation_bigrams))
            variation_score += bigram_matches * 0.5
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ —Ç—Ä–∏–≥—Ä–∞–º–º–∞–º (—Ç—Ä–∏ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            if len(query_words) >= 3 and len(variation_words) >= 3:
                query_trigrams = set()
                for i in range(len(query_words) - 2):
                    query_trigrams.add(f"{query_words[i]} {query_words[i+1]} {query_words[i+2]}")
                
                variation_trigrams = set()
                for i in range(len(variation_words) - 2):
                    variation_trigrams.add(f"{variation_words[i]} {variation_words[i+1]} {variation_words[i+2]}")
                
                trigram_matches = len(query_trigrams.intersection(variation_trigrams))
                variation_score += trigram_matches * 1.0
        
        # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É
        main_question = item.get("question", "").lower()
        main_question_words = main_question.split()
        
        # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
        main_score = calculate_tfidf_score(query_words, main_question_words) * 2
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
        question_phrases = [
            "–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º",
            "–º–æ–∂–Ω–æ", "–Ω—É–∂–Ω–æ", "–µ—Å—Ç—å", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–¥–µ–ª–∞—Ç—å"
        ]
        
        for phrase in question_phrases:
            if phrase in text_cleaned and phrase in main_question:
                main_score += 0.5
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å–ª–æ–≤ (–≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        query_words_set = set(query_words)
        question_words_set = set(main_question_words)
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ—Ä—è–¥–∫–∞
        ordered_matches = 0
        for i, word in enumerate(query_words):
            if word in main_question_words:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤–æ–ø—Ä–æ—Å–µ
                for j, q_word in enumerate(main_question_words):
                    if word == q_word:
                        # –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –±–∞–ª–ª
                        position_bonus = 1.0 - (abs(i - j) * 0.1)
                        ordered_matches += max(0.1, position_bonus)
                        break
        
        main_score += ordered_matches * 0.3
        
        # 4. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        important_words = ["–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ç–∞—Ä–∏—Ñ", "–∫–∞—Ä—Ç–∞", "–±–∞–ª–∞–Ω—Å", "–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–∫–∞–∑"]
        for word in important_words:
            if word in text_cleaned and word in main_question:
                main_score += 1.0  # –í—ã—Å–æ–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–ª–æ–≤ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        max_possible_keywords = len(keywords) * 5  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        normalized_keyword_score = keyword_score / max_possible_keywords if max_possible_keywords > 0 else 0
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        score = (
            normalized_keyword_score * 6.0 +    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ = —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            variation_score * 1.5 +             # –í–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            main_score * 3.0                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        )
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if keyword_score > 0 and variation_score > 0 and main_score > 0:
            score *= 1.2  # 20% –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        
        if score > best_score:
            best_score = score
            best_match = item
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–∞–ª–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–π
    if best_score >= 0.5:  # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        logger.info(f"‚úÖ Fallback –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω: {best_score:.3f}")
        if best_match:
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {best_match.get('question', '')[:50]}...")
        return best_match
    
    logger.info(f"‚ùå Fallback –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–ª—É—á—à–∏–π –±–∞–ª–ª: {best_score:.3f})")
    return None

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞ —Å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º"""
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    processed_text = preprocess_text(request.text)
    if not processed_text:
        raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
    detected_lang = detect_language(processed_text)
    final_locale = request.locale if request.locale in ['ru', 'kz', 'en'] else detected_lang
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞
    intent, confidence = classify_intent(processed_text)
    
    logger.info(f"User: {request.user_id}, Intent: {intent}, Confidence: {confidence}, Locale: {final_locale}")
    
    response_text = ""
    source = "llm"
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É
    if intent == "faq":
        faq_result = search_faq(processed_text)
        if faq_result:
            response_text = faq_result["answer"]
            source = "kb"
            confidence = 0.9
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ FAQ, –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
            prompt = llm_client.create_taxi_context_prompt(processed_text, intent, final_locale)
            response_text = llm_client.generate_response(prompt)
    
    elif intent == "ride_status":
        ride_data = get_ride_status(request.user_id)
        if ride_data.get("status") == "no_rides":
            response_text = "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"
        else:
            driver = ride_data.get("driver", {})
            response_text = f"–í–∞—à–∞ –ø–æ–µ–∑–¥–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ. –í–æ–¥–∏—Ç–µ–ª—å: {driver.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –º–∞—à–∏–Ω–∞: {driver.get('car', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –Ω–æ–º–µ—Ä: {driver.get('plate', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}. –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏–±—ã—Ç–∏—è: {ride_data.get('estimated_arrival', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
        source = "kb"
        confidence = 0.95
    
    elif intent == "receipt":
        receipt_data = send_receipt(request.user_id)
        response_text = receipt_data["message"]
        source = "kb"
        confidence = 0.95
    
    elif intent == "cards":
        cards_data = list_cards(request.user_id)
        if cards_data["status"] == "success":
            cards = cards_data["cards"]
            primary_card = next((card for card in cards if card.get("is_primary")), None)
            response_text = f"–í–∞—à–∏ –∫–∞—Ä—Ç—ã: {', '.join([f'{card['type']} ****{card['last_four']}' for card in cards])}. –û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ä—Ç–∞: {primary_card['type']} ****{primary_card['last_four'] if primary_card else '–ù–µ –≤—ã–±—Ä–∞–Ω–∞'}"
        else:
            response_text = cards_data["message"]
        source = "kb"
        confidence = 0.95
    
    elif intent == "complaint":
        escalation_data = escalate_to_human(request.user_id, processed_text)
        response_text = escalation_data["message"]
        source = "kb"
        confidence = 0.95
    
    else:
        # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
        prompt = llm_client.create_taxi_context_prompt(processed_text, intent, final_locale)
        response_text = llm_client.generate_response(prompt)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info(f"Response: {response_text[:100]}..., Source: {source}, Intent: {intent}, Confidence: {confidence}")
    
    return ChatResponse(
        response=response_text,
        intent=intent,
        confidence=confidence,
        source=source,
        timestamp=datetime.now().isoformat()
    )

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –º–æ–∫–æ–≤
@app.get("/ride-status/{user_id}")
async def get_ride_status_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏"""
    return get_ride_status(user_id)

@app.post("/send-receipt/{user_id}")
async def send_receipt_endpoint(user_id: str):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —á–µ–∫"""
    return send_receipt(user_id)

@app.get("/cards/{user_id}")
async def list_cards_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç"""
    return list_cards(user_id)

@app.post("/escalate/{user_id}")
async def escalate_endpoint(user_id: str, description: str):
    """–≠—Å–∫–∞–ª–∏—Ä–æ–≤–∞—Ç—å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    return escalate_to_human(user_id, description)

@app.get("/webapp")
async def webapp():
    """–û—Ç–¥–∞—á–∞ WebApp –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    return FileResponse("webapp.html")

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
