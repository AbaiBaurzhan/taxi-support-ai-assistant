import json
import re
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException

# –ò–º–ø–æ—Ä—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    from enhanced_morphological_analyzer import enhance_classification_with_morphology, enhanced_analyzer
    MORPHOLOGY_AVAILABLE = True
    print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except ImportError:
    MORPHOLOGY_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Taxi Support AI Assistant", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="."), name="static")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str  # "kb" –∏–ª–∏ "llm"
    timestamp: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_json_file(filename: str) -> Dict[str, Any]:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –ø—É—Ç–µ–π
    possible_paths = [
        filename,
        f"../{filename}",
        f"./{filename}"
    ]
    
    for path in possible_paths:
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω —Ñ–∞–π–ª: {path}")
                return data
        except FileNotFoundError:
            continue
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ {path}: {e}")
            continue
    
    logger.error(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ –ø—É—Ç–µ–π: {possible_paths}")
    return {}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
fixtures = load_json_file("fixtures.json")
kb_data = load_json_file("kb.json")

# –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ fixtures - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å
if isinstance(fixtures, list):
    # –ï—Å–ª–∏ fixtures –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ —Å–ø–∏—Å–æ–∫, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    fixtures = {
        "rides": fixtures,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞–∫ –µ—Å—Ç—å
        "receipts": [], 
        "cards": [], 
        "tickets": {"next_id": 1001}
    }
    print("‚ö†Ô∏è fixtures.json –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ —Å–ø–∏—Å–æ–∫, –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É")
elif not isinstance(fixtures, dict):
    # –ï—Å–ª–∏ fixtures –Ω–µ —Å–ª–æ–≤–∞—Ä—å –∏ –Ω–µ —Å–ø–∏—Å–æ–∫, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    fixtures = {"rides": [], "receipts": [], "cards": [], "tickets": {"next_id": 1001}}
    print("‚ö†Ô∏è fixtures.json –∏–º–µ–µ—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç, —Å–æ–∑–¥–∞–Ω–∞ –ø—É—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")

# –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ kb_data - —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å  
if isinstance(kb_data, list):
    kb_data = {"faq": []}
    print("‚ö†Ô∏è kb.json –∑–∞–≥—Ä—É–∂–µ–Ω –∫–∞–∫ —Å–ø–∏—Å–æ–∫, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—É—Å—Ç–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã"""
    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    
    text = emoji_pattern.sub('', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[^\w\s\-.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
    try:
        lang = detect(text)
        if lang in ['ru', 'kk']:
            return 'ru'  # –†—É—Å—Å–∫–∏–π/–∫–∞–∑–∞—Ö—Å–∫–∏–π
        elif lang == 'en':
            return 'en'
        else:
            return 'ru'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π
    except LangDetectException:
        return 'ru'

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
def classify_intent(text: str) -> tuple[str, float]:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    text_lower = text.lower()
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∑–∞–∫–∞–∑–∞ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å
    if any(phrase in text_lower for phrase in ['–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–∫–∞–∑', '–ø—Ä–µ–¥–∑–∞–∫–∞–∑', '–∑–∞—Ä–∞–Ω–µ–µ', '–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞—Ç—å']):
        return 'faq', 0.9  # –°—Ä–∞–∑—É –≤–æ–∑–≤—Ä–∞—â–∞–µ–º FAQ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö —Å–ª–æ–≤
    # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ FAQ —Å–ª–æ–≤–∞, –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º FAQ
    specific_faq_words = ['–Ω–∞—Ü–µ–Ω–∫–∞', '–¥–æ–ø–ª–∞—Ç–∞', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–º–æ—Ç–æ—á–∞—Å—ã', '–±–∞–ª–∞–Ω—Å', '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ']
    if any(word in text_lower for word in specific_faq_words):
        # –î–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–æ–π –±–æ–Ω—É—Å –¥–ª—è FAQ –∏ –æ–±–Ω—É–ª—è–µ–º cards
        faq_bonus = 10
        cards_penalty = -10  # –®—Ç—Ä–∞—Ñ –¥–ª—è cards
    else:
        faq_bonus = 0
        cards_penalty = 0
    
    # FAQ –∏–Ω—Ç–µ–Ω—Ç—ã (–≤–∫–ª—é—á–∞–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π)
    faq_keywords = ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—á–µ—Ç', '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç', 
                   '–ø—Ä–æ–º–æ–∫–æ–¥', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ', '–∫–æ–¥', '–≤–≤–µ—Å—Ç–∏',
                   '–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑',
                   '—Å–≤—è–∑–∞—Ç—å—Å—è', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–≤–æ–¥–∏—Ç–µ–ª—å', '–∫–æ–Ω—Ç–∞–∫—Ç',
                   '–Ω–µ –ø—Ä–∏–µ—Ö–∞–ª', '–æ–ø–æ–∑–¥–∞–ª', '–∂–¥–∞—Ç—å', '–ø—Ä–æ–±–ª–µ–º–∞',
                   '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–∫–∞–∑', '–ø—Ä–µ–¥–∑–∞–∫–∞–∑', '–∑–∞—Ä–∞–Ω–µ–µ', '–≤—Ä–µ–º—è',
                   '–∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞—Ç—å', '–≤—ã–∑–æ–≤', '–Ω–∞–∑–Ω–∞—á–∏—Ç—å –≤—Ä–µ–º—è',
                   # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ FAQ
                   '–Ω–∞—Ü–µ–Ω–∫–∞', '–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç', '–¥–æ–ø–ª–∞—Ç–∞', '–Ω–∞–¥–±–∞–≤–∫–∞', '—Å–ø—Ä–æ—Å', '–ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —Å–ø—Ä–æ—Å', '–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ',
                   '–∫–æ–º—Ñ–æ—Ä—Ç', '–∫–ª–∞—Å—Å', '–º–∞—à–∏–Ω–∞', '–ø—Ä–µ–º–∏—É–º', '–∫–∞–º—Ä–∏', '–¥–æ—Ä–æ–∂–µ', '—É–¥–æ–±—Å—Ç–≤–æ',
                   '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä', '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ', '–æ—Ü–µ–Ω–∫–∞',
                   '–¥–æ—Å—Ç–∞–≤–∫–∞', '–∑–∞–∫–∞–∑', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞', '–æ—Ç–∫—É–¥–∞', '–∫—É–¥–∞', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–ø–æ–ª—É—á–∞—Ç–µ–ª—å',
                   '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–∑–∞–∫–∞–∑—ã', '–ª–µ–Ω—Ç–∞ –∑–∞–∫–∞–∑–æ–≤', '–±–∞–ª–∞–Ω—Å', 'id', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–±–Ω—ã–π',
                   '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ', 'qiwi', 'cyberplat', '–∫–∞—Å—Å–∞24', '–µ–¥–∏–Ω–∏—Ü–∞', 'kaspi', 'visa', 'mastercard',
                   '–º–æ—Ç–æ—á–∞—Å—ã', '–º–∏–Ω—É—Ç—ã', '–ø–æ–µ–∑–¥–∫–∞', '–≤—Ä–µ–º—è', '—Ç–∞—Ä–∏—Ñ', '–¥–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–∫–∞–∑—ã',
                   '–æ–∂–∏–¥–∞–Ω–∏–µ', '–ø–æ–µ—Ö–∞–ª–∏', '–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å', '–∑–∞–∫–∞–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω', '–∫–ª–∏–µ–Ω—Ç', '–∞–¥—Ä–µ—Å',
                   '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ', 'google play', 'app store', 'gps', '–≤—ã–ª–µ—Ç–∞–µ—Ç', '–∑–∞–≤–∏—Å–∞–µ—Ç',
                   '—Ä–∞–±–æ—Ç–∞–µ—Ç', '–≥—Ä—É–∑', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ', '—Ç–æ–≤–∞—Ä—ã', '–¥–æ–∫—É–º–µ–Ω—Ç—ã']
    
    # –°—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏ (—Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞)
    ride_status_keywords = ['–≥–¥–µ –≤–æ–¥–∏—Ç–µ–ª—å', '—Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏', '–∞–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–µ–∑–¥–∫–∏']
    
    # –ß–µ–∫ (—Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞)
    receipt_keywords = ['—á–µ–∫', '–∫–≤–∏—Ç–∞–Ω—Ü–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–ø—Ä–∞–≤–∫–∞', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å —á–µ–∫']
    
    # –ö–∞—Ä—Ç—ã (—Ç–æ–ª—å–∫–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞, –∏—Å–∫–ª—é—á–∞–µ–º "–æ–ø–ª–∞—Ç–∞" –∏ "–¥–æ–ø–ª–∞—Ç–∞")
    cards_keywords = ['–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–æ—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ä—Ç–∞', '–ø—Ä–∏–≤—è–∑–∞—Ç—å –∫–∞—Ä—Ç—É', '–æ—Å–Ω–æ–≤–Ω–∞—è']
    
    # –ñ–∞–ª–æ–±—ã
    complaint_keywords = ['—Å–ø–∏—Å–∞–ª–∏ –¥–≤–∞–∂–¥—ã', '–¥–≤–æ–π–Ω–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ', '–∂–∞–ª–æ–±–∞', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ —Å–ø–∏—Å–∞–ª–∏']
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    faq_score = sum(1 for keyword in faq_keywords if keyword in text_lower) + faq_bonus
    ride_score = sum(1 for keyword in ride_status_keywords if keyword in text_lower)
    receipt_score = sum(1 for keyword in receipt_keywords if keyword in text_lower)
    cards_score = sum(1 for keyword in cards_keywords if keyword in text_lower) + cards_penalty
    complaint_score = sum(1 for keyword in complaint_keywords if keyword in text_lower)
    
    
    scores = {
        'faq': faq_score,
        'ride_status': ride_score,
        'receipt': receipt_score,
        'cards': cards_score,
        'complaint': complaint_score
    }
    
    max_intent = max(scores, key=scores.get)
    max_score = scores[max_intent]
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —á–µ—Ç–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞, —Å—á–∏—Ç–∞–µ–º FAQ
    if max_score == 0:
        return 'faq', 0.5
    
    confidence = min(max_score / 3.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 1.0
    return max_intent, confidence

# –ú–æ–∫–∏ –¥–ª—è —Ç–∞–∫—Å–∏
def get_ride_status(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    rides = fixtures.get("rides", [])
    user_rides = [ride for ride in rides if ride.get("user_id") == user_id]
    
    if not user_rides:
        return {"status": "no_rides", "message": "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"}
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ–µ–∑–¥–∫—É
    return user_rides[-1]

def send_receipt(user_id: str) -> Dict[str, Any]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–µ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
    receipts = fixtures.get("receipts", [])
    user_receipts = [receipt for receipt in receipts if receipt.get("user_id") == user_id]
    
    if not user_receipts:
        return {"status": "no_receipts", "message": "–£ –≤–∞—Å –Ω–µ—Ç —á–µ–∫–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"}
    
    latest_receipt = user_receipts[-1]
    return {
        "status": "sent",
        "receipt": latest_receipt,
        "message": f"–ß–µ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ email. –°—É–º–º–∞: {latest_receipt['amount']} —Ç–µ–Ω–≥–µ"
    }

def list_cards(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    cards = fixtures.get("cards", [])
    user_cards = [card for card in cards if card.get("user_id") == user_id]
    
    if not user_cards:
        return {"status": "no_cards", "message": "–£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç"}
    
    return {"status": "success", "cards": user_cards}

def escalate_to_human(user_id: str, description: str) -> Dict[str, Any]:
    """–≠—Å–∫–∞–ª–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    tickets = fixtures.get("tickets", {})
    next_id = tickets.get("next_id", 1001)
    
    new_ticket = {
        "ticket_id": f"TKT_{next_id}",
        "user_id": user_id,
        "subject": "–≠—Å–∫–∞–ª–∞—Ü–∏—è –æ—Ç –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
        "description": description,
        "status": "open",
        "created_at": datetime.now().isoformat(),
        "priority": "medium"
    }
    
    tickets["tickets"].append(new_ticket)
    tickets["next_id"] = next_id + 1
    
    return {
        "status": "escalated",
        "ticket_id": new_ticket["ticket_id"],
        "message": f"–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–¥–∞–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É. –ù–æ–º–µ—Ä —Ç–∏–∫–µ—Ç–∞: {new_ticket['ticket_id']}"
    }

# –¢–†–ï–•–£–†–û–í–ù–ï–í–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–û–ò–°–ö–ê
def search_with_three_filters(query: str, faq_items: List[Dict]) -> List[tuple]:
    """
    –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏:
    1. question_variations (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.5)
    2. keywords (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.3) 
    3. answer content (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.2)
    """
    if not query or not faq_items:
        return []
    
    results = []
    query_lower = query.lower().strip()
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
    query_words = re.sub(r'[^\w\s]', ' ', query_lower).split()
    query_words = [word for word in query_words if len(word) > 2]
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö FAQ
    # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–æ–¥–µ—Ä–∂–∏—Ç —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞, –∏—Å–∫–ª—é—á–∞–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ FAQ
    if '—Ä–∞—Å—Ü–µ–Ω–∫–∞' in query_lower:
        # –î–ª—è "—Ä–∞—Å—Ü–µ–Ω–∫–∞" –∏—Å–∫–ª—é—á–∞–µ–º FAQ –æ –Ω–∞—Ü–µ–Ω–∫–µ
        faq_items = [item for item in faq_items if '–Ω–∞—Ü–µ–Ω–∫' not in item.get('question', '').lower()]
    elif '–Ω–∞—Ü–µ–Ω–∫–∞' in query_lower or '–¥–æ–ø–ª–∞—Ç–∞' in query_lower:
        # –î–ª—è "–Ω–∞—Ü–µ–Ω–∫–∞" –∏ "–¥–æ–ø–ª–∞—Ç–∞" –∏—Å–∫–ª—é—á–∞–µ–º FAQ –æ —Ä–∞—Å—Ü–µ–Ω–∫–µ
        faq_items = [item for item in faq_items if '—Ä–∞—Å—Ü–µ–Ω–∫' not in item.get('question', '').lower()]
    
    for item in faq_items:
        # Filter 1: Question Variations (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.5)
        variations_score = search_question_variations(query_lower, item.get("question_variations", []))
        
        # Filter 2: Keywords (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.3)
        keywords_score = search_keywords(query_words, item.get("keywords", []))
        
        # Filter 3: Answer Content (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0.2)
        answer_score = search_answer_content(query_words, item.get("answer", ""))
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
        total_score = (
            variations_score * 0.5 +    # 50% - –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            keywords_score * 0.3 +      # 30% - —Å—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
            answer_score * 0.2          # 20% - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        )
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –¥–ª—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏—è
        if total_score >= 0.3:  # 30% –º–∏–Ω–∏–º—É–º
            results.append((item, total_score, {
                'variations': variations_score,
                'keywords': keywords_score, 
                'answer': answer_score
            }))
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –æ–±—â–µ–≥–æ –±–∞–ª–ª–∞
    return sorted(results, key=lambda x: x[1], reverse=True)


def search_question_variations(query: str, variations: List[str]) -> float:
    """Filter 1: –ü–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –≤–æ–ø—Ä–æ—Å–æ–≤ (–≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)"""
    if not variations:
        return 0.0
    
    best_score = 0.0
    
    for variation in variations:
        variation_lower = variation.lower()
        
        # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if query == variation_lower:
            return 1.0
        
        # –û—á–µ–Ω—å –±–ª–∏–∑–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (90%+)
        similarity = calculate_text_similarity(query, variation_lower)
        if similarity > 0.9:
            best_score = max(best_score, similarity)
        
        # –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (80%+)
        elif similarity > 0.8:
            best_score = max(best_score, similarity * 0.9)
        
        # –£–º–µ—Ä–µ–Ω–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (70%+)
        elif similarity > 0.7:
            best_score = max(best_score, similarity * 0.7)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
        if query in variation_lower or variation_lower in query:
            partial_score = min(len(query), len(variation_lower)) / max(len(query), len(variation_lower))
            best_score = max(best_score, partial_score * 0.6)
    
    return best_score


def search_keywords(query_words: List[str], keywords: List[str]) -> float:
    """Filter 2: –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π"""
    if not query_words or not keywords:
        return 0.0
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    priority_keywords = {
        # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        '–Ω–∞—Ü–µ–Ω–∫–∞': 15, '–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç': 15, '–¥–æ–ø–ª–∞—Ç–∞': 15, '–Ω–∞–¥–±–∞–≤–∫–∞': 15, '—Å–ø—Ä–æ—Å': 15, '–ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —Å–ø—Ä–æ—Å': 15, '–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ': 15,
        '–∫–æ–º—Ñ–æ—Ä—Ç': 15, '–∫–∞–º—Ä–∏': 15, '–ø—Ä–µ–º–∏—É–º': 15, '–∫–ª–∞—Å—Å': 15, '–º–∞—à–∏–Ω–∞': 15, '–¥–æ—Ä–æ–∂–µ': 15, '—É–¥–æ–±—Å—Ç–≤–æ': 15,
        '–º–æ—Ç–æ—á–∞—Å—ã': 15, '–º–∏–Ω—É—Ç—ã': 15, '–ø–æ–µ–∑–¥–∫–∞': 15, '–≤—Ä–µ–º—è': 15, '–¥–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–∫–∞–∑—ã': 15,
        '–±–∞–ª–∞–Ω—Å': 15, '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ': 15, 'qiwi': 15, 'cyberplat': 15, '–∫–∞—Å—Å–∞24': 15, '–µ–¥–∏–Ω–∏—Ü–∞': 15, 'kaspi': 15, 'visa': 15, 'mastercard': 15,
        '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': 15, 'google play': 15, 'app store': 15, 'gps': 15, '–≤—ã–ª–µ—Ç–∞–µ—Ç': 15, '–∑–∞–≤–∏—Å–∞–µ—Ç': 15,
        '–≤–æ–¥–∏—Ç–µ–ª—å': 15, '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è': 15, '–ª–µ–Ω—Ç–∞ –∑–∞–∫–∞–∑–æ–≤': 15, '–∑–∞–∫–∞–∑—ã': 15, 'id': 15, '–∫–ª–∏–µ–Ω—Ç': 15, '–ø—Ä–æ–±–Ω—ã–π': 15,
        
        # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ä–∞—Å—Ü–µ–Ω–∫–∏
        '—Ä–∞—Å—Ü–µ–Ω–∫–∞': 20, '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä': 20, '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä': 20, '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ': 20, '–æ—Ü–µ–Ω–∫–∞': 20,
        
        # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å–ª–æ–≤–∞
        '–¥–æ—Å—Ç–∞–≤–∫–∞': 15, '–∫—É—Ä—å–µ—Ä': 15, '–ø–æ—Å—ã–ª–∫–∞': 15, '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å': 15, '–∑–∞–∫–∞–∑': 15, '–æ—Ç–∫—É–¥–∞': 15, '–∫—É–¥–∞': 15, '—Ç–µ–ª–µ—Ñ–æ–Ω': 15, '–ø–æ–ª—É—á–∞—Ç–µ–ª—å': 15,
        '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∑–∞–∫–∞–∑': 15, '–ø—Ä–µ–¥–∑–∞–∫–∞–∑': 15, '–∑–∞—Ä–∞–Ω–µ–µ': 15,
        '–æ–∂–∏–¥–∞–Ω–∏–µ': 15, '–ø–æ–µ—Ö–∞–ª–∏': 15, '–æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å': 15, '–∑–∞–∫–∞–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω': 15, '–∫–ª–∏–µ–Ω—Ç': 15, '–∞–¥—Ä–µ—Å': 15,
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': 15, '–≥—Ä—É–∑': 15, '—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ': 15, '—Ç–æ–≤–∞—Ä—ã': 15, '–¥–æ–∫—É–º–µ–Ω—Ç—ã': 15,
        
        # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - –æ–±—â–∏–µ —Å–ª–æ–≤–∞ (–º–æ–≥—É—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤–∞—Ç—å)
        '—Ü–µ–Ω–∞': 3, '—Å—Ç–æ–∏–º–æ—Å—Ç—å': 3, '—Ç–∞—Ä–∏—Ñ': 3
    }
    
    total_score = 0.0
    max_possible = len(keywords) * 20  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
    
    for keyword in keywords:
        keyword_lower = keyword.lower().strip()
        keyword_root = extract_word_root(keyword_lower)
        max_similarity = 0
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        keyword_priority = priority_keywords.get(keyword_lower, 5)  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 5
        
        for word in query_words:
            word_lower = word.lower().strip()
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            if word_lower == keyword_lower:
                total_score += keyword_priority
                max_similarity = 1.0
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
            similarity = calculate_word_similarity(word_lower, keyword_lower)
            max_similarity = max(max_similarity, similarity)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–æ—Ä–Ω–µ–º
            word_root = extract_word_root(word_lower)
            root_similarity = calculate_word_similarity(word_root, keyword_root)
            max_similarity = max(max_similarity, root_similarity)
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –≤ –±–∞–ª–ª—ã —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
        if max_similarity >= 1.0:      # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            total_score += keyword_priority
        elif max_similarity >= 0.9:    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
            total_score += keyword_priority * 0.9
        elif max_similarity >= 0.8:    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
            total_score += keyword_priority * 0.8
        elif max_similarity >= 0.7:    # –•–æ—Ä–æ—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += keyword_priority * 0.7
        elif max_similarity >= 0.6:    # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += keyword_priority * 0.5
        elif max_similarity >= 0.5:    # –°–ª–∞–±–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += keyword_priority * 0.3
        elif max_similarity >= 0.4:    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            total_score += keyword_priority * 0.1
    
    return total_score / max_possible if max_possible > 0 else 0.0


def search_answer_content(query_words: List[str], answer: str) -> float:
    """Filter 3: –ü–æ–∏—Å–∫ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É –æ—Ç–≤–µ—Ç–æ–≤"""
    if not query_words or not answer:
        return 0.0
    
    answer_lower = answer.lower()
    answer_words = re.sub(r'[^\w\s]', ' ', answer_lower).split()
    answer_words = [word for word in answer_words if len(word) > 2]
    
    if not answer_words:
        return 0.0
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç
    common_words = set(query_words).intersection(set(answer_words))
    if not common_words:
        return 0.0
    
    # –ü—Ä–æ—Å—Ç–æ–π TF-IDF
    tf_score = len(common_words) / len(answer_words)
    idf_score = 1 + (len(common_words) / len(query_words))
    tfidf_score = tf_score * idf_score
    
    # –ë–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –≤ –Ω–∞—á–∞–ª–µ –æ—Ç–≤–µ—Ç–∞
    position_bonus = 0
    for i, word in enumerate(answer_words[:10]):  # –ü–µ—Ä–≤—ã–µ 10 —Å–ª–æ–≤
        if word in query_words:
            position_bonus += (10 - i) / 10 * 0.1
    
    # –ë–æ–Ω—É—Å –∑–∞ —Ñ—Ä–∞–∑–æ–≤—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
    phrase_bonus = 0
    for i in range(len(query_words) - 1):
        phrase = f"{query_words[i]} {query_words[i+1]}"
        if phrase in answer_lower:
            phrase_bonus += 0.2
    
    return min(1.0, tfidf_score + position_bonus + phrase_bonus)


def calculate_text_similarity(text1: str, text2: str) -> float:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏"""
    if not text1 or not text2:
        return 0.0
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if text1 == text2:
        return 1.0
    
    # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    distance = levenshtein_distance(text1, text2)
    max_len = max(len(text1), len(text2))
    similarity = 1 - (distance / max_len)
    
    return max(0.0, similarity)


def calculate_word_similarity(word1, word2):
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
    if not word1 or not word2:
        return 0
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–∞
    w1, w2 = word1.lower(), word2.lower()
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if w1 == w2:
        return 1.0
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ—Ä–Ω–∏
    root1, root2 = extract_word_root(w1), extract_word_root(w2)
    if root1 == root2:
        return 0.9
    
    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
    if len(root1) >= 4 and len(root2) >= 4:
        common_len = 0
        min_len = min(len(root1), len(root2))
        for i in range(min_len):
            if root1[i] == root2[i]:
                common_len += 1
            else:
                break
        
        if common_len >= 4:
            return 0.7 + (common_len / min_len) * 0.2
    
    # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫
    def levenshtein_distance(s1, s2):
        if len(s1) < len(s2):
            return levenshtein_distance(s2, s1)
        if len(s2) == 0:
            return len(s1)
        
        previous_row = list(range(len(s2) + 1))
        for i, c1 in enumerate(s1):
            current_row = [i + 1]
            for j, c2 in enumerate(s2):
                insertions = previous_row[j + 1] + 1
                deletions = current_row[j] + 1
                substitutions = previous_row[j] + (c1 != c2)
                current_row.append(min(insertions, deletions, substitutions))
            previous_row = current_row
        
        return previous_row[-1]
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏
    if len(w1) >= 3 and len(w2) >= 3:
        distance = levenshtein_distance(w1, w2)
        max_len = max(len(w1), len(w2))
        similarity = 1 - (distance / max_len)
        
        if similarity > 0.6:  # 60% —Å—Ö–æ–∂–µ—Å—Ç–∏
            return similarity * 0.5
    
    # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
    phonetic_map = {
        '–∫': ['–≥', '—Ö', '–∫'], '–ø': ['–±', '–ø'], '—Ç': ['–¥', '—Ç'],
        '—Å': ['–∑', '—Ü', '—Å'], '—Ñ': ['–≤', '—Ñ'], '—à': ['—â', '–∂', '—à'],
        '—á': ['—â', '—á'], '—Ä': ['—Ä', '–ª'], '–ª': ['—Ä', '–ª']
    }
    
    if len(w1) >= 3 and len(w2) >= 3:
        phonetic_score = 0
        min_len = min(len(w1), len(w2))
        
        for i in range(min_len):
            if w1[i] == w2[i]:
                phonetic_score += 1
            elif w1[i] in phonetic_map.get(w2[i], []) or w2[i] in phonetic_map.get(w1[i], []):
                phonetic_score += 0.7
        
        phonetic_similarity = phonetic_score / min_len
        if phonetic_similarity > 0.7:
            return phonetic_similarity * 0.4
    
    return 0


def extract_word_root(word):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –æ–∫–æ–Ω—á–∞–Ω–∏—è, —Å—É—Ñ—Ñ–∏–∫—Å—ã –∏ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏"""
    if len(word) < 4:
        return word
    
    root = word.lower()
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤
    special_cases = {
        '–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–∏': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫—É': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–µ': '–¥–æ—Å—Ç–∞–≤',
        '–¥–æ—Å—Ç–∞–≤—â–∏–∫': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å': '–¥–æ—Å—Ç–∞–≤',
        '–ø–µ—Ä–µ–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–ø–æ–¥–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤',
        '–≤–æ–¥–∏—Ç–µ–ª—å': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª–∏': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç—å': '–≤–æ–¥–∏',
        '–≤–æ–¥–∏—Ç': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–æ': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∏': '–≤–æ–¥–∏',
        '–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—á–∏–∫': '–∑–∞–∫–∞–∑',
        '–∑–∞–∫–∞–∑—á–∏—Ü–∞': '–∑–∞–∫–∞–∑', '–ø–µ—Ä–µ–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–ø–æ–¥–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑',
        '—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': '—Ü–µ–Ω', '–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω', '–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω',
        '–∫–∞—Ä—Ç–æ—á–∫–∞': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ': '–∫–∞—Ä—Ç',
        '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞': '–±–∞–ª–∞–Ω—Å', '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å': '–±–∞–ª–∞–Ω—Å',
        '—Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è': '—Ç–∞—Ä–∏—Ñ', '—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å': '—Ç–∞—Ä–∏—Ñ'
    }
    
    if root in special_cases:
        return special_cases[root]
    
    # –†—É—Å—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    russian_endings = [
        '–æ–≤–∞—Ç—å—Å—è', '–∏–≤–∞—Ç—å—Å—è', '–µ–≤–∞—Ç—å—Å—è', '–∞—Ç—å—Å—è', '–∏—Ç—å—Å—è', '–µ—Ç—å—Å—è', '—É—Ç—å—Å—è',
        '–æ–≤–∞–Ω–∏–µ', '–∏—Ä–æ–≤–∞–Ω–∏–µ', '–µ–≤–∞–Ω–∏–µ', '–∞–Ω–∏–µ', '–µ–Ω–∏–µ', '–µ–Ω–∏–µ',
        '—Å–∫–∏–π', '—Å–∫–∞—è', '—Å–∫–æ–µ', '—Å–∫–∏–µ', '—Å–∫–æ–≥–æ', '—Å–∫–æ–π', '—Å–∫—É—é', '—Å–∫–∏–º', '—Å–∫–æ–º', '—Å–∫–∏—Ö', '—Å–∫–∏–º–∏',
        '–æ—Å—Ç—å', '–µ—Å—Ç—å', '—Å—Ç–≤–æ', '—Ç–µ–ª—å', '—Ç–µ–ª—å–Ω–∏—Ü–∞',
        '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–≥–æ', '–æ–π', '—É—é', '—ã–º', '–æ–º', '–∏—Ö', '—ã–º–∏',
        '–∏—è', '–∏–π', '–∏–µ', '–∏—é', '–∏–µ–º', '–∏–∏', '–∏—è–º–∏', '–∏—è—Ö',
        '–∏–∫', '–∏—Ü', '–∏—á', '–∏—â', '–Ω–∏–∫', '–Ω–∏—Ü', '—â–∏–∫', '—â–∏—Ü',
        '–∞', '–æ', '—É', '—ã', '–∏', '–µ', '—é', '–µ–º', '–æ–º', '–∞—Ö', '—è–º–∏', '—è—Ö',
        '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏', '–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã'
    ]
    
    # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏ (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    prefixes = [
        '–ø–µ—Ä–µ', '–ø—Ä–µ–¥', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—Ä–∞–∑', '—Ä–∞—Å', '–∏–∑', '–∏—Å',
        '–æ—Ç', '–æ–±', '–≤', '–≤–æ', '–∑–∞', '–Ω–∞', '–¥–æ', '–ø–æ', '—Å–æ', '–≤—ã', '—É'
    ]
    
    # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    for ending in sorted(russian_endings, key=len, reverse=True):
        if root.endswith(ending) and len(root) > len(ending) + 2:
            root = root[:-len(ending)]
            break
    
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    for prefix in sorted(prefixes, key=len, reverse=True):
        if root.startswith(prefix) and len(root) > len(prefix) + 3:
            root = root[len(prefix):]
            break
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ—Ä–Ω—è
    return root if len(root) >= 3 else word.lower()


def search_faq(text: str) -> Optional[Dict[str, Any]]:
    """–¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π FAQ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""
    if not text or not kb_data:
        return None
    
    faq_items = kb_data.get("faq", [])
    if not faq_items:
        return None
    
    logger.info(f"üîç –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –¥–ª—è: '{text}'")
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞
    results = search_with_three_filters(text, faq_items)
    
    if results:
        best_match, best_score, filter_scores = results[0]
        
        logger.info(f"üéØ –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {best_score:.3f}")
        logger.info(f"   üìã Filter 1 (variations): {filter_scores['variations']:.3f}")
        logger.info(f"   üîë Filter 2 (keywords): {filter_scores['keywords']:.3f}")
        logger.info(f"   üìÑ Filter 3 (answer): {filter_scores['answer']:.3f}")
        
        # –ü–æ—Ä–æ–≥–∏ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è
        if best_score >= 0.7:  # –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            logger.info(f"‚úÖ –û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {best_score:.3f}")
            return best_match
        elif best_score >= 0.5:  # –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            logger.info(f"‚úÖ –•–æ—Ä–æ—à–µ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {best_score:.3f}")
            return best_match
        elif best_score >= 0.3:  # –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            logger.info(f"‚ö†Ô∏è –£–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {best_score:.3f}")
            return best_match
    
    # Fallback –∫ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–º—É –∞–Ω–∞–ª–∏–∑—É –µ—Å–ª–∏ —Ç—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    if MORPHOLOGY_AVAILABLE:
        try:
            result = enhance_classification_with_morphology(text, kb_data)
            confidence = result.get('confidence', 0)
            logger.info(f"üîç Fallback –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: confidence={confidence:.2f}")
            
            if result.get('matched_item') and confidence > 0.1:
                logger.info(f"‚úÖ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π fallback –Ω–∞–π–¥–µ–Ω: {confidence:.2f}")
                return result['matched_item']
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ fallback: {e}")
    
    logger.info(f"‚ùå –¢—Ä–µ—Ö—É—Ä–æ–≤–Ω–µ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    return None

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞ —Å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º"""
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    processed_text = preprocess_text(request.text)
    if not processed_text:
        raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
    detected_lang = detect_language(processed_text)
    final_locale = request.locale if request.locale in ['ru', 'kz', 'en'] else detected_lang
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞
    intent, confidence = classify_intent(processed_text)
    
    logger.info(f"User: {request.user_id}, Intent: {intent}, Confidence: {confidence}, Locale: {final_locale}")
    
    response_text = ""
    source = "kb"
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –∏–Ω—Ç–µ–Ω—Ç–∞–º
    if intent == "ride_status":
        result = get_ride_status(request.user_id)
        response_text = result.get("message", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏")
        
    elif intent == "receipt":
        result = send_receipt(request.user_id)
        response_text = result.get("message", "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —á–µ–∫")
        
    elif intent == "cards":
        result = list_cards(request.user_id)
        response_text = result.get("message", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç")
        
    elif intent == "complaint":
        result = escalate_to_human(request.user_id, processed_text)
        response_text = result.get("message", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–∏–∫–µ—Ç")
        
    else:  # FAQ
        # –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        faq_result = search_faq(processed_text)
        
        if faq_result:
            response_text = faq_result.get("answer", "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç")
            source = "kb"
        else:
            # Fallback –æ—Ç–≤–µ—Ç –¥–ª—è FAQ
            response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —Å–º–æ–≥ –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É."
            source = "fallback"
    
    logger.info(f"Response: {response_text[:100]}..., Source: {source}, Intent: {intent}, Confidence: {confidence}")
    
    return ChatResponse(
        response=response_text,
        intent=intent,
        confidence=confidence,
        source=source,
        timestamp=datetime.now().isoformat()
    )

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã
@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/webapp")
async def webapp():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ"""
    possible_paths = [
        "../webapp.html",
        "./webapp.html",
        "webapp.html"
    ]
    
    for path in possible_paths:
        if os.path.exists(path):
            return FileResponse(path)
    
    # Fallback HTML –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω
    fallback_html = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Taxi Support</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
    </head>
    <body>
        <h1>Taxi Support AI Assistant</h1>
        <p>–°–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç. –í–µ–±-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ –ø–æ–∑–∂–µ.</p>
    </body>
    </html>
    """
    
    from fastapi.responses import HTMLResponse
    return HTMLResponse(content=fallback_html)

@app.get("/ride-status/{user_id}")
async def get_ride_status_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏"""
    return get_ride_status(user_id)

@app.post("/send-receipt/{user_id}")
async def send_receipt_endpoint(user_id: str):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —á–µ–∫"""
    return send_receipt(user_id)

@app.get("/cards/{user_id}")
async def list_cards_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç"""
    return list_cards(user_id)

@app.post("/escalate/{user_id}")
async def escalate_endpoint(user_id: str, description: str):
    """–≠—Å–∫–∞–ª–∏—Ä–æ–≤–∞—Ç—å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    return escalate_to_human(user_id, description)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)