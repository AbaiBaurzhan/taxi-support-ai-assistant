import json
import re
import os
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from langdetect import detect
from langdetect.lang_detect_exception import LangDetectException

# –ò–º–ø–æ—Ä—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
try:
    from enhanced_morphological_analyzer import enhance_classification_with_morphology, enhanced_analyzer
    MORPHOLOGY_AVAILABLE = True
    print("‚úÖ –£–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
except ImportError:
    MORPHOLOGY_AVAILABLE = False
    print("‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Taxi Support AI Assistant", version="1.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã
app.mount("/static", StaticFiles(directory="."), name="static")

# –ú–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str  # "kb" –∏–ª–∏ "llm"
    timestamp: str

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
def load_json_file(filename: str) -> Dict[str, Any]:
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        logger.error(f"–§–∞–π–ª {filename} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return {}
    except json.JSONDecodeError as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –≤ {filename}: {e}")
        return {}

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
fixtures = load_json_file("fixtures.json")
kb_data = load_json_file("kb.json")

# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
def preprocess_text(text: str) -> str:
    """–£–±–∏—Ä–∞–µ—Ç —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã"""
    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏
    emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        "]+", flags=re.UNICODE)
    
    text = emoji_pattern.sub('', text)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
    text = re.sub(r'[^\w\s\-.,!?]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
def detect_language(text: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞"""
    try:
        lang = detect(text)
        if lang in ['ru', 'kk']:
            return 'ru'  # –†—É—Å—Å–∫–∏–π/–∫–∞–∑–∞—Ö—Å–∫–∏–π
        elif lang == 'en':
            return 'en'
        else:
            return 'ru'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ä—É—Å—Å–∫–∏–π
    except LangDetectException:
        return 'ru'

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–æ–≤
def classify_intent(text: str) -> tuple[str, float]:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    text_lower = text.lower()
    
    # FAQ –∏–Ω—Ç–µ–Ω—Ç—ã
    faq_keywords = ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—á–µ—Ç', '—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç', 
                   '–ø—Ä–æ–º–æ–∫–æ–¥', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ', '–∫–æ–¥', '–≤–≤–µ—Å—Ç–∏',
                   '–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑',
                   '—Å–≤—è–∑–∞—Ç—å—Å—è', '–ø–æ–∑–≤–æ–Ω–∏—Ç—å', '–≤–æ–¥–∏—Ç–µ–ª—å', '–∫–æ–Ω—Ç–∞–∫—Ç',
                   '–Ω–µ –ø—Ä–∏–µ—Ö–∞–ª', '–æ–ø–æ–∑–¥–∞–ª', '–∂–¥–∞—Ç—å', '–ø—Ä–æ–±–ª–µ–º–∞']
    
    # –°—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏
    ride_status_keywords = ['–≥–¥–µ –≤–æ–¥–∏—Ç–µ–ª—å', '—Å—Ç–∞—Ç—É—Å', '–ø–æ–µ–∑–¥–∫–∞', '–∑–∞–∫–∞–∑', '–æ–∂–∏–¥–∞–Ω–∏–µ']
    
    # –ß–µ–∫
    receipt_keywords = ['—á–µ–∫', '–∫–≤–∏—Ç–∞–Ω—Ü–∏—è', '–¥–æ–∫—É–º–µ–Ω—Ç', '—Å–ø—Ä–∞–≤–∫–∞']
    
    # –ö–∞—Ä—Ç—ã
    cards_keywords = ['–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–æ—Å–Ω–æ–≤–Ω–∞—è', '–ø–ª–∞—Ç–µ–∂', '–æ–ø–ª–∞—Ç–∞']
    
    # –ñ–∞–ª–æ–±—ã
    complaint_keywords = ['—Å–ø–∏—Å–∞–ª–∏ –¥–≤–∞–∂–¥—ã', '–¥–≤–æ–π–Ω–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ', '–∂–∞–ª–æ–±–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ']
    
    # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    faq_score = sum(1 for keyword in faq_keywords if keyword in text_lower)
    ride_score = sum(1 for keyword in ride_status_keywords if keyword in text_lower)
    receipt_score = sum(1 for keyword in receipt_keywords if keyword in text_lower)
    cards_score = sum(1 for keyword in cards_keywords if keyword in text_lower)
    complaint_score = sum(1 for keyword in complaint_keywords if keyword in text_lower)
    
    scores = {
        'faq': faq_score,
        'ride_status': ride_score,
        'receipt': receipt_score,
        'cards': cards_score,
        'complaint': complaint_score
    }
    
    max_intent = max(scores, key=scores.get)
    max_score = scores[max_intent]
    
    # –ï—Å–ª–∏ –Ω–µ—Ç —á–µ—Ç–∫–æ–≥–æ –∏–Ω—Ç–µ–Ω—Ç–∞, —Å—á–∏—Ç–∞–µ–º FAQ
    if max_score == 0:
        return 'faq', 0.5
    
    confidence = min(max_score / 3.0, 1.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 1.0
    return max_intent, confidence

# –ú–æ–∫–∏ –¥–ª—è —Ç–∞–∫—Å–∏
def get_ride_status(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_rides = fixtures.get("rides", {}).get(user_id, {})
    if not user_rides:
        return {"status": "no_rides", "message": "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"}
    
    return user_rides

def send_receipt(user_id: str) -> Dict[str, Any]:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —á–µ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
    user_receipts = fixtures.get("receipts", {}).get(user_id, [])
    if not user_receipts:
        return {"status": "no_receipts", "message": "–£ –≤–∞—Å –Ω–µ—Ç —á–µ–∫–æ–≤ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏"}
    
    latest_receipt = user_receipts[-1]
    return {
        "status": "sent",
        "receipt": latest_receipt,
        "message": f"–ß–µ–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –Ω–∞ email. –°—É–º–º–∞: {latest_receipt['amount']} —Ç–µ–Ω–≥–µ"
    }

def list_cards(user_id: str) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    user_cards = fixtures.get("cards", {}).get(user_id, [])
    if not user_cards:
        return {"status": "no_cards", "message": "–£ –≤–∞—Å –Ω–µ—Ç –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö –∫–∞—Ä—Ç"}
    
    return {"status": "success", "cards": user_cards}

def escalate_to_human(user_id: str, description: str) -> Dict[str, Any]:
    """–≠—Å–∫–∞–ª–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    tickets = fixtures.get("tickets", {})
    next_id = tickets.get("next_id", 1001)
    
    new_ticket = {
        "ticket_id": f"TKT_{next_id}",
        "user_id": user_id,
        "subject": "–≠—Å–∫–∞–ª–∞—Ü–∏—è –æ—Ç –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
        "description": description,
        "status": "open",
        "created_at": datetime.now().isoformat(),
        "priority": "medium"
    }
    
    tickets["tickets"].append(new_ticket)
    tickets["next_id"] = next_id + 1
    
    return {
        "status": "escalated",
        "ticket_id": new_ticket["ticket_id"],
        "message": f"–í–∞—à –∑–∞–ø—Ä–æ—Å –ø–µ—Ä–µ–¥–∞–Ω –æ–ø–µ—Ä–∞—Ç–æ—Ä—É. –ù–æ–º–µ—Ä —Ç–∏–∫–µ—Ç–∞: {new_ticket['ticket_id']}"
    }

# –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ FAQ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º
def search_faq(text: str) -> Optional[Dict[str, Any]]:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π FAQ —Å –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º"""
    if not text or not kb_data:
        return None
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
    if MORPHOLOGY_AVAILABLE:
        try:
            result = enhance_classification_with_morphology(text, kb_data)
            confidence = result.get('confidence', 0)
            logger.info(f"üîç –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑: confidence={confidence:.2f}, intent={result.get('intent', 'unknown')}")
            
            # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–æ –º–∏–Ω–∏–º—É–º–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            if result.get('matched_item') and confidence > 0.05:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥
                logger.info(f"‚úÖ –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω: {confidence:.2f}")
                return result['matched_item']
            elif result.get('matched_item'):
                logger.info(f"‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω, –Ω–æ –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            import traceback
            traceback.print_exc()
    else:
        logger.warning("‚ö†Ô∏è –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫")
    
    # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
    faq_items = kb_data.get("faq", [])
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    slang_replacements = {
        '—á–µ': '—á—Ç–æ',
        '—Ç–∞–º': '',
        '–ø–æ': '',
        '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç': '–∫–∞–∫',
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': '—Ä–∞–±–æ—Ç–∞',
        '–∫–∞–∫': '–∫–∞–∫',
        '–≥–¥–µ': '–≥–¥–µ',
        '—á—Ç–æ': '—á—Ç–æ',
        '–∫–∞–∫–∏–µ': '–∫–∞–∫–∏–µ',
        '–º–æ–∂–Ω–æ': '–º–æ–∂–Ω–æ',
        '–Ω—É–∂–Ω–æ': '–Ω—É–∂–Ω–æ',
        '–µ—Å—Ç—å': '–µ—Å—Ç—å',
        '–±—ã—Ç—å': '–±—ã—Ç—å',
        '–≤—Ä–µ–º—è': '–≤—Ä–µ–º—è',
        '–¥–µ–Ω—å–≥–∏': '–¥–µ–Ω—å–≥–∏',
        '—Ü–µ–Ω–∞': '—Ü–µ–Ω–∞',
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': '—Å—Ç–æ–∏–º–æ—Å—Ç—å'
    }
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    import re
    text_cleaned = re.sub(r'[^\w\s]', ' ', text_lower)
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    
    processed_text = text_lower
    for slang, replacement in slang_replacements.items():
        processed_text = processed_text.replace(slang, replacement)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    processed_text = ' '.join(processed_text.split())
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    def calculate_tfidf_score(query_words, doc_words):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not query_words or not doc_words:
            return 0
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        common_words = set(query_words).intersection(set(doc_words))
        if not common_words:
            return 0
        
        # –ü—Ä–æ—Å—Ç–æ–π TF-IDF: log(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤ / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        tf_score = len(common_words) / len(doc_words)
        idf_score = 1 + (len(common_words) / len(query_words))
        
        return tf_score * idf_score
    
    best_match = None
    best_score = 0
    
    for item in faq_items:
        score = 0
        
        # –£–õ–¨–¢–†–ê-–ì–ò–ë–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —É–º–Ω—ã–º stemming
        keywords = item.get("keywords", [])
        keyword_score = 0
        
        def calculate_word_similarity(word1, word2):
            """–í—ã—á–∏—Å–ª—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏"""
            if not word1 or not word2:
                return 0
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å–ª–æ–≤–∞
            w1, w2 = word1.lower(), word2.lower()
            
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            if w1 == w2:
                return 1.0
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ—Ä–Ω–∏
            root1, root2 = extract_word_root(w1), extract_word_root(w2)
            if root1 == root2:
                return 0.9
            
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
            if len(root1) >= 4 and len(root2) >= 4:
                common_len = 0
                min_len = min(len(root1), len(root2))
                for i in range(min_len):
                    if root1[i] == root2[i]:
                        common_len += 1
                    else:
                        break
                
                if common_len >= 4:
                    return 0.7 + (common_len / min_len) * 0.2
            
            # –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –æ–ø–µ—á–∞—Ç–æ–∫
            def levenshtein_distance(s1, s2):
                if len(s1) < len(s2):
                    return levenshtein_distance(s2, s1)
                if len(s2) == 0:
                    return len(s1)
                
                previous_row = list(range(len(s2) + 1))
                for i, c1 in enumerate(s1):
                    current_row = [i + 1]
                    for j, c2 in enumerate(s2):
                        insertions = previous_row[j + 1] + 1
                        deletions = current_row[j] + 1
                        substitutions = previous_row[j] + (c1 != c2)
                        current_row.append(min(insertions, deletions, substitutions))
                    previous_row = current_row
                
                return previous_row[-1]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–ø–µ—á–∞—Ç–∫–∏
            if len(w1) >= 3 and len(w2) >= 3:
                distance = levenshtein_distance(w1, w2)
                max_len = max(len(w1), len(w2))
                similarity = 1 - (distance / max_len)
                
                if similarity > 0.6:  # 60% —Å—Ö–æ–∂–µ—Å—Ç–∏
                    return similarity * 0.5
            
            # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
            phonetic_map = {
                '–∫': ['–≥', '—Ö', '–∫'], '–ø': ['–±', '–ø'], '—Ç': ['–¥', '—Ç'],
                '—Å': ['–∑', '—Ü', '—Å'], '—Ñ': ['–≤', '—Ñ'], '—à': ['—â', '–∂', '—à'],
                '—á': ['—â', '—á'], '—Ä': ['—Ä', '–ª'], '–ª': ['—Ä', '–ª']
            }
            
            if len(w1) >= 3 and len(w2) >= 3:
                phonetic_score = 0
                min_len = min(len(w1), len(w2))
                
                for i in range(min_len):
                    if w1[i] == w2[i]:
                        phonetic_score += 1
                    elif w1[i] in phonetic_map.get(w2[i], []) or w2[i] in phonetic_map.get(w1[i], []):
                        phonetic_score += 0.7
                
                phonetic_similarity = phonetic_score / min_len
                if phonetic_similarity > 0.7:
                    return phonetic_similarity * 0.4
            
            return 0
        
        def extract_word_root(word):
            """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –æ–∫–æ–Ω—á–∞–Ω–∏—è, —Å—É—Ñ—Ñ–∏–∫—Å—ã –∏ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏"""
            if len(word) < 4:
                return word
            
            root = word.lower()
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤
            special_cases = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–∏': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫—É': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–µ': '–¥–æ—Å—Ç–∞–≤',
                '–¥–æ—Å—Ç–∞–≤—â–∏–∫': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å': '–¥–æ—Å—Ç–∞–≤',
                '–ø–µ—Ä–µ–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–ø–æ–¥–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤',
                '–≤–æ–¥–∏—Ç–µ–ª—å': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª–∏': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç—å': '–≤–æ–¥–∏',
                '–≤–æ–¥–∏—Ç': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–æ': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∏': '–≤–æ–¥–∏',
                '–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—á–∏–∫': '–∑–∞–∫–∞–∑',
                '–∑–∞–∫–∞–∑—á–∏—Ü–∞': '–∑–∞–∫–∞–∑', '–ø–µ—Ä–µ–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–ø–æ–¥–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑',
                '—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': '—Ü–µ–Ω', '–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω', '–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω',
                '–∫–∞—Ä—Ç–æ—á–∫–∞': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ': '–∫–∞—Ä—Ç',
                '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞': '–±–∞–ª–∞–Ω—Å', '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å': '–±–∞–ª–∞–Ω—Å',
                '—Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è': '—Ç–∞—Ä–∏—Ñ', '—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å': '—Ç–∞—Ä–∏—Ñ'
            }
            
            if root in special_cases:
                return special_cases[root]
            
            # –†—É—Å—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            russian_endings = [
                '–æ–≤–∞—Ç—å—Å—è', '–∏–≤–∞—Ç—å—Å—è', '–µ–≤–∞—Ç—å—Å—è', '–∞—Ç—å—Å—è', '–∏—Ç—å—Å—è', '–µ—Ç—å—Å—è', '—É—Ç—å—Å—è',
                '–æ–≤–∞–Ω–∏–µ', '–∏—Ä–æ–≤–∞–Ω–∏–µ', '–µ–≤–∞–Ω–∏–µ', '–∞–Ω–∏–µ', '–µ–Ω–∏–µ', '–µ–Ω–∏–µ',
                '—Å–∫–∏–π', '—Å–∫–∞—è', '—Å–∫–æ–µ', '—Å–∫–∏–µ', '—Å–∫–æ–≥–æ', '—Å–∫–æ–π', '—Å–∫—É—é', '—Å–∫–∏–º', '—Å–∫–æ–º', '—Å–∫–∏—Ö', '—Å–∫–∏–º–∏',
                '–æ—Å—Ç—å', '–µ—Å—Ç—å', '—Å—Ç–≤–æ', '—Ç–µ–ª—å', '—Ç–µ–ª—å–Ω–∏—Ü–∞',
                '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–≥–æ', '–æ–π', '—É—é', '—ã–º', '–æ–º', '–∏—Ö', '—ã–º–∏',
                '–∏—è', '–∏–π', '–∏–µ', '–∏—é', '–∏–µ–º', '–∏–∏', '–∏—è–º–∏', '–∏—è—Ö',
                '–∏–∫', '–∏—Ü', '–∏—á', '–∏—â', '–Ω–∏–∫', '–Ω–∏—Ü', '—â–∏–∫', '—â–∏—Ü',
                '–∞', '–æ', '—É', '—ã', '–∏', '–µ', '—é', '–µ–º', '–æ–º', '–∞—Ö', '—è–º–∏', '—è—Ö',
                '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏', '–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã'
            ]
            
            # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏ (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            prefixes = [
                '–ø–µ—Ä–µ', '–ø—Ä–µ–¥', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—Ä–∞–∑', '—Ä–∞—Å', '–∏–∑', '–∏—Å',
                '–æ—Ç', '–æ–±', '–≤', '–≤–æ', '–∑–∞', '–Ω–∞', '–¥–æ', '–ø–æ', '—Å–æ', '–≤—ã', '—É'
            ]
            
            # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            for ending in russian_endings:
                if root.endswith(ending) and len(root) > len(ending) + 2:
                    root = root[:-len(ending)]
                    break
            
            # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
            for prefix in prefixes:
                if root.startswith(prefix) and len(root) > len(prefix) + 3:
                    root = root[len(prefix):]
                    break
            
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ—Ä–Ω—è
            return root if len(root) >= 3 else word.lower()
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            keyword_root = extract_word_root(keyword_lower)
            max_similarity = 0
            
            # –£–õ–¨–¢–†–ê-–ü–†–û–î–í–ò–ù–£–¢–´–ô –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Å–ª–æ–≤–∞–º –∑–∞–ø—Ä–æ—Å–∞
            for word in text_cleaned.split():
                if len(word) < 3:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
                    continue
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
                similarity = calculate_word_similarity(word, keyword_lower)
                max_similarity = max(max_similarity, similarity)
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –∫–æ—Ä–Ω–µ–º
                word_root = extract_word_root(word)
                root_similarity = calculate_word_similarity(word_root, keyword_root)
                max_similarity = max(max_similarity, root_similarity)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –≤ –±–∞–ª–ª—ã
            if max_similarity >= 1.0:      # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                keyword_score += 8
            elif max_similarity >= 0.9:    # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
                keyword_score += 7
            elif max_similarity >= 0.8:    # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
                keyword_score += 6
            elif max_similarity >= 0.7:    # –•–æ—Ä–æ—à–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 5
            elif max_similarity >= 0.6:    # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 3
            elif max_similarity >= 0.5:    # –°–ª–∞–±–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 2
            elif max_similarity >= 0.4:    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                keyword_score += 1
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –∑–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if max_similarity > 0:
                # –ë–æ–Ω—É—Å –∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                word_count = text_cleaned.count(keyword_lower)
                if word_count > 1:
                    keyword_score += word_count * 0.5
                
                # –ë–æ–Ω—É—Å –∑–∞ –ø–æ–∑–∏—Ü–∏—é —Å–ª–æ–≤–∞ (–Ω–∞—á–∞–ª–æ –≤–∞–∂–Ω–µ–µ)
                words = text_cleaned.split()
                for i, word in enumerate(words):
                    if calculate_word_similarity(word, keyword_lower) > 0.7:
                        position_bonus = 1.0 - (i / len(words)) * 0.5
                        keyword_score += position_bonus * 0.5
                        break
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ñ–æ—Ä–º–∞–º —Å–ª–æ–≤–∞ (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏)
            keyword_forms = [
                keyword_lower,
                keyword_lower + '–∞', keyword_lower + '–æ', keyword_lower + '—É', keyword_lower + '—ã', keyword_lower + '–∏',
                keyword_lower + '–∞–º', keyword_lower + '–∞–º–∏', keyword_lower + '–∞—Ö',
                keyword_lower + '–æ–π', keyword_lower + '—É—é', keyword_lower + '—ã–º', keyword_lower + '–æ–º',
                keyword_lower + '—ã–π', keyword_lower + '–∞—è', keyword_lower + '–æ–µ', keyword_lower + '—ã–µ',
                keyword_lower + '–æ–≥–æ', keyword_lower + '–æ–π', keyword_lower + '–∏—Ö', keyword_lower + '—ã–º–∏',
                keyword_lower[:-1] if keyword_lower.endswith('–∞') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞'
                keyword_lower[:-1] if keyword_lower.endswith('–æ') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–æ'
                keyword_lower[:-1] if keyword_lower.endswith('–∏') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∏'
                keyword_lower[:-2] if keyword_lower.endswith('—ã–π') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '—ã–π'
                keyword_lower[:-2] if keyword_lower.endswith('–∞—è') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞—è'
            ]
            
            for form in keyword_forms:
                if form in text_cleaned:
                    keyword_score += 3
                    found_by_root = True
                    break
            
            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
            
            # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º —Å–∏–Ω–æ–Ω–∏–º–∞–º
            extended_synonyms = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–¥–æ—Å—Ç–∞–≤–∫–∏', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å', '–¥–æ—Å—Ç–∞–≤—â–∏–∫', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞', 
                           '–∫—É—Ä—å–µ—Ä', '–∫—É—Ä—å–µ—Ä—Å–∫–∞—è', '–ø–æ—Å—ã–ª–∫–∞', '–ø–µ—Ä–µ–≤–æ–∑–∫–∞', '—Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞',
                           '–≥—Ä—É–∑', '–≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–∫–∞', '–ª–æ–≥–∏—Å—Ç–∏–∫–∞', '–æ—Ç–ø—Ä–∞–≤–∫–∞', '–ø–æ–ª—É—á–µ–Ω–∏–µ'],
                '—Ü–µ–Ω–∞': ['—Ü–µ–Ω–∞', '—Ü–µ–Ω—ã', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–ø—Ä–∞–π—Å', '–ø—Ä–µ–π—Å–∫—É—Ä–∞–Ω—Ç',
                        '—Å–µ–±–µ—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Å—Ç–æ–∏—Ç', '—Å—Ç–æ–∏—Ç—å', '–ø–ª–∞—Ç–∞', '–æ–ø–ª–∞—Ç–∞', '–¥–µ–Ω—å–≥–∏'],
                '–∫–∞—Ä—Ç–∞': ['–∫–∞—Ä—Ç–∞', '–∫–∞—Ä—Ç—ã', '–∫–∞—Ä—Ç–æ—á–∫–∞', '–ø–ª–∞—Å—Ç–∏–∫', '–ø–ª–∞—Ç–µ–∂', '–±–∞–Ω–∫–æ–≤—Å–∫–∞—è',
                         '–∫—Ä–µ–¥–∏—Ç–∫–∞', '–¥–µ–±–µ—Ç–∫–∞', '—Å–±–µ—Ä–±–∞–Ω–∫', 'visa', 'mastercard'],
                '–±–∞–ª–∞–Ω—Å': ['–±–∞–ª–∞–Ω—Å', '—Å—á–µ—Ç', '—Å—á–µ—Ç–∞', '–¥–µ–Ω—å–≥–∏', '—Å—Ä–µ–¥—Å—Ç–≤–∞', '–∫–∞–ø–∏—Ç–∞–ª', '—Ñ–∏–Ω–∞–Ω—Å—ã',
                          '–Ω–∞–ª–∏—á–∫–∞', '–Ω–∞–ª–∏—á–Ω—ã–µ', '–∫–æ–ø–µ–π–∫–∏', '—Ä—É–±–ª–∏', '—Ç–µ–Ω–≥–µ'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': ['–≤–æ–¥–∏—Ç–µ–ª—å', '–≤–æ–¥–∏—Ç–µ–ª–∏', '—à–æ—Ñ–µ—Ä', '—Ç–∞–∫—Å–∏—Å—Ç', '–ø–µ—Ä–µ–≤–æ–∑—á–∏–∫', '–º–∞—à–∏–Ω–∏—Å—Ç',
                            '—É–ø—Ä–∞–≤–ª—è—é—â–∏–π', '–∫–æ–Ω–¥—É–∫—Ç–æ—Ä', '–ø–∏–ª–æ—Ç'],
                '–∑–∞–∫–∞–∑': ['–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å', '–ø–æ–∫—É–ø–∫–∞', '–±—Ä–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ',
                         '—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ', '–ø—Ä–æ—Å—å–±–∞', '–∑–∞—è–≤–∫–∞']
            }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã –¥–ª—è –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
            if keyword_lower in extended_synonyms:
                for synonym in extended_synonyms[keyword_lower]:
                    for word in text_cleaned.split():
                        if len(word) >= 3:
                            synonym_similarity = calculate_word_similarity(word, synonym)
                            if synonym_similarity > 0.7:
                                keyword_score += synonym_similarity * 3
                                break
            
            # –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø–æ–∏—Å–∫
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ –ø–æ–Ω—è—Ç–∏—è
            related_concepts = {
                '–¥–æ—Å—Ç–∞–≤–∫–∞': ['–∞–¥—Ä–µ—Å', '–ø–æ–ª—É—á–∞—Ç–µ–ª—å', '–æ—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å', '–ø–æ—Å—ã–ª–∫–∞', '–ø–∞–∫–µ—Ç', '—Ç–æ–≤–∞—Ä'],
                '—Ü–µ–Ω–∞': ['–¥–µ—à–µ–≤–æ', '–¥–æ—Ä–æ–≥–æ', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ–∫–æ–¥', '–∞–∫—Ü–∏—è', '—Ä–∞—Å–ø—Ä–æ–¥–∞–∂–∞'],
                '–∫–∞—Ä—Ç–∞': ['–ø—Ä–∏–≤—è–∑–∞—Ç—å', '–ø—Ä–∏–≤—è–∑–∫–∞', '–æ–ø–ª–∞—Ç–∞', '—Å–ø–∏—Å–∞–Ω–∏–µ', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ'],
                '–±–∞–ª–∞–Ω—Å': ['–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ', '—Å–ø–∏—Å–∞—Ç—å', '—Å–ø–∏—Å–∞–Ω–∏–µ', '–∏—Å—Ç–æ—Ä–∏—è'],
                '–≤–æ–¥–∏—Ç–µ–ª—å': ['–º–∞—à–∏–Ω–∞', '–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '—Ç–∞–∫—Å–∏', '–ø–æ–µ–∑–¥–∫–∞', '–º–∞—Ä—à—Ä—É—Ç'],
                '–∑–∞–∫–∞–∑': ['—Å–¥–µ–ª–∞—Ç—å', '–æ—Ñ–æ—Ä–º–∏—Ç—å', '–æ—Ç–º–µ–Ω–∏—Ç—å', '–∏–∑–º–µ–Ω–∏—Ç—å', '—Å—Ç–∞—Ç—É—Å']
            }
            
            if keyword_lower in related_concepts:
                for concept in related_concepts[keyword_lower]:
                    for word in text_cleaned.split():
                        if len(word) >= 3:
                            concept_similarity = calculate_word_similarity(word, concept)
                            if concept_similarity > 0.8:
                                keyword_score += concept_similarity * 2
                                break
        
        # –ì–õ–£–ë–û–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤
        variations = item.get("question_variations", [])
        variation_score = 0
        query_words = text_cleaned.split()
        
        for variation in variations:
            variation_words = variation.lower().split()
            
            # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
            tfidf_score = calculate_tfidf_score(query_words, variation_words)
            variation_score += tfidf_score
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º (–¥–≤–∞ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            query_bigrams = set()
            for i in range(len(query_words) - 1):
                query_bigrams.add(f"{query_words[i]} {query_words[i+1]}")
            
            variation_bigrams = set()
            for i in range(len(variation_words) - 1):
                variation_bigrams.add(f"{variation_words[i]} {variation_words[i+1]}")
            
            bigram_matches = len(query_bigrams.intersection(variation_bigrams))
            variation_score += bigram_matches * 0.5
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ —Ç—Ä–∏–≥—Ä–∞–º–º–∞–º (—Ç—Ä–∏ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            if len(query_words) >= 3 and len(variation_words) >= 3:
                query_trigrams = set()
                for i in range(len(query_words) - 2):
                    query_trigrams.add(f"{query_words[i]} {query_words[i+1]} {query_words[i+2]}")
                
                variation_trigrams = set()
                for i in range(len(variation_words) - 2):
                    variation_trigrams.add(f"{variation_words[i]} {variation_words[i+1]} {variation_words[i+2]}")
                
                trigram_matches = len(query_trigrams.intersection(variation_trigrams))
                variation_score += trigram_matches * 1.0
        
        # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É
        main_question = item.get("question", "").lower()
        main_question_words = main_question.split()
        
        # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
        main_score = calculate_tfidf_score(query_words, main_question_words) * 2
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
        question_phrases = [
            "–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º",
            "–º–æ–∂–Ω–æ", "–Ω—É–∂–Ω–æ", "–µ—Å—Ç—å", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–¥–µ–ª–∞—Ç—å"
        ]
        
        for phrase in question_phrases:
            if phrase in text_cleaned and phrase in main_question:
                main_score += 0.5
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å–ª–æ–≤ (–≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ—Ä—è–¥–∫–∞
        ordered_matches = 0
        for i, word in enumerate(query_words):
            if word in main_question_words:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤–æ–ø—Ä–æ—Å–µ
                for j, q_word in enumerate(main_question_words):
                    if word == q_word:
                        # –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –±–∞–ª–ª
                        position_bonus = 1.0 - (abs(i - j) * 0.1)
                        ordered_matches += max(0.1, position_bonus)
                        break
        
        main_score += ordered_matches * 0.3
        
        # 4. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        important_words = ["–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ç–∞—Ä–∏—Ñ", "–∫–∞—Ä—Ç–∞", "–±–∞–ª–∞–Ω—Å", "–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–∫–∞–∑"]
        for word in important_words:
            if word in text_cleaned and word in main_question:
                main_score += 1.0  # –í—ã—Å–æ–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        
        # –£–õ–¨–¢–†–ê-–ü–†–û–î–í–ò–ù–£–¢–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–ª–æ–≤
        max_possible_keywords = len(keywords) * 8  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        normalized_keyword_score = keyword_score / max_possible_keywords if max_possible_keywords > 0 else 0
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        score = (
            normalized_keyword_score * 10.0 +   # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ = –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
            variation_score * 2.0 +             # –í–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            main_score * 4.0                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        )
        
        # –ú–ù–û–ñ–ï–°–¢–í–ï–ù–ù–´–ï –±–æ–Ω—É—Å—ã –∑–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        bonus_multiplier = 1.0
        
        if keyword_score > 0 and variation_score > 0 and main_score > 0:
            bonus_multiplier *= 1.3  # 30% –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        
        if keyword_score > len(keywords) * 5:  # –í—ã—Å–æ–∫–∏–π –±–∞–ª–ª –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            bonus_multiplier *= 1.2  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π 20% –±–æ–Ω—É—Å
        
        if variation_score > 2.0:  # –•–æ—Ä–æ—à–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
            bonus_multiplier *= 1.1  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π 10% –±–æ–Ω—É—Å
        
        score *= bonus_multiplier
        
        # –§–ò–ù–ê–õ–¨–ù–´–ô –±–æ–Ω—É—Å –∑–∞ –∫–∞—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        if score > 5.0:  # –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∏–π –æ–±—â–∏–π –±–∞–ª–ª
            score += 1.0  # –ê–±—Å–æ–ª—é—Ç–Ω—ã–π –±–æ–Ω—É—Å
        
        if score > best_score:
            best_score = score
            best_match = item
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–∞–ª–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–π
    if best_score >= 0.5:  # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        logger.info(f"‚úÖ Fallback –ø–æ–∏—Å–∫ –Ω–∞–π–¥–µ–Ω: {best_score:.3f}")
        if best_match:
            logger.info(f"üìã –ù–∞–π–¥–µ–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {best_match.get('question', '')[:50]}...")
        return best_match
    
    logger.info(f"‚ùå Fallback –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞—à–µ–ª —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π (–ª—É—á—à–∏–π –±–∞–ª–ª: {best_score:.3f})")
    return None

# –û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç
@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞ —Å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º"""
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    processed_text = preprocess_text(request.text)
    if not processed_text:
        raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —è–∑—ã–∫–∞
    detected_lang = detect_language(processed_text)
    final_locale = request.locale if request.locale in ['ru', 'kz', 'en'] else detected_lang
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–Ω—Ç–µ–Ω—Ç–∞
    intent, confidence = classify_intent(processed_text)
    
    logger.info(f"User: {request.user_id}, Intent: {intent}, Confidence: {confidence}, Locale: {final_locale}")
    
    response_text = ""
    source = "llm"
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –∏–Ω—Ç–µ–Ω—Ç—É
    if intent == "faq":
        faq_result = search_faq(processed_text)
        if faq_result:
            response_text = faq_result["answer"]
            source = "kb"
            confidence = 0.9
        else:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ FAQ, –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
            # LLM –æ—Ç–∫–ª—é—á–µ–Ω
            response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
    
    elif intent == "ride_status":
        ride_data = get_ride_status(request.user_id)
        if ride_data.get("status") == "no_rides":
            response_text = "–£ –≤–∞—Å –Ω–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–µ–∑–¥–æ–∫"
        else:
            driver = ride_data.get("driver", {})
            response_text = f"–í–∞—à–∞ –ø–æ–µ–∑–¥–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ. –í–æ–¥–∏—Ç–µ–ª—å: {driver.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –º–∞—à–∏–Ω–∞: {driver.get('car', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}, –Ω–æ–º–µ—Ä: {driver.get('plate', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}. –û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è –ø—Ä–∏–±—ã—Ç–∏—è: {ride_data.get('estimated_arrival', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}"
        source = "kb"
        confidence = 0.95
    
    elif intent == "receipt":
        receipt_data = send_receipt(request.user_id)
        response_text = receipt_data["message"]
        source = "kb"
        confidence = 0.95
    
    elif intent == "cards":
        cards_data = list_cards(request.user_id)
        if cards_data["status"] == "success":
            cards = cards_data["cards"]
            primary_card = next((card for card in cards if card.get("is_primary")), None)
            response_text = f"–í–∞—à–∏ –∫–∞—Ä—Ç—ã: {', '.join([f'{card['type']} ****{card['last_four']}' for card in cards])}. –û—Å–Ω–æ–≤–Ω–∞—è –∫–∞—Ä—Ç–∞: {primary_card['type']} ****{primary_card['last_four'] if primary_card else '–ù–µ –≤—ã–±—Ä–∞–Ω–∞'}"
        else:
            response_text = cards_data["message"]
        source = "kb"
        confidence = 0.95
    
    elif intent == "complaint":
        escalation_data = escalate_to_human(request.user_id, processed_text)
        response_text = escalation_data["message"]
        source = "kb"
        confidence = 0.95
    
    else:
        # –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º LLM
        # LLM –æ—Ç–∫–ª—é—á–µ–Ω
        response_text = "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π."
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    logger.info(f"Response: {response_text[:100]}..., Source: {source}, Intent: {intent}, Confidence: {confidence}")
    
    return ChatResponse(
        response=response_text,
        intent=intent,
        confidence=confidence,
        source=source,
        timestamp=datetime.now().isoformat()
    )

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è –º–æ–∫–æ–≤
@app.get("/ride-status/{user_id}")
async def get_ride_status_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –ø–æ–µ–∑–¥–∫–∏"""
    return get_ride_status(user_id)

@app.post("/send-receipt/{user_id}")
async def send_receipt_endpoint(user_id: str):
    """–û—Ç–ø—Ä–∞–≤–∏—Ç—å —á–µ–∫"""
    return send_receipt(user_id)

@app.get("/cards/{user_id}")
async def list_cards_endpoint(user_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∫–∞—Ä—Ç"""
    return list_cards(user_id)

@app.post("/escalate/{user_id}")
async def escalate_endpoint(user_id: str, description: str):
    """–≠—Å–∫–∞–ª–∏—Ä–æ–≤–∞—Ç—å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É"""
    return escalate_to_human(user_id, description)

@app.get("/webapp")
async def webapp():
    """–û—Ç–¥–∞—á–∞ WebApp –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞"""
    return FileResponse("webapp.html")

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–∏—Å–∞"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

if __name__ == "__main__":
    import uvicorn
    import os
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
