"""
–¢–µ—Å—Ç —É–ª—å—Ç—Ä–∞-–≥–∏–±–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
"""

import sys
import os
sys.path.append('backend')

import json
import re

def search_faq_ultra_flexible(text: str, kb_data: dict):
    """–£–ª—å—Ç—Ä–∞-–≥–∏–±–∫–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ FAQ"""
    if not text or not kb_data:
        return None
    
    # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
    faq_items = kb_data.get("faq", [])
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    slang_replacements = {
        '—á–µ': '—á—Ç–æ',
        '—Ç–∞–º': '',
        '–ø–æ': '',
        '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç': '–∫–∞–∫',
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': '—Ä–∞–±–æ—Ç–∞',
        '–∫–∞–∫': '–∫–∞–∫',
        '–≥–¥–µ': '–≥–¥–µ',
        '—á—Ç–æ': '—á—Ç–æ',
        '–∫–∞–∫–∏–µ': '–∫–∞–∫–∏–µ',
        '–º–æ–∂–Ω–æ': '–º–æ–∂–Ω–æ',
        '–Ω—É–∂–Ω–æ': '–Ω—É–∂–Ω–æ',
        '–µ—Å—Ç—å': '–µ—Å—Ç—å',
        '–±—ã—Ç—å': '–±—ã—Ç—å',
        '–≤—Ä–µ–º—è': '–≤—Ä–µ–º—è',
        '–¥–µ–Ω—å–≥–∏': '–¥–µ–Ω—å–≥–∏',
        '—Ü–µ–Ω–∞': '—Ü–µ–Ω–∞',
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': '—Å—Ç–æ–∏–º–æ—Å—Ç—å'
    }
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    text_cleaned = re.sub(r'[^\w\s]', ' ', text_lower)
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    
    processed_text = text_lower
    for slang, replacement in slang_replacements.items():
        processed_text = processed_text.replace(slang, replacement)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    processed_text = ' '.join(processed_text.split())
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    def calculate_tfidf_score(query_words, doc_words):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not query_words or not doc_words:
            return 0
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        common_words = set(query_words).intersection(set(doc_words))
        if not common_words:
            return 0
        
        # –ü—Ä–æ—Å—Ç–æ–π TF-IDF: log(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤ / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        tf_score = len(common_words) / len(doc_words)
        idf_score = 1 + (len(common_words) / len(query_words))
        
        return tf_score * idf_score
    
    best_match = None
    best_score = 0
    
    for item in faq_items:
        score = 0
        
        # –£–õ–¨–¢–†–ê-–ì–ò–ë–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        keywords = item.get("keywords", [])
        keyword_score = 0
        
        for keyword in keywords:
            # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–≤—ã—Å—à–∏–π –±–∞–ª–ª)
            if keyword.lower() in text_cleaned:
                keyword_score += 5
                continue
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–æ—Ä–Ω—é —Å–ª–æ–≤–∞ (stemming)
            keyword_root = keyword.lower()
            for word in text_cleaned.split():
                if len(keyword_root) > 3 and len(word) > 3:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π –∫–æ—Ä–µ–Ω—å (–ø–µ—Ä–≤—ã–µ 4-5 —Å–∏–º–≤–æ–ª–æ–≤)
                    common_root = min(len(keyword_root), len(word))
                    if keyword_root[:common_root] == word[:common_root] and common_root >= 4:
                        keyword_score += 4
                        break
            
            # 3. –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)
            if keyword.lower() in text_lower or keyword.lower() in processed_text:
                keyword_score += 3
                continue
            
            # 4. –û–±—Ä–∞—Ç–Ω–æ–µ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å –≤ –∫–ª—é—á–µ–≤–æ–º —Å–ª–æ–≤–µ)
            for word in text_cleaned.split():
                if len(word) > 3 and word in keyword.lower():
                    keyword_score += 2
                    break
            
            # 5. –ü–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º –∏ –≤–∞—Ä–∏–∞—Ü–∏—è–º
            keyword_variations = [
                keyword.lower(),
                keyword.lower().replace('–∏', ''),
                keyword.lower().replace('–æ', '–∞'),
                keyword.lower().replace('–µ', '—ë'),
                keyword.lower() + '—ã',
                keyword.lower() + '–∏',
                keyword.lower() + '–∞',
                keyword.lower() + '—É'
            ]
            
            for variation in keyword_variations:
                if variation in text_cleaned:
                    keyword_score += 2
                    break
            
            # 6. –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–ø–æ—Ö–æ–∂–∏–µ –∑–≤—É–∫–∏)
            phonetic_map = {
                '–∫': ['–≥', '—Ö'],
                '–ø': ['–±'],
                '—Ç': ['–¥'],
                '—Å': ['–∑', '—Ü'],
                '—Ñ': ['–≤'],
                '—à': ['—â', '–∂'],
                '—á': ['—â']
            }
            
            for word in text_cleaned.split():
                if len(word) >= 4 and len(keyword) >= 4:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
                    similarity_count = 0
                    min_len = min(len(word), len(keyword))
                    
                    for i in range(min_len):
                        if word[i] == keyword[i]:
                            similarity_count += 1
                        elif word[i] in phonetic_map.get(keyword[i], []):
                            similarity_count += 0.5
                    
                    if similarity_count >= min_len * 0.7:  # 70% —Å—Ö–æ–¥—Å—Ç–≤–∞
                        keyword_score += 1.5
                        break
        
        # –ì–õ–£–ë–û–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤
        variations = item.get("question_variations", [])
        variation_score = 0
        query_words = text_cleaned.split()
        
        for variation in variations:
            variation_words = variation.lower().split()
            
            # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
            tfidf_score = calculate_tfidf_score(query_words, variation_words)
            variation_score += tfidf_score
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º (–¥–≤–∞ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            query_bigrams = set()
            for i in range(len(query_words) - 1):
                query_bigrams.add(f"{query_words[i]} {query_words[i+1]}")
            
            variation_bigrams = set()
            for i in range(len(variation_words) - 1):
                variation_bigrams.add(f"{variation_words[i]} {variation_words[i+1]}")
            
            bigram_matches = len(query_bigrams.intersection(variation_bigrams))
            variation_score += bigram_matches * 0.5
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ —Ç—Ä–∏–≥—Ä–∞–º–º–∞–º (—Ç—Ä–∏ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            if len(query_words) >= 3 and len(variation_words) >= 3:
                query_trigrams = set()
                for i in range(len(query_words) - 2):
                    query_trigrams.add(f"{query_words[i]} {query_words[i+1]} {query_words[i+2]}")
                
                variation_trigrams = set()
                for i in range(len(variation_words) - 2):
                    variation_trigrams.add(f"{variation_words[i]} {variation_words[i+1]} {variation_words[i+2]}")
                
                trigram_matches = len(query_trigrams.intersection(variation_trigrams))
                variation_score += trigram_matches * 1.0
        
        # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É
        main_question = item.get("question", "").lower()
        main_question_words = main_question.split()
        
        # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
        main_score = calculate_tfidf_score(query_words, main_question_words) * 2
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
        question_phrases = [
            "–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º",
            "–º–æ–∂–Ω–æ", "–Ω—É–∂–Ω–æ", "–µ—Å—Ç—å", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–¥–µ–ª–∞—Ç—å"
        ]
        
        for phrase in question_phrases:
            if phrase in text_cleaned and phrase in main_question:
                main_score += 0.5
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å–ª–æ–≤ (–≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        query_words_set = set(query_words)
        question_words_set = set(main_question_words)
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ—Ä—è–¥–∫–∞
        ordered_matches = 0
        for i, word in enumerate(query_words):
            if word in main_question_words:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤–æ–ø—Ä–æ—Å–µ
                for j, q_word in enumerate(main_question_words):
                    if word == q_word:
                        # –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –±–∞–ª–ª
                        position_bonus = 1.0 - (abs(i - j) * 0.1)
                        ordered_matches += max(0.1, position_bonus)
                        break
        
        main_score += ordered_matches * 0.3
        
        # 4. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        important_words = ["–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ç–∞—Ä–∏—Ñ", "–∫–∞—Ä—Ç–∞", "–±–∞–ª–∞–Ω—Å", "–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–∫–∞–∑"]
        for word in important_words:
            if word in text_cleaned and word in main_question:
                main_score += 1.0  # –í—ã—Å–æ–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–ª–æ–≤ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        max_possible_keywords = len(keywords) * 5  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        normalized_keyword_score = keyword_score / max_possible_keywords if max_possible_keywords > 0 else 0
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        score = (
            normalized_keyword_score * 6.0 +    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ = —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            variation_score * 1.5 +             # –í–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            main_score * 3.0                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        )
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if keyword_score > 0 and variation_score > 0 and main_score > 0:
            score *= 1.2  # 20% –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        
        if score > best_score:
            best_score = score
            best_match = item
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–∞–ª–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–π
    if best_score >= 0.3:  # –ï—â–µ –±–æ–ª—å—à–µ –ø–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥
        return best_match
    
    return None

def load_kb_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    try:
        with open('backend/kb.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ kb.json: {e}")
        return None

def test_ultra_flexible_search():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–ª—å—Ç—Ä–∞-–≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫"""
    print("üöÄ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –£–õ–¨–¢–†–ê-–ì–ò–ë–ö–û–ì–û –ø–æ–∏—Å–∫–∞")
    print("=" * 60)
    
    kb_data = load_kb_data()
    if not kb_data:
        return
    
    # –°–ª–æ–∂–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        # –¢–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
        "–¥–æ—Å—Ç–∞–≤–∫–∞",
        "—Ü–µ–Ω–∞ –ø–æ–µ–∑–¥–∫–∏",
        "–±–∞–ª–∞–Ω—Å –∫–∞—Ä—Ç—ã",
        
        # –°–ª–µ–Ω–≥–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        "—á–µ —Ç–∞–º –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ",
        "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞—Ä–∏—Ñ",
        "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç —Ç–∞–∫—Å–∏",
        
        # –û–ø–µ—á–∞—Ç–∫–∏ –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏
        "–¥–æ—Å—Ç–∞–≤–∫",  # –±–µ–∑ –æ–∫–æ–Ω—á–∞–Ω–∏—è
        "—Ü–µ–Ω—ã",  # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
        "–∫–∞—Ä—Ç—ã",  # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
        "—Ç–∞—Ä–∏—Ñ—ã",  # –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ
        
        # –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ
        "–¥–æ—Å—Ç–∞–≤–≥–∫–∞",  # –æ–ø–µ—á–∞—Ç–∫–∞
        "—Ü–µ–Ω–∞",  # –ø–æ—Ö–æ–∂–µ –Ω–∞ "—Ü–µ–Ω—ã"
        "–∫–∞—Ä—Ç–∞",  # –ø–æ—Ö–æ–∂–µ –Ω–∞ "–∫–∞—Ä—Ç—ã"
        
        # –°–ª–æ–∂–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        "–∫–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å –∫–∞—Ä—Ç—ã",
        "–≥–¥–µ –º–æ–π –∑–∞–∫–∞–∑ –¥–æ—Å—Ç–∞–≤–∫–∏",
        "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∫–æ–º—Ñ–æ—Ä—Ç –∫–ª–∞—Å—Å",
        "–º–æ–∂–Ω–æ –ª–∏ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥–∑–∞–∫–∞–∑",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞ –∑–∞ —Ç–∞—Ä–∏—Ñ",
        "–≤–æ–¥–∏—Ç–µ–ª—å –Ω–µ –ø—Ä–∏–µ—Ö–∞–ª –Ω–∞ –∑–∞–∫–∞–∑",
        "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—Ä—Ç–æ–π",
        "–∫–∞–∫–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –¥–ª—è –æ–ø–ª–∞—Ç—ã"
    ]
    
    print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º {len(test_queries)} —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
    print()
    
    success_count = 0
    for i, query in enumerate(test_queries, 1):
        print(f"{i:2d}. –ó–∞–ø—Ä–æ—Å: '{query}'")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–ª—å—Ç—Ä–∞-–≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫
        result = search_faq_ultra_flexible(query, kb_data)
        
        if result:
            print(f"    ‚úÖ –ù–ê–ô–î–ï–ù–û!")
            print(f"    üìã Question: {result.get('question', '')[:60]}...")
            success_count += 1
        else:
            print(f"    ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û")
        
        print()
    
    success_rate = (success_count / len(test_queries)) * 100
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"–£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω–æ: {success_count}/{len(test_queries)} ({success_rate:.1f}%)")
    
    if success_rate >= 90:
        print("üéâ –ü–†–ï–í–û–°–•–û–î–ù–û! –£–ª—å—Ç—Ä–∞-–≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ!")
    elif success_rate >= 80:
        print("üöÄ –û–¢–õ–ò–ß–ù–û! –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞!")
    elif success_rate >= 70:
        print("‚úÖ –•–û–†–û–®–û! –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è!")
    else:
        print("‚ùå –ù—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è")

if __name__ == "__main__":
    test_ultra_flexible_search()
