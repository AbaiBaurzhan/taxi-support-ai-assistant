#!/usr/bin/env python3
"""
üöÄ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ç—Ä–µ–Ω–µ—Ä –º–æ–¥–µ–ª–∏ APARU
–£–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤
"""

import json
import logging
import re
from typing import Dict, List, Any
from pathlib import Path

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedModelTrainer:
    def __init__(self):
        self.knowledge_base = []
        self.context_patterns = {}
        self.answer_templates = {}
        
    def load_knowledge_base(self, file_path: str = "database_Aparu/BZ.txt"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–∞—Ä—Å–∏—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        logger.info("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
        
        parsed_data = []
        current_question = None
        current_answer = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if line.startswith('- question:'):
                        if current_question and current_answer:
                            parsed_data.append(self._process_qa_pair(current_question, "\n".join(current_answer)))
                        current_question = line.replace('- question:', '').strip()
                        current_answer = []
                    elif line.startswith('answer:'):
                        current_answer.append(line.replace('answer:', '').strip())
                    elif line and current_question:
                        current_answer.append(line)
            
            if current_question and current_answer:
                parsed_data.append(self._process_qa_pair(current_question, "\n".join(current_answer)))
            
            self.knowledge_base = parsed_data
            logger.info(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(parsed_data)} –∑–∞–ø–∏—Å–µ–π")
            return parsed_data
            
        except FileNotFoundError:
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return []
    
    def _process_qa_pair(self, question: str, answer: str) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–∞—Ä—É –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"""
        return {
            'question': question,
            'answer': answer,
            'context_keywords': self._extract_context_keywords(question, answer),
            'answer_structure': self._analyze_answer_structure(answer),
            'variations': self._generate_contextual_variations(question),
            'category': self._categorize_question(question),
            'confidence_level': self._calculate_confidence_level(question, answer)
        }
    
    def _extract_context_keywords(self, question: str, answer: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        text = (question + " " + answer).lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        keywords = []
        
        # –¢–∞—Ä–∏—Ñ—ã –∏ —Ü–µ–Ω—ã
        if any(word in text for word in ['—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–Ω–∞—Ü–µ–Ω–∫–∞']):
            keywords.extend(['—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–Ω–∞—Ü–µ–Ω–∫–∞'])
        
        # –ó–∞–∫–∞–∑—ã –∏ –ø–æ–µ–∑–¥–∫–∏
        if any(word in text for word in ['–∑–∞–∫–∞–∑', '–ø–æ–µ–∑–¥–∫–∞', '—Ç–∞–∫—Å–∏', '–≤—ã–∑–æ–≤']):
            keywords.extend(['–∑–∞–∫–∞–∑', '–ø–æ–µ–∑–¥–∫–∞', '—Ç–∞–∫—Å–∏', '–≤—ã–∑–æ–≤'])
        
        # –ë–∞–ª–∞–Ω—Å –∏ –æ–ø–ª–∞—Ç–∞
        if any(word in text for word in ['–±–∞–ª–∞–Ω—Å', '–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂']):
            keywords.extend(['–±–∞–ª–∞–Ω—Å', '–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂'])
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        if any(word in text for word in ['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–º–æ—Ç–æ—á–∞—Å—ã', 'gps']):
            keywords.extend(['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–º–æ—Ç–æ—á–∞—Å—ã', 'gps'])
        
        # –î–æ—Å—Ç–∞–≤–∫–∞
        if any(word in text for word in ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞']):
            keywords.extend(['–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞'])
        
        return list(set(keywords))
    
    def _analyze_answer_structure(self, answer: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞"""
        return {
            'has_greeting': '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ' in answer.lower(),
            'has_signature': '–∫–æ–º–∞–Ω–¥–∞ –∞–ø–∞—Ä—É' in answer.lower(),
            'has_links': bool(re.search(r'https?://', answer)),
            'has_instructions': any(word in answer.lower() for word in ['–Ω–∞–∂–º–∏—Ç–µ', '–≤—ã–±–µ—Ä–∏—Ç–µ', '–∑–∞–ø–æ–ª–Ω–∏—Ç–µ']),
            'length': len(answer),
            'sentences': len(re.split(r'[.!?]+', answer))
        }
    
    def _generate_contextual_variations(self, question: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞"""
        variations = [question]
        question_lower = question.lower()
        
        # –°–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏—è
        synonyms = {
            '—á—Ç–æ —Ç–∞–∫–æ–µ': ['–æ–±—ä—è—Å–Ω–∏', '—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ', '—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç'],
            '–∫–∞–∫': ['–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º', '–∫–∞–∫ –º–æ–∂–Ω–æ', '–∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ'],
            '–ø–æ—á–µ–º—É': ['–∏–∑-–∑–∞ —á–µ–≥–æ', '–ø–æ –∫–∞–∫–æ–π –ø—Ä–∏—á–∏–Ω–µ', '–æ—Ç—á–µ–≥–æ'],
            '–≥–¥–µ': ['–≤ –∫–∞–∫–æ–º –º–µ—Å—Ç–µ', '–≤ –∫–∞–∫–æ–π —á–∞—Å—Ç–∏', '–≥–¥–µ –Ω–∞–π—Ç–∏'],
            '–∫–æ–≥–¥–∞': ['–≤ –∫–∞–∫–æ–µ –≤—Ä–µ–º—è', '–≤ –∫–∞–∫–æ–π –º–æ–º–µ–Ω—Ç', '–∫–æ–≥–¥–∞ –º–æ–∂–Ω–æ']
        }
        
        for original, alternatives in synonyms.items():
            if original in question_lower:
                for alt in alternatives:
                    variations.append(question_lower.replace(original, alt))
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
        if '–Ω–∞—Ü–µ–Ω–∫–∞' in question_lower:
            variations.extend(['–ø–æ—á–µ–º—É –¥–æ—Ä–æ–≥–æ', '–≤—ã—Å–æ–∫–∞—è —Ü–µ–Ω–∞', '–∑–∞—á–µ–º –¥–æ–ø–ª–∞—Ç–∞'])
        if '–±–∞–ª–∞–Ω—Å' in question_lower:
            variations.extend(['–ø–æ–ø–æ–ª–Ω–∏—Ç—å —Å—á–µ—Ç', '–∫–∞–∫ –ø–æ–ª–æ–∂–∏—Ç—å –¥–µ–Ω—å–≥–∏', '–ø–æ–ø–æ–ª–Ω–µ–Ω–∏–µ'])
        if '–∑–∞–∫–∞–∑' in question_lower:
            variations.extend(['–æ—Ç–º–µ–Ω–∏—Ç—å –ø–æ–µ–∑–¥–∫—É', '–æ—Ç–º–µ–Ω–∞', '–∫–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å'])
        if '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ' in question_lower:
            variations.extend(['–∞–ø–ø–∞—Ä—É –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–ø—Ä–æ–±–ª–µ–º–∞ —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º', '–æ—à–∏–±–∫–∞'])
        if '–≤–æ–¥–∏—Ç–µ–ª—å' in question_lower:
            variations.extend(['–≥–¥–µ —Ç–∞–∫—Å–∏—Å—Ç', '—Å–≤—è–∑—å —Å –≤–æ–¥–∏—Ç–µ–ª–µ–º', '–∫–æ–Ω—Ç–∞–∫—Ç—ã'])
        if '—Ü–µ–Ω–∞' in question_lower:
            variations.extend(['—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç', '—Ä–∞—Å—Ü–µ–Ω–∫–∏', '—Ç–∞—Ä–∏—Ñ—ã'])
        if '–¥–æ—Å—Ç–∞–≤–∫–∞' in question_lower:
            variations.extend(['–∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –ø–æ—Å—ã–ª–∫—É', '–∑–∞–∫–∞–∑–∞—Ç—å –∫—É—Ä—å–µ—Ä–∞', '–∫—É—Ä—å–µ—Ä'])
        if '–∫–æ–º—Ñ–æ—Ä—Ç' in question_lower:
            variations.extend(['—Ç–∞—Ä–∏—Ñ –∫–æ–º—Ñ–æ—Ä—Ç', '—á—Ç–æ –∑–∞ –∫–æ–º—Ñ–æ—Ä—Ç –∫–ª–∞—Å—Å', '–∫–æ–º—Ñ–æ—Ä—Ç –∫–ª–∞—Å—Å'])
        if '–º–æ—Ç–æ—á–∞—Å—ã' in question_lower:
            variations.extend(['–ø–æ—á–µ–º—É —Å—á–∏—Ç–∞—é—Ç –≤—Ä–µ–º—è', '–æ–ø–ª–∞—Ç–∞ –∑–∞ –≤—Ä–µ–º—è', '–≤—Ä–µ–º—è –ø–æ–µ–∑–¥–∫–∏'])
        if '—Ç–∞–∫—Å–∏' in question_lower:
            variations.extend(['–∫–∞–∫ –≤—ã–∑–≤–∞—Ç—å –º–∞—à–∏–Ω—É', '–∑–∞–∫–∞–∑–∞—Ç—å —Ç–∞–∫—Å–∏', '–≤—ã–∑–æ–≤ —Ç–∞–∫—Å–∏'])
        if '—ç–∫–æ–Ω–æ–º' in question_lower:
            variations.extend(['–¥–µ—à–µ–≤–æ–µ —Ç–∞–∫—Å–∏', '—Å–∞–º—ã–π –¥–µ—à–µ–≤—ã–π —Ç–∞—Ä–∏—Ñ', '—ç–∫–æ–Ω–æ–º –∫–ª–∞—Å—Å'])
        if '—É–Ω–∏–≤–µ—Ä—Å–∞–ª' in question_lower:
            variations.extend(['–º–∞—à–∏–Ω–∞ —Å –±–æ–ª—å—à–∏–º –±–∞–≥–∞–∂–Ω–∏–∫–æ–º', '—É–Ω–∏–≤–µ—Ä—Å–∞–ª –∫–ª–∞—Å—Å'])
        if '–≥—Ä—É–∑–æ–≤–æ–µ' in question_lower:
            variations.extend(['–≥—Ä—É–∑–æ–ø–µ—Ä–µ–≤–æ–∑–∫–∏', '–∑–∞–∫–∞–∑–∞—Ç—å –≥—Ä—É–∑–æ–≤–∏–∫', '–≥—Ä—É–∑–æ–≤–æ–µ —Ç–∞–∫—Å–∏'])
        if '—ç–≤–∞–∫—É–∞—Ç–æ—Ä' in question_lower:
            variations.extend(['–≤—ã–∑–≤–∞—Ç—å —ç–≤–∞–∫—É–∞—Ç–æ—Ä', '—ç–≤–∞–∫—É–∞—Ç–æ—Ä —É—Å–ª—É–≥–∞'])
        if '—Å–ø–∏—Å–∞–ª–∏' in question_lower:
            variations.extend(['–¥–≤–æ–π–Ω–æ–µ —Å–ø–∏—Å–∞–Ω–∏–µ', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —Å—É–º–º–∞', '–æ—à–∏–±–∫–∞ –æ–ø–ª–∞—Ç—ã'])
        if '—Å–ø—Ä–∞–≤–∫–∞' in question_lower:
            variations.extend(['–ø–æ–ª—É—á–∏—Ç—å —á–µ–∫', '–Ω—É–∂–µ–Ω –¥–æ–∫—É–º–µ–Ω—Ç', '–¥–æ–∫—É–º–µ–Ω—Ç'])
        if '–Ω–∞–ª–∏—á–Ω—ã–µ' in question_lower:
            variations.extend(['–æ–ø–ª–∞—Ç–∞ –∫—ç—à–µ–º', '–º–æ–∂–Ω–æ –ª–∏ –ø–ª–∞—Ç–∏—Ç—å –Ω–∞–ª–∏—á–∫–æ–π', '–Ω–∞–ª–∏—á–Ω—ã–µ –¥–µ–Ω—å–≥–∏'])
        if '–∫–∞—Ä—Ç–∞' in question_lower:
            variations.extend(['–ø—Ä–∏–≤—è–∑–∞—Ç—å –±–∞–Ω–∫–æ–≤—Å–∫—É—é –∫–∞—Ä—Ç—É', '–∫–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –∫–∞—Ä—Ç—É', '–±–∞–Ω–∫–æ–≤—Å–∫–∞—è –∫–∞—Ä—Ç–∞'])
        if '–≤–µ—â–∏' in question_lower:
            variations.extend(['–∑–∞–±—ã–ª –≤–µ—â–∏', '–ø–æ—Ç–µ—Ä—è–ª –≤ —Ç–∞–∫—Å–∏', '–∑–∞–±—ã—Ç—ã–µ –≤–µ—â–∏'])
        if '–æ—Ç–∑—ã–≤' in question_lower:
            variations.extend(['–æ—Å—Ç–∞–≤–∏—Ç—å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π', '–æ—Ü–µ–Ω–∏—Ç—å –ø–æ–µ–∑–¥–∫—É', '–æ—Ü–µ–Ω–∫–∞'])
        if '–ø—Ä–æ–º–æ–∫–æ–¥' in question_lower:
            variations.extend(['–≥–¥–µ –≤–∑—è—Ç—å –ø—Ä–æ–º–æ–∫–æ–¥', '—Å–∫–∏–¥–∫–∞', '–ø—Ä–æ–º–æ –∫–æ–¥'])
        if '–º–∞–º–∞' in question_lower or '–¥—Ä—É–≥–æ–º—É' in question_lower:
            variations.extend(['–∑–∞–∫–∞–∑–∞—Ç—å –¥–ª—è –¥—Ä—É–≥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞', '–∑–∞–∫–∞–∑ –¥–ª—è –¥—Ä—É–≥–æ–≥–æ'])
        if '—Ä–µ–π—Ç–∏–Ω–≥' in question_lower:
            variations.extend(['–∫–∞–∫ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç—Å—è —Ä–µ–π—Ç–∏–Ω–≥', '–æ—Ü–µ–Ω–∫–∞ –≤–æ–¥–∏—Ç–µ–ª—è', '—Ä–µ–π—Ç–∏–Ω–≥ –≤–æ–¥–∏—Ç–µ–ª—è'])
        if '–ø–ª–æ—Ö–æ' in question_lower or '–≥—Ä—É–±–æ' in question_lower:
            variations.extend(['–∂–∞–ª–æ–±–∞ –Ω–∞ –≤–æ–¥–∏—Ç–µ–ª—è', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω –≤–æ–¥–∏—Ç–µ–ª–µ–º', '–ø–ª–æ—Ö–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ'])
        
        return list(set(variations))
    
    def _categorize_question(self, question: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–Ω–∞—Ü–µ–Ω–∫–∞']):
            return 'pricing'
        elif any(word in question_lower for word in ['–∑–∞–∫–∞–∑', '–ø–æ–µ–∑–¥–∫–∞', '—Ç–∞–∫—Å–∏', '–≤—ã–∑–æ–≤']):
            return 'booking'
        elif any(word in question_lower for word in ['–±–∞–ª–∞–Ω—Å', '–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂']):
            return 'payment'
        elif any(word in question_lower for word in ['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–º–æ—Ç–æ—á–∞—Å—ã', 'gps']):
            return 'technical'
        elif any(word in question_lower for word in ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞']):
            return 'delivery'
        elif any(word in question_lower for word in ['–≤–æ–¥–∏—Ç–µ–ª—å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Å–≤—è–∑—å']):
            return 'driver'
        elif any(word in question_lower for word in ['–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑']):
            return 'cancellation'
        elif any(word in question_lower for word in ['–∂–∞–ª–æ–±–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω']):
            return 'complaint'
        else:
            return 'general'
    
    def _calculate_confidence_level(self, question: str, answer: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –æ—Ç–≤–µ—Ç–µ"""
        confidence = 0.5  # –ë–∞–∑–æ–≤—ã–π —É—Ä–æ–≤–µ–Ω—å
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
        if '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ' in answer.lower():
            confidence += 0.1
        if '–∫–æ–º–∞–Ω–¥–∞ –∞–ø–∞—Ä—É' in answer.lower():
            confidence += 0.1
        if len(answer) > 100:
            confidence += 0.1
        if len(answer) > 200:
            confidence += 0.1
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ—Å—Ç—å
        if any(word in answer.lower() for word in ['—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']):
            confidence += 0.1
        if any(word in answer.lower() for word in ['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '–Ω–∞–∂–º–∏—Ç–µ', '–≤—ã–±–µ—Ä–∏—Ç–µ']):
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    def create_enhanced_modelfile(self, output_path: str = "EnhancedAPARU.modelfile"):
        """–°–æ–∑–¥–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π Modelfile –¥–ª—è Ollama"""
        logger.info("üîß –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π Modelfile...")
        
        modelfile_content = f"""FROM llama2:7b

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è APARU Support
SYSTEM \"\"\"
–¢—ã - AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏ APARU. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–ª–∏–µ–Ω—Ç–æ–≤ —Ç–æ—á–Ω–æ, –≤–µ–∂–ª–∏–≤–æ –∏ –ø–æ –¥–µ–ª—É.

–ü–†–ê–í–ò–õ–ê –û–¢–í–ï–¢–û–í:
1. –í—Å–µ–≥–¥–∞ –Ω–∞—á–∏–Ω–∞–π —Å "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!"
2. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
3. –ù–ï —Å–º–µ—à–∏–≤–∞–π –æ—Ç–≤–µ—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
4. –ù–ï –≤—ã–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
5. –ó–∞–∫–∞–Ω—á–∏–≤–∞–π –ø–æ–¥–ø–∏—Å—å—é "–° —É–≤–∞–∂–µ–Ω–∏–µ–º, –∫–æ–º–∞–Ω–¥–∞ –ê–ü–ê–†–£"
6. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏ –æ–± —ç—Ç–æ–º

–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:
- –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ
- –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç–≤–µ—Ç
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–µ—Å–ª–∏ –µ—Å—Ç—å)
- –ü–æ–¥–ø–∏—Å—å

–ö–û–ù–¢–ï–ö–°–¢–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò:
- pricing: —Ç–∞—Ä–∏—Ñ—ã, —Ü–µ–Ω—ã, –Ω–∞—Ü–µ–Ω–∫–∏
- booking: –∑–∞–∫–∞–∑—ã, –ø–æ–µ–∑–¥–∫–∏, –≤—ã–∑–æ–≤—ã
- payment: –±–∞–ª–∞–Ω—Å, –æ–ø–ª–∞—Ç–∞, –ø–ª–∞—Ç–µ–∂–∏
- technical: –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ, —Ç–∞–∫—Å–æ–º–µ—Ç—Ä, GPS
- delivery: –¥–æ—Å—Ç–∞–≤–∫–∞, –∫—É—Ä—å–µ—Ä—ã
- driver: –≤–æ–¥–∏—Ç–µ–ª–∏, –∫–æ–Ω—Ç–∞–∫—Ç—ã
- cancellation: –æ—Ç–º–µ–Ω—ã, –æ—Ç–∫–∞–∑—ã
- complaint: –∂–∞–ª–æ–±—ã, –ø—Ä–æ–±–ª–µ–º—ã
- general: –æ–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã

–ë–ê–ó–ê –ó–ù–ê–ù–ò–ô:
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        for i, item in enumerate(self.knowledge_base, 1):
            modelfile_content += f"""
–í–û–ü–†–û–° {i}: {item['question']}
–ö–ê–¢–ï–ì–û–†–ò–Ø: {item['category']}
–ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {', '.join(item['context_keywords'])}
–û–¢–í–ï–¢: {item['answer']}
–£–í–ï–†–ï–ù–ù–û–°–¢–¨: {item['confidence_level']:.2f}
---
"""
        
        modelfile_content += """

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
- –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –∏ –æ–ø—Ä–µ–¥–µ–ª—è–π –∫–∞—Ç–µ–≥–æ—Ä–∏—é
- –ò—â–∏ –æ—Ç–≤–µ—Ç –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- –ù–ï —Å–º–µ—à–∏–≤–∞–π –æ—Ç–≤–µ—Ç—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ APARU, –≤–µ–∂–ª–∏–≤–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤—å –∫ –ø–æ–¥–¥–µ—Ä–∂–∫–µ
\"\"\"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
PARAMETER temperature 0.3
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER stop "–° —É–≤–∞–∂–µ–Ω–∏–µ–º, –∫–æ–º–∞–Ω–¥–∞ –ê–ü–ê–†–£"
PARAMETER stop "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!"
"""
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(modelfile_content)
        
        logger.info(f"‚úÖ Modelfile —Å–æ–∑–¥–∞–Ω: {output_path}")
        return output_path
    
    def create_context_aware_search(self, output_path: str = "context_aware_search.py"):
        """–°–æ–∑–¥–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫"""
        logger.info("üîç –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫...")
        
        search_code = f'''#!/usr/bin/env python3
"""
üîç –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –¥–ª—è APARU
–ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –∏ —É–ª—É—á—à–∞–µ—Ç –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
"""

import json
import logging
from typing import Dict, List, Any, Tuple
from fuzzywuzzy import fuzz
import re

logger = logging.getLogger(__name__)

class ContextAwareSearch:
    def __init__(self, knowledge_base_path: str = "enhanced_aparu_knowledge.json"):
        self.knowledge_base = self._load_knowledge_base(knowledge_base_path)
        self.context_patterns = self._build_context_patterns()
        
    def _load_knowledge_base(self, path: str) -> List[Dict]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {{path}}")
            return []
    
    def _build_context_patterns(self) -> Dict[str, List[str]]:
        """–°—Ç—Ä–æ–∏—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        return {{
            'pricing': ['—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–Ω–∞—Ü–µ–Ω–∫–∞', '–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ'],
            'booking': ['–∑–∞–∫–∞–∑', '–ø–æ–µ–∑–¥–∫–∞', '—Ç–∞–∫—Å–∏', '–≤—ã–∑–æ–≤', '–≤—ã–∑–≤–∞—Ç—å', '–∑–∞–∫–∞–∑–∞—Ç—å'],
            'payment': ['–±–∞–ª–∞–Ω—Å', '–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂', '–∫–∞—Ä—Ç–∞', '–Ω–∞–ª–∏—á–Ω—ã–µ'],
            'technical': ['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–º–æ—Ç–æ—á–∞—Å—ã', 'gps', '–æ—à–∏–±–∫–∞', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç'],
            'delivery': ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–ø–µ—Ä–µ–¥–∞—Ç—å'],
            'driver': ['–≤–æ–¥–∏—Ç–µ–ª—å', '—Ç–∞–∫—Å–∏—Å—Ç', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Å–≤—è–∑—å', '–Ω–æ–º–µ—Ä'],
            'cancellation': ['–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑', '–æ—Ç–º–µ–Ω–∏—Ç—å –∑–∞–∫–∞–∑'],
            'complaint': ['–∂–∞–ª–æ–±–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω', '–ø–ª–æ—Ö–æ', '–≥—Ä—É–±–æ'],
            'general': ['—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–ø–æ—á–µ–º—É']
        }}
    
    def get_contextual_answer(self, question: str, threshold: float = 0.4) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
        –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç —Å–º–µ—à–∏–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        """
        question_lower = question.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –≤–æ–ø—Ä–æ—Å–∞
        question_category = self._categorize_question(question_lower)
        
        # –ò—â–µ–º –æ—Ç–≤–µ—Ç—ã —Ç–æ–ª—å–∫–æ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_items = [item for item in self.knowledge_base if item.get('category') == question_category]
        
        if not category_items:
            # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—â–µ–º –≤–æ –≤—Å–µ—Ö
            category_items = self.knowledge_base
        
        best_match = None
        best_score = 0
        
        for item in category_items:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å
            score = fuzz.ratio(question_lower, item['question'].lower())
            if score > best_score:
                best_score = score
                best_match = item
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            for variation in item.get('variations', []):
                score = fuzz.ratio(question_lower, variation.lower())
                if score > best_score:
                    best_score = score
                    best_match = item
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            for keyword in item.get('context_keywords', []):
                if keyword.lower() in question_lower:
                    score = fuzz.ratio(question_lower, keyword.lower()) + 20
                    if score > best_score:
                        best_score = score
                        best_match = item
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score
        normalized_score = best_score / 100.0
        
        if normalized_score >= threshold and best_match:
            return {{
                'answer': best_match['answer'],
                'confidence': normalized_score,
                'source': 'knowledge_base',
                'category': question_category,
                'context_keywords': best_match.get('context_keywords', []),
                'prevented_mixing': True
            }}
        else:
            return {{
                'answer': "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
                'confidence': 0.0,
                'source': 'fallback',
                'category': question_category,
                'context_keywords': [],
                'prevented_mixing': True
            }}
    
    def _categorize_question(self, question: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        for category, keywords in self.context_patterns.items():
            if any(keyword in question for keyword in keywords):
                return category
        return 'general'
    
    def validate_answer_consistency(self, question: str, answer: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞"""
        question_category = self._categorize_question(question.lower())
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤–æ–ø—Ä–æ—Å–∞
        if question_category == 'pricing' and not any(word in answer.lower() for word in ['—Ç–∞—Ä–∏—Ñ', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å']):
            return False
        elif question_category == 'booking' and not any(word in answer.lower() for word in ['–∑–∞–∫–∞–∑', '–ø–æ–µ–∑–¥–∫–∞', '—Ç–∞–∫—Å–∏']):
            return False
        elif question_category == 'payment' and not any(word in answer.lower() for word in ['–±–∞–ª–∞–Ω—Å', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂']):
            return False
        
        return True

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ main.py
_context_aware_search = ContextAwareSearch()

def get_contextual_answer(question: str) -> str:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç"""
    result = _context_aware_search.get_contextual_answer(question)
    return result['answer']
'''
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(search_code)
        
        logger.info(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫ —Å–æ–∑–¥–∞–Ω: {output_path}")
        return output_path
    
    def save_enhanced_knowledge_base(self, output_path: str = "enhanced_aparu_knowledge.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        logger.info("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
        
        logger.info(f"‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_path}")
        return output_path
    
    def train_advanced_model(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        self.load_knowledge_base()
        
        # 2. –°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π Modelfile
        modelfile_path = self.create_enhanced_modelfile()
        
        # 3. –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–æ—Å–æ–∑–Ω–∞–Ω–Ω—ã–π –ø–æ–∏—Å–∫
        search_path = self.create_context_aware_search()
        
        # 4. –°–æ—Ö—Ä–∞–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
        kb_path = self.save_enhanced_knowledge_base()
        
        logger.info("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
        logger.info(f"   - {modelfile_path}")
        logger.info(f"   - {search_path}")
        logger.info(f"   - {kb_path}")
        
        return {
            'modelfile': modelfile_path,
            'search': search_path,
            'knowledge_base': kb_path
        }

if __name__ == "__main__":
    trainer = AdvancedModelTrainer()
    results = trainer.train_advanced_model()
    print("üéØ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results}")
