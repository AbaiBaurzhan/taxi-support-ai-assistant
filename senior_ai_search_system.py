#!/usr/bin/env python3
"""
üß† Senior AI Engineer - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ APARU
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞ —Å –≥–∏–±—Ä–∏–¥–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
"""

import json
import logging
import pickle
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
from datetime import datetime
import hashlib

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import SnowballStemmer
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è fuzzy search
try:
    from fuzzywuzzy import fuzz, process
    FUZZY_AVAILABLE = True
except ImportError:
    FUZZY_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SeniorAISearchSystem:
    def __init__(self, knowledge_base_path: str = "senior_ai_knowledge_base.json", index_path: str = "senior_ai_search_index.pkl"):
        self.knowledge_base_path = knowledge_base_path
        self.index_path = index_path
        self.knowledge_base = []
        self.embeddings_model = None
        self.embeddings_index = None
        self.question_embeddings = None
        self.text_to_item = {}
        self.stop_words = set()
        self.stemmer = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._load_knowledge_base()
        self._initialize_text_processing()
        self._initialize_embeddings()
        self._load_search_index()
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.request_log = []
        self.knowledge_expansions = []
        
        # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        self.quality_metrics = {
            'total_requests': 0,
            'successful_matches': 0,
            'high_confidence_matches': 0,
            'category_distribution': {},
            'avg_response_time': 0.0
        }
    
    def _load_knowledge_base(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        try:
            with open(self.knowledge_base_path, 'r', encoding='utf-8') as f:
                self.knowledge_base = json.load(f)
            logger.info(f"‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
        except FileNotFoundError:
            logger.error(f"‚ùå –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.knowledge_base_path}")
            self.knowledge_base = []
    
    def _initialize_text_processing(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            self.stop_words = set([
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
            ])
            
            # –°—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            if NLTK_AVAILABLE:
                self.stemmer = SnowballStemmer('russian')
            
            logger.info("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            self.stop_words = set()
            self.stemmer = None
    
    def _initialize_embeddings(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if EMBEDDINGS_AVAILABLE:
            try:
                self.embeddings_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                self.embeddings_model = None
        else:
            logger.warning("‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    def _load_search_index(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å"""
        try:
            with open(self.index_path, 'rb') as f:
                index_data = pickle.load(f)
            
            self.embeddings_index = index_data['embeddings_index']
            self.question_embeddings = index_data['question_embeddings']
            self.text_to_item = index_data['text_to_item']
            
            logger.info("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω")
        except FileNotFoundError:
            logger.warning(f"‚ö†Ô∏è –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.index_path}")
            self.embeddings_index = None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            self.embeddings_index = None
    
    def normalize_text_advanced(self, text: str) -> str:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return ""
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
        import re
        text = re.sub(r'[^\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = text.split()
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = [word for word in words if word not in self.stop_words and len(word) > 2]
        
        # –°—Ç–µ–º–º–∏–Ω–≥
        if self.stemmer:
            words = [self.stemmer.stem(word) for word in words]
        
        return ' '.join(words)
    
    def search_by_embeddings_advanced(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º"""
        if not self.embeddings_model or not self.embeddings_index:
            return []
        
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
            normalized_query = self.normalize_text_advanced(query)
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.embeddings_model.encode([normalized_query])
            faiss.normalize_L2(query_embedding)
            
            # –ü–æ–∏—Å–∫ –≤ FAISS
            scores, indices = self.embeddings_index.search(query_embedding, top_k)
            
            results = []
            for score, idx in zip(scores[0], indices[0]):
                if idx < len(self.question_embeddings):
                    # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
                    text = list(self.text_to_item.keys())[idx]
                    item = self.text_to_item[text]
                    item_id = item['id'] - 1  # –ò–Ω–¥–µ–∫—Å –≤ –º–∞—Å—Å–∏–≤–µ
                    results.append((item_id, float(score)))
            
            return results
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º: {e}")
            return []
    
    def search_by_keywords_advanced(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        normalized_query = self.normalize_text_advanced(query)
        query_words = set(normalized_query.split())
        
        results = []
        
        for idx, item in enumerate(self.knowledge_base):
            score = 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            for keyword in item.get('keywords', []):
                keyword_normalized = self.normalize_text_advanced(keyword)
                if keyword_normalized in query_words:
                    score += 2  # –í—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å
            question_normalized = self.normalize_text_advanced(item['question'])
            question_words = set(question_normalized.split())
            common_words = query_words.intersection(question_words)
            score += len(common_words) * 1.5
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏
            for variation in item.get('variations', []):
                variation_normalized = self.normalize_text_advanced(variation)
                variation_words = set(variation_normalized.split())
                common_words = query_words.intersection(variation_words)
                score += len(common_words) * 1.0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç–≤–µ—Ç
            answer_normalized = self.normalize_text_advanced(item['answer'])
            answer_words = set(answer_normalized.split())
            common_words = query_words.intersection(answer_words)
            score += len(common_words) * 0.5
            
            if score > 0:
                results.append((idx, score))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    def search_by_fuzzy_advanced(self, query: str, top_k: int = 5) -> List[Tuple[int, float]]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫ –ø–æ fuzzy matching"""
        if not FUZZY_AVAILABLE:
            return []
        
        try:
            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤–∞—Ä–∏–∞—Ü–∏–∏
            all_questions = []
            question_to_idx = {}
            
            for idx, item in enumerate(self.knowledge_base):
                all_questions.append(item['question'])
                question_to_idx[item['question']] = idx
                
                for variation in item.get('variations', []):
                    all_questions.append(variation)
                    question_to_idx[variation] = idx
            
            # Fuzzy search —Å —Ä–∞–∑–Ω—ã–º–∏ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏
            matches = process.extract(query, all_questions, limit=top_k*2, scorer=fuzz.ratio)
            
            results = []
            seen_indices = set()
            
            for match_text, score in matches:
                if match_text in question_to_idx:
                    idx = question_to_idx[match_text]
                    if idx not in seen_indices:
                        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score –∫ 0-1
                        normalized_score = score / 100.0
                        results.append((idx, normalized_score))
                        seen_indices.add(idx)
            
            return results[:top_k]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ fuzzy search: {e}")
            return []
    
    def hybrid_search_advanced(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫"""
        start_time = datetime.now()
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ç –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞
        embedding_results = self.search_by_embeddings_advanced(query, top_k)
        keyword_results = self.search_by_keywords_advanced(query, top_k)
        fuzzy_results = self.search_by_fuzzy_advanced(query, top_k)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        combined_scores = {}
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–µ –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        embedding_weight = 0.6  # –í—ã—Å–æ–∫–∏–π –≤–µ—Å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
        keyword_weight = 0.3    # –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        fuzzy_weight = 0.1      # –ù–∏–∑–∫–∏–π –≤–µ—Å –¥–ª—è fuzzy matching
        
        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
        for idx, score in embedding_results:
            if idx not in combined_scores:
                combined_scores[idx] = 0
            combined_scores[idx] += score * embedding_weight
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for idx, score in keyword_results:
            if idx not in combined_scores:
                combined_scores[idx] = 0
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º score –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            normalized_score = min(score / 20.0, 1.0)  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º –º–∞–∫—Å–∏–º—É–º 20 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
            combined_scores[idx] += normalized_score * keyword_weight
        
        # Fuzzy matching
        for idx, score in fuzzy_results:
            if idx not in combined_scores:
                combined_scores[idx] = 0
            combined_scores[idx] += score * fuzzy_weight
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é score
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        results = []
        for idx, score in sorted_results[:top_k]:
            if idx < len(self.knowledge_base):
                item = self.knowledge_base[idx]
                results.append({
                    'id': item.get('id', idx),
                    'question': item['question'],
                    'answer': item['answer'],
                    'category': item.get('category', 'general'),
                    'confidence': min(score, 1.0),
                    'keywords': item.get('keywords', []),
                    'variations': item.get('variations', []),
                    'metadata': item.get('metadata', {})
                })
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        response_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"üîç –ü–æ–∏—Å–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω –∑–∞ {response_time:.3f} —Å–µ–∫—É–Ω–¥")
        
        return results
    
    def ask_question_advanced(self, query: str) -> Dict[str, Any]:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        start_time = datetime.now()
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å
        request_id = hashlib.md5(f"{query}_{datetime.now()}".encode()).hexdigest()[:8]
        self.request_log.append({
            'id': request_id,
            'query': query,
            'timestamp': datetime.now().isoformat()
        })
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫
        results = self.hybrid_search_advanced(query, top_k=3)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.quality_metrics['total_requests'] += 1
        response_time = (datetime.now() - start_time).total_seconds()
        self.quality_metrics['avg_response_time'] = (
            (self.quality_metrics['avg_response_time'] * (self.quality_metrics['total_requests'] - 1) + response_time) 
            / self.quality_metrics['total_requests']
        )
        
        if not results:
            return {
                'answer': '–ù—É–∂–Ω–∞ —É—Ç–æ—á–Ω—è—é—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
                'confidence': 0.0,
                'category': 'general',
                'suggestions': [],
                'request_id': request_id,
                'response_time': response_time,
                'source': 'no_match'
            }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        best_result = results[0]
        
        if best_result['confidence'] >= 0.7:  # –í—ã—Å–æ–∫–∏–π –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
            self.quality_metrics['successful_matches'] += 1
            if best_result['confidence'] >= 0.9:
                self.quality_metrics['high_confidence_matches'] += 1
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            category = best_result['category']
            self.quality_metrics['category_distribution'][category] = self.quality_metrics['category_distribution'].get(category, 0) + 1
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            return {
                'answer': best_result['answer'],
                'confidence': best_result['confidence'],
                'category': best_result['category'],
                'suggestions': [],
                'request_id': request_id,
                'response_time': response_time,
                'source': 'knowledge_base',
                'metadata': best_result.get('metadata', {})
            }
        else:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —É—Ç–æ—á–Ω–µ–Ω–∏–µ —Å –±–ª–∏–∂–∞–π—à–∏–º–∏ –≤–æ–ø—Ä–æ—Å–∞–º–∏
            suggestions = []
            for result in results[:3]:
                suggestions.append({
                    'question': result['question'],
                    'confidence': result['confidence'],
                    'category': result['category']
                })
            
            return {
                'answer': '–ù—É–∂–Ω–∞ —É—Ç–æ—á–Ω—è—é—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è',
                'confidence': best_result['confidence'],
                'category': best_result.get('category', 'general'),
                'suggestions': suggestions,
                'request_id': request_id,
                'response_time': response_time,
                'source': 'clarification_needed'
            }
    
    def get_quality_metrics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º—ã"""
        base_metrics = {
            **self.quality_metrics,
            'total_knowledge_records': len(self.knowledge_base),
            'embeddings_available': self.embeddings_model is not None,
            'fuzzy_available': FUZZY_AVAILABLE,
            'nltk_available': NLTK_AVAILABLE
        }
        
        if self.quality_metrics['total_requests'] == 0:
            return base_metrics
        
        success_rate = self.quality_metrics['successful_matches'] / self.quality_metrics['total_requests']
        high_confidence_rate = self.quality_metrics['high_confidence_matches'] / self.quality_metrics['total_requests']
        
        return {
            **base_metrics,
            'success_rate': success_rate,
            'high_confidence_rate': high_confidence_rate
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_senior_ai_search_instance = SeniorAISearchSystem()

def ask_question_advanced(query: str) -> Dict[str, Any]:
    """–û—Å–Ω–æ–≤–Ω–æ–π API –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
    return _senior_ai_search_instance.ask_question_advanced(query)

def get_quality_metrics() -> Dict[str, Any]:
    """API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
    return _senior_ai_search_instance.get_quality_metrics()

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞
    search_system = SeniorAISearchSystem()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = search_system.get_quality_metrics()
    print(f"üìä –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞:")
    print(f"   –ó–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ: {metrics['total_knowledge_records']}")
    print(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {'‚úÖ' if metrics['embeddings_available'] else '‚ùå'}")
    print(f"   Fuzzy search: {'‚úÖ' if metrics['fuzzy_available'] else '‚ùå'}")
    print(f"   NLTK –æ–±—Ä–∞–±–æ—Ç–∫–∞: {'‚úÖ' if metrics['nltk_available'] else '‚ùå'}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–∏—Å–∫
    test_questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞?",
        "–ü–æ—á–µ–º—É —Ç–∞–∫ –¥–æ—Ä–æ–≥–æ?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ —Ç–∞—Ä–∏—Ñ –ö–æ–º—Ñ–æ—Ä—Ç?",
        "–ö–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å?",
        "–ö–∞–∫ –æ—Ç–º–µ–Ω–∏—Ç—å –∑–∞–∫–∞–∑?",
        "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "–ö–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –º–æ—Ç–æ—á–∞—Å—ã?"
    ]
    
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–æ–∏—Å–∫–∞:")
    for question in test_questions:
        result = search_system.ask_question_advanced(question)
        print(f"‚ùì {question}")
        print(f"‚úÖ {result['answer'][:100]}...")
        print(f"üìä –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.3f}")
        print(f"üè∑Ô∏è –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {result['category']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['response_time']:.3f}s")
        if result.get('suggestions'):
            print(f"üí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {len(result['suggestions'])}")
        print()
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    final_metrics = search_system.get_quality_metrics()
    print(f"\nüìà –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞:")
    print(f"   –£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {final_metrics['success_rate']:.1%}")
    print(f"   –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {final_metrics['high_confidence_rate']:.1%}")
    print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {final_metrics['avg_response_time']:.3f}s")
    
    print(f"\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º:")
    for category, count in final_metrics['category_distribution'].items():
        print(f"   {category}: {count} –∑–∞–ø—Ä–æ—Å–æ–≤")
