#!/usr/bin/env python3
"""
üöÄ –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ò–ò –ú–û–î–ï–õ–¨ APARU
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
"""

import json
import re
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
from fuzzywuzzy import fuzz, process
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ó–∞–≥—Ä—É–∂–∞–µ–º NLTK –¥–∞–Ω–Ω—ã–µ
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

app = FastAPI(title="APARU Enhanced AI Assistant", version="3.0.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–¥–µ–ª–∏
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str
    timestamp: str
    suggestions: List[str] = []

class HealthResponse(BaseModel):
    status: str
    architecture: str = "enhanced"
    timestamp: str

# –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
ENHANCED_KB = {
    "–Ω–∞—Ü–µ–Ω–∫–∞": {
        "answer": "–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ. –ù–∞—Ü–µ–Ω–∫–∞ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é —Ç–∞—Ä–∏—Ñ–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ö–æ–º–ø–∞–Ω–∏–∏ –∏ –µ—ë –Ω–∞–ª–∏—á–∏–µ —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è –æ–±—ä—ë–º–æ–º —Å–ø—Ä–æ—Å–∞ –Ω–∞ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º—ã–π –º–æ–º–µ–Ω—Ç. –í –≤–∞—à–µ–º —Å–ª—É—á–∞–µ –Ω–∞–±–ª—é–¥–∞–ª—Å—è –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —Å–ø—Ä–æ—Å –Ω–∞ —É—Å–ª—É–≥–∏ –∏–∑-–∑–∞ –ø–æ–≥–æ–¥–Ω—ã—Ö —É—Å–ª–æ–≤–∏–π. –î–∞–Ω–Ω–∞—è –º–µ—Ä–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–∞ –¥–ª—è –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–æ–¥–∏—Ç–µ–ª–µ–π –Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–∫–∞–∑–æ–≤, —á—Ç–æ –≤ —Å–≤–æ—é –æ—á–µ—Ä–µ–¥—å —Å–æ–∫—Ä–∞—â–∞–µ—Ç –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –¥–ª—è –≤–∞—Å. –°—Ç–æ–∏–º–æ—Å—Ç—å –æ–ø–ª–∞—Ç—ã –ø–æ–µ–∑–¥–∫–∏ —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å–æ–≥–ª–∞—Å–Ω–æ –ø–æ–∫–∞–∑–∞–Ω–∏—è–º —Ç–∞–∫—Å–æ–º–µ—Ç—Ä–∞. –ë–ª–∞–≥–æ–¥–∞—Ä–∏–º –∑–∞ –æ–±—Ä–∞—â–µ–Ω–∏–µ! –° —É–≤–∞–∂–µ–Ω–∏–µ–º, –∫–æ–º–∞–Ω–¥–∞ –ê–ü–ê–†–£.",
        "keywords": ["–Ω–∞—Ü–µ–Ω–∫–∞", "–Ω–∞—Ü–µ–Ω–∫–∏", "–Ω–∞—Ü–µ–Ω–∫—É", "–Ω–∞—Ü–µ–Ω–∫–æ–π", "–¥–æ—Ä–æ–≥–æ", "–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ", "–ø–æ–≤—ã—à–µ–Ω–∏–µ", "–¥–æ–ø–ª–∞—Ç–∞", "–ø–æ—á–µ–º—É —Ç–∞–∫ –¥–æ—Ä–æ–≥–æ", "–æ—Ç–∫—É–¥–∞ –¥–æ–ø–ª–∞—Ç–∞", "–ø–æ–≤—ã—Å–∏–ª–∏ —Ü–µ–Ω—É", "–∑–∞—á–µ–º –¥–æ–ø–ª–∞—á–∏–≤–∞—Ç—å"],
        "synonyms": ["–¥–æ—Ä–æ–≥–æ", "–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ", "–ø–æ–≤—ã—à–µ–Ω–∏–µ", "–¥–æ–ø–ª–∞—Ç–∞", "–ø–æ–≤—ã—Å–∏–ª–∏", "–ø–æ–¥–Ω—è–ª–∏ —Ü–µ–Ω—É"],
        "variations": ["–Ω–∞—Ü–µ–Ω–∫a", "–Ω–∞—Ü–µ–Ω–∫y", "–Ω–∞—Ü–µ–Ω–∫i", "–Ω–∞—Ü–µ–Ω–∫e", "–Ω–∞—Ü–µ–Ω–∫o–π"]
    },
    "–¥–æ—Å—Ç–∞–≤–∫–∞": {
        "answer": "–î–ª—è –∑–∞–∫–∞–∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∏ —á–µ—Ä–µ–∑ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ APARU: 1) –û—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ 2) –í—ã–±–µ—Ä–∏—Ç–µ —Ä–∞–∑–¥–µ–ª '–î–æ—Å—Ç–∞–≤–∫–∞' 3) –£–∫–∞–∂–∏—Ç–µ –∞–¥—Ä–µ—Å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏ –ø–æ–ª—É—á–µ–Ω–∏—è 4) –í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø –¥–æ—Å—Ç–∞–≤–∫–∏ 5) –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –∑–∞–∫–∞–∑. –ö—É—Ä—å–µ—Ä —Å–≤—è–∂–µ—Ç—Å—è —Å –≤–∞–º–∏ –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π.",
        "keywords": ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–¥–æ—Å—Ç–∞–≤–∫–∏", "–¥–æ—Å—Ç–∞–≤–∫—É", "–¥–æ—Å—Ç–∞–≤–∫–æ–π", "–∫—É—Ä—å–µ—Ä", "–ø–æ—Å—ã–ª–∫–∞", "–æ—Ç–ø—Ä–∞–≤–∏—Ç—å", "–∑–∞–∫–∞–∑–∞—Ç—å", "–¥–æ—Å—Ç–∞–≤–∏—Ç—å", "–ø–µ—Ä–µ–¥–∞—Ç—å"],
        "synonyms": ["–∫—É—Ä—å–µ—Ä", "–ø–æ—Å—ã–ª–∫–∞", "–æ—Ç–ø—Ä–∞–≤–∏—Ç—å", "–∑–∞–∫–∞–∑–∞—Ç—å", "–¥–æ—Å—Ç–∞–≤–∏—Ç—å", "–ø–µ—Ä–µ–¥–∞—Ç—å", "–æ—Ç–ø—Ä–∞–≤–∫–∞"],
        "variations": ["–¥–æ—Å—Ç–∞–≤–∫y", "–¥–æ—Å—Ç–∞–≤–∫a", "–¥–æ—Å—Ç–∞–≤–∫i", "–¥–æ—Å—Ç–∞–≤–∫e", "–¥–æ—Å—Ç–∞–≤–∫o–π"]
    },
    "–±–∞–ª–∞–Ω—Å": {
        "answer": "–î–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ APARU: 1) –û—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ 2) –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª '–ü—Ä–æ—Ñ–∏–ª—å' 3) –í—ã–±–µ—Ä–∏—Ç–µ '–ü–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å' 4) –í—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã 5) –í–≤–µ–¥–∏—Ç–µ —Å—É–º–º—É 6) –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏—é. –ë–∞–ª–∞–Ω—Å –ø–æ–ø–æ–ª–Ω–∏—Ç—Å—è –≤ —Ç–µ—á–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–∏–Ω—É—Ç.",
        "keywords": ["–±–∞–ª–∞–Ω—Å", "–±–∞–ª–∞–Ω—Å–∞", "–±–∞–ª–∞–Ω—Å—É", "–±–∞–ª–∞–Ω—Å–æ–º", "—Å—á–µ—Ç", "–∫–æ—à–µ–ª–µ–∫", "–ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–ø–ª–∞—Ç–µ–∂", "–¥–µ–Ω—å–≥–∏", "—Å—Ä–µ–¥—Å—Ç–≤–∞"],
        "synonyms": ["—Å—á–µ—Ç", "–∫–æ—à–µ–ª–µ–∫", "–ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–ø–ª–∞—Ç–µ–∂", "–¥–µ–Ω—å–≥–∏", "—Å—Ä–µ–¥—Å—Ç–≤–∞", "—Ñ–∏–Ω–∞–Ω—Å—ã"],
        "variations": ["–±–∞–ªa–Ω—Å", "–±–∞–ªa–Ωc", "–±–∞–ªa–Ω—Å—É", "–±–∞–ªa–Ω—Å–æ–º", "–±–∞–ªa–Ωca"]
    },
    "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": {
        "answer": "–ï—Å–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ APARU –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: 1) –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ 2) –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É 3) –û–±–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏ 4) –û—á–∏—Å—Ç–∏—Ç–µ –∫—ç—à –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è 5) –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ –Ω–µ —Ä–µ—à–∞–µ—Ç—Å—è, –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
        "keywords": ["–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏—é", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º", "–ø—Ä–æ–≥—Ä–∞–º–º–∞", "—Å–æ—Ñ—Ç", "–∞–ø–ø", "—Ä–∞–±–æ—Ç–∞—Ç—å", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–≥–ª—é—á–∏—Ç", "–≤–∏—Å–∏—Ç"],
        "synonyms": ["–ø—Ä–æ–≥—Ä–∞–º–º–∞", "—Å–æ—Ñ—Ç", "–∞–ø–ø", "—Ä–∞–±–æ—Ç–∞—Ç—å", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–≥–ª—é—á–∏—Ç", "–≤–∏—Å–∏—Ç", "—Ç–æ—Ä–º–æ–∑–∏—Ç"],
        "variations": ["–ø—Ä–∏–ª–æ–∂–µ–Ω–∏e", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏a", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏y", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏i", "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏o–º"]
    }
}

class EnhancedAIModel:
    def __init__(self):
        self.stemmer = SnowballStemmer('russian')
        self.stop_words = set(stopwords.words('russian'))
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words=list(self.stop_words),
            ngram_range=(1, 2)
        )
        self._prepare_knowledge_base()
        
    def _prepare_knowledge_base(self):
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        self.questions = []
        self.answers = []
        self.categories = []
        
        for category, data in ENHANCED_KB.items():
            # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å
            self.questions.append(f"–ß—Ç–æ —Ç–∞–∫–æ–µ {category}?")
            self.answers.append(data["answer"])
            self.categories.append(category)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
            for keyword in data["keywords"]:
                if keyword != category:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                    self.questions.append(f"–ö–∞–∫ {keyword}?")
                    self.answers.append(data["answer"])
                    self.categories.append(category)
        
        # –û–±—É—á–∞–µ–º TF-IDF
        self.tfidf_matrix = self.vectorizer.fit_transform(self.questions)
        logger.info(f"‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞: {len(self.questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    def normalize_text(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç"""
        # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏ –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'[^\w\s]', ' ', text.lower())
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        text = re.sub(r'\s+', ' ', text).strip()
        return text
    
    def find_best_match(self, query: str) -> Dict[str, Any]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        normalized_query = self.normalize_text(query)
        
        # 1. –ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        direct_match = self._direct_search(normalized_query)
        if direct_match["confidence"] > 0.8:
            return direct_match
        
        # 2. Fuzzy –ø–æ–∏—Å–∫
        fuzzy_match = self._fuzzy_search(normalized_query)
        if fuzzy_match["confidence"] > 0.6:
            return fuzzy_match
        
        # 3. TF-IDF –ø–æ–∏—Å–∫
        tfidf_match = self._tfidf_search(normalized_query)
        if tfidf_match["confidence"] > 0.5:
            return tfidf_match
        
        # 4. Fallback
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
            "category": "unknown",
            "confidence": 0.0,
            "source": "fallback",
            "suggestions": self._get_suggestions(normalized_query)
        }
    
    def _direct_search(self, query: str) -> Dict[str, Any]:
        """–ü—Ä—è–º–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        for category, data in ENHANCED_KB.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            for keyword in data["keywords"]:
                if keyword in query:
                    return {
                        "answer": data["answer"],
                        "category": category,
                        "confidence": 0.9,
                        "source": "direct"
                    }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–∏–Ω–æ–Ω–∏–º—ã
            for synonym in data["synonyms"]:
                if synonym in query:
                    return {
                        "answer": data["answer"],
                        "category": category,
                        "confidence": 0.8,
                        "source": "synonym"
                    }
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ (–æ–ø–µ—á–∞—Ç–∫–∏)
            for variation in data["variations"]:
                if variation in query:
                    return {
                        "answer": data["answer"],
                        "category": category,
                        "confidence": 0.7,
                        "source": "variation"
                    }
        
        return {"confidence": 0.0}
    
    def _fuzzy_search(self, query: str) -> Dict[str, Any]:
        """Fuzzy –ø–æ–∏—Å–∫"""
        best_score = 0
        best_match = None
        
        for category, data in ENHANCED_KB.items():
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            all_keywords = data["keywords"] + data["synonyms"] + data["variations"]
            
            for keyword in all_keywords:
                score = fuzz.partial_ratio(query, keyword)
                if score > best_score:
                    best_score = score
                    best_match = {
                        "answer": data["answer"],
                        "category": category,
                        "confidence": score / 100,
                        "source": "fuzzy"
                    }
        
        return best_match if best_match and best_match["confidence"] > 0.6 else {"confidence": 0.0}
    
    def _tfidf_search(self, query: str) -> Dict[str, Any]:
        """TF-IDF –ø–æ–∏—Å–∫"""
        try:
            query_vector = self.vectorizer.transform([query])
            similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()
            
            best_idx = np.argmax(similarities)
            best_score = similarities[best_idx]
            
            if best_score > 0.3:
                return {
                    "answer": self.answers[best_idx],
                    "category": self.categories[best_idx],
                    "confidence": float(best_score),
                    "source": "tfidf"
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ TF-IDF –ø–æ–∏—Å–∫–∞: {e}")
        
        return {"confidence": 0.0}
    
    def _get_suggestions(self, query: str) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ—Ö–æ–∂–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        suggestions = []
        
        for category, data in ENHANCED_KB.items():
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
            suggestions.append(f"–ß—Ç–æ —Ç–∞–∫–æ–µ {category}?")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            for keyword in data["keywords"][:3]:  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3
                if keyword != category:
                    suggestions.append(f"–ö–∞–∫ {keyword}?")
        
        return suggestions[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–æ–¥–µ–ª–∏
ai_model = EnhancedAIModel()

@app.get("/")
async def root():
    return {"message": "APARU Enhanced AI Assistant", "status": "running", "version": "3.0.0"}

@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(
        status="healthy",
        architecture="enhanced",
        timestamp=datetime.now().isoformat()
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞"""
    try:
        result = ai_model.find_best_match(request.text)
        
        return ChatResponse(
            response=result["answer"],
            intent=result["category"],
            confidence=result["confidence"],
            source=result["source"],
            timestamp=datetime.now().isoformat(),
            suggestions=result.get("suggestions", [])
        )
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ /chat: {e}")
        return ChatResponse(
            response="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.",
            intent="error",
            confidence=0.0,
            source="error",
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )

if __name__ == "__main__":
    import uvicorn
    
    # Railway –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é PORT
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
