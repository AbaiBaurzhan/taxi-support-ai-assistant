#!/usr/bin/env python3
"""
üß† –£–õ–£–ß–®–ï–ù–ù–ê–Ø LLM –°–ò–°–¢–ï–ú–ê –° –û–ë–£–ß–ï–ù–ò–ï–ú –ù–ê –í–ê–†–ò–ê–¶–ò–Ø–•
LLM –∏—Å–ø–æ–ª—å–∑—É–µ—Ç question_variations –∏ keywords –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤
"""

import json
import logging
import requests
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="APARU Trained LLM AI", version="6.0.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–¥–µ–ª–∏
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str
    timestamp: str
    suggestions: List[str] = []

class HealthResponse(BaseModel):
    status: str
    architecture: str = "trained_llm"
    timestamp: str
    llm_available: bool = False

class TrainedLLMClient:
    def __init__(self):
        self.ollama_url = os.environ.get("OLLAMA_URL", "http://localhost:11434")
        self.model_name = "aparu-senior-ai"
        self.ollama_available = False
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        self.knowledge_base = self._load_knowledge_base()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        self._check_ollama_model()
    
    def _load_knowledge_base(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"""
        try:
            with open("BZ.txt", "r", encoding="utf-8") as f:
                data = json.load(f)
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: BZ.txt ({len(data)} –≤–æ–ø—Ä–æ—Å–æ–≤)")
                return data
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _check_ollama_model(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama –∏ –º–æ–¥–µ–ª–∏"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if any(m["name"].startswith(self.model_name) for m in models):
                    self.ollama_available = True
                    logger.info(f"‚úÖ Ollama –∏ –º–æ–¥–µ–ª—å '{self.model_name}' –¥–æ—Å—Ç—É–ø–Ω—ã")
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ Ollama")
            else:
                logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")
    
    def find_best_answer(self, question: str) -> Dict[str, Any]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è –æ–±—É—á–µ–Ω–Ω—É—é LLM"""
        start_time = datetime.now()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é LLM –¥–ª—è –ø–æ–∏—Å–∫–∞
        if self.ollama_available:
            try:
                logger.info("üß† –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—É—é LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–∞...")
                result = self._trained_llm_search(question)
                if result:
                    processing_time = (datetime.now() - start_time).total_seconds()
                    logger.info(f"‚úÖ LLM –ø–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {processing_time:.2f}—Å")
                    return result
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –ø–æ–∏—Å–∫–∞: {e}")
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        logger.info("üîÑ Fallback –∫ –ø–æ–∏—Å–∫—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º...")
        return self._keyword_search(question)
    
    def _trained_llm_search(self, question: str) -> Dict[str, Any]:
        """–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±—É—á–µ–Ω–Ω—É—é LLM –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–∞ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –∏ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        try:
            # –°–æ–∑–¥–∞–µ–º –æ–±—É—á–∞—é—â–∏–π –ø—Ä–æ–º–ø—Ç —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            training_prompt = self._create_training_prompt(question)
            
            payload = {
                "model": self.model_name,
                "prompt": training_prompt,
                "stream": False,
                "options": {
                    "temperature": 0.1,   # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
                    "num_predict": 10,    # –ö–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
                    "num_ctx": 1024,      # –î–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
                    "repeat_penalty": 1.0,
                    "top_k": 3,           # –ù–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                    "top_p": 0.8,         # –£–º–µ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                    "stop": ["\n\n", "–û—Ç–≤–µ—Ç:", "–ö–∞—Ç–µ–≥–æ—Ä–∏—è:", "–û–±—ä—è—Å–Ω–µ–Ω–∏–µ:"]  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
                }
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=15  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', '').strip()
                
                # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç LLM
                result = self._parse_llm_response(answer, question)
                if result:
                    return result
                
                logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {answer}")
                return None
            
            logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –æ—à–∏–±–∫—É: {response.status_code}")
            return None
            
        except requests.exceptions.Timeout:
            logger.error(f"‚ùå LLM –ø–æ–∏—Å–∫ —Ç–∞–π–º–∞—É—Ç (>15—Å)")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM –ø–æ–∏—Å–∫–∞: {e}")
            return None
    
    def _create_training_prompt(self, question: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –æ–±—É—á–∞—é—â–∏–π –ø—Ä–æ–º–ø—Ç —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"""
        prompt = f"""–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏ APARU. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: "{question}"

–ë–ê–ó–ê –ó–ù–ê–ù–ò–ô APARU (—Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏):"""

        for i, item in enumerate(self.knowledge_base, 1):
            variations = item.get("question_variations", [])
            keywords = item.get("keywords", [])
            answer = item.get("answer", "")
            
            prompt += f"\n\n{i}. –í–ê–†–ò–ê–¶–ò–ò –í–û–ü–†–û–°–û–í:"
            for variation in variations[:5]:  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 5 –≤–∞—Ä–∏–∞—Ü–∏–π
                prompt += f"\n   - {variation}"
            
            prompt += f"\n   –ö–õ–Æ–ß–ï–í–´–ï –°–õ–û–í–ê: {', '.join(keywords)}"
            prompt += f"\n   –û–¢–í–ï–¢: {answer[:200]}..."
        
        prompt += f"""

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: "{question}"
2. –ù–∞–π–¥–∏ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø—É–Ω–∫—Ç, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–∏–±–æ–ª–µ–µ —Ç–æ—á–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –≤–æ–ø—Ä–æ—Å—É
3. –°—Ä–∞–≤–Ω–∏ –≤–æ–ø—Ä–æ—Å —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
4. –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø—É–Ω–∫—Ç, –≤–µ—Ä–Ω–∏ –µ–≥–æ –Ω–æ–º–µ—Ä (1, 2, 3, –∏ —Ç.–¥.)
5. –ï—Å–ª–∏ –Ω–∏ –æ–¥–∏–Ω –ø—É–Ω–∫—Ç –Ω–µ –ø–æ–¥—Ö–æ–¥–∏—Ç, –≤–µ—Ä–Ω–∏ "0"

–¢–≤–æ–π –æ—Ç–≤–µ—Ç (—Ç–æ–ª—å–∫–æ –Ω–æ–º–µ—Ä):"""

        return prompt
    
    def _parse_llm_response(self, answer: str, question: str) -> Optional[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ò—â–µ–º —á–∏—Å–ª–æ –≤ –æ—Ç–≤–µ—Ç–µ
            import re
            numbers = re.findall(r'\d+', answer)
            if numbers:
                num = int(numbers[0])
                if 1 <= num <= len(self.knowledge_base):
                    # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
                    kb_item = self.knowledge_base[num - 1]
                    return {
                        "answer": kb_item.get("answer", "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"),
                        "category": f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è {num}",
                        "confidence": 0.95,
                        "source": "trained_llm_search"
                    }
                elif num == 0:
                    return {
                        "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –∏–ª–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
                        "category": "unknown",
                        "confidence": 0.5,
                        "source": "trained_llm_no_match"
                    }
            return None
        except:
            return None
    
    def _keyword_search(self, question: str) -> Dict[str, Any]:
        """Fallback –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        question_lower = question.lower()
        
        best_match = None
        best_score = 0
        
        for item in self.knowledge_base:
            keywords = item.get("keywords", [])
            variations = item.get("question_variations", [])
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_matches = 0
            for keyword in keywords:
                if keyword.lower() in question_lower:
                    keyword_matches += 1
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π
            variation_matches = 0
            for variation in variations:
                if variation.lower() in question_lower:
                    variation_matches += 1
            
            # –û–±—â–∏–π —Å—á–µ—Ç
            total_score = keyword_matches + variation_matches
            
            if total_score > best_score:
                best_score = total_score
                best_match = item
        
        if best_match and best_score > 0:
            return {
                "answer": best_match.get("answer", "–û—Ç–≤–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω"),
                "category": "keyword_match",
                "confidence": min(0.9, best_score * 0.2),
                "source": "keyword_search"
            }
        
        # Fallback
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
            "category": "unknown",
            "confidence": 0.0,
            "source": "fallback"
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
trained_llm_client = TrainedLLMClient()

@app.get("/")
async def root():
    return {
        "message": "APARU Trained LLM AI", 
        "status": "running", 
        "version": "6.0.0",
        "architecture": "trained_llm",
        "llm_available": trained_llm_client.ollama_available
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(
        status="healthy",
        architecture="trained_llm",
        timestamp=datetime.now().isoformat(),
        llm_available=trained_llm_client.ollama_available
    )

@app.get("/webapp", response_class=HTMLResponse)
async def webapp():
    """Telegram WebApp –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"""
    try:
        with open("webapp.html", "r", encoding="utf-8") as f:
            html_content = f.read()
        return HTMLResponse(content=html_content)
    except FileNotFoundError:
        return HTMLResponse(
            content="<h1>WebApp –Ω–µ –Ω–∞–π–¥–µ–Ω</h1><p>–§–∞–π–ª webapp.html –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç</p>",
            status_code=404
        )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞"""
    try:
        result = trained_llm_client.find_best_answer(request.text)
        
        return ChatResponse(
            response=result["answer"],
            intent=result["category"],
            confidence=result["confidence"],
            source=result["source"],
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ /chat: {e}")
        return ChatResponse(
            response="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.",
            intent="error",
            confidence=0.0,
            source="error",
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )

if __name__ == "__main__":
    import uvicorn
    
    # Railway –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é PORT
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port)
