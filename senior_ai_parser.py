#!/usr/bin/env python3
"""
üöÄ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä JSON –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π APARU
Senior AI Engineer - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–µ–Ω–∏—è
"""

import json
import logging
import pickle
import numpy as np
from typing import Dict, List, Any, Tuple
from pathlib import Path
from datetime import datetime
import hashlib

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.stem import SnowballStemmer
    from nltk.tokenize import word_tokenize
    nltk.download('stopwords', quiet=True)
    nltk.download('punkt', quiet=True)
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
try:
    from sentence_transformers import SentenceTransformer
    import faiss
    EMBEDDINGS_AVAILABLE = True
except ImportError:
    EMBEDDINGS_AVAILABLE = False

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è fuzzy search
try:
    from fuzzywuzzy import fuzz, process
    FUZZY_AVAILABLE = True
except ImportError:
    FUZZY_AVAILABLE = False

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class SeniorAIParser:
    def __init__(self):
        self.knowledge_base = []
        self.embeddings_model = None
        self.embeddings_index = None
        self.question_embeddings = []
        self.stop_words = set()
        self.stemmer = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._initialize_text_processing()
        self._initialize_embeddings()
    
    def _initialize_text_processing(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –†—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            self.stop_words = set([
                '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ', '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥', '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂', '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å', '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ', '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ', '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞', '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ', '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π', '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞', '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É'
            ])
            
            # –°—Ç–µ–º–º–µ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            if NLTK_AVAILABLE:
                self.stemmer = SnowballStemmer('russian')
            
            logger.info("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞: {e}")
            self.stop_words = set()
            self.stemmer = None
    
    def _initialize_embeddings(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if EMBEDDINGS_AVAILABLE:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
                self.embeddings_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
                logger.info("‚úÖ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
                self.embeddings_model = None
        else:
            logger.warning("‚ö†Ô∏è –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
    
    def parse_json_knowledge_base(self, file_path: str = "BZ.txt") -> List[Dict[str, Any]]:
        """–ü–∞—Ä—Å–∏—Ç JSON –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        logger.info(f"üìö –ü–∞—Ä—Å–∏–º JSON –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {file_path}")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not isinstance(data, list):
                logger.error("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON - –æ–∂–∏–¥–∞–µ—Ç—Å—è –º–∞—Å—Å–∏–≤")
                return []
            
            for i, item in enumerate(data):
                parsed_item = self._parse_knowledge_item(item, i + 1)
                if parsed_item:
                    self.knowledge_base.append(parsed_item)
            
            logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
            return self.knowledge_base
            
        except FileNotFoundError:
            logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return []
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON: {e}")
            return []
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return []
    
    def _parse_knowledge_item(self, item: Dict[str, Any], item_id: int) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            question_variations = item.get('question_variations', [])
            keywords = item.get('keywords', [])
            answer = item.get('answer', '')
            
            if not question_variations or not answer:
                logger.warning(f"‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç {item_id}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–ª–∏ –æ—Ç–≤–µ—Ç")
                return None
            
            # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å - –ø–µ—Ä–≤—ã–π –≤ —Å–ø–∏—Å–∫–µ –≤–∞—Ä–∏–∞—Ü–∏–π
            main_question = question_variations[0]
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            expanded_keywords = self._expand_keywords(keywords, question_variations, answer)
            
            # –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
            category = self._categorize_question_advanced(main_question, answer, keywords)
            
            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            confidence = self._calculate_advanced_confidence(main_question, answer, keywords, question_variations)
            
            parsed_item = {
                'id': item_id,
                'question': main_question,
                'answer': answer,
                'variations': question_variations,
                'keywords': expanded_keywords,
                'category': category,
                'confidence': confidence,
                'source': 'BZ.txt',
                'metadata': {
                    'total_variations': len(question_variations),
                    'total_keywords': len(expanded_keywords),
                    'answer_length': len(answer),
                    'complexity_score': self._calculate_complexity_score(answer),
                    'parsed_at': datetime.now().isoformat()
                }
            }
            
            logger.info(f"‚úÖ –≠–ª–µ–º–µ–Ω—Ç {item_id} –æ–±—Ä–∞–±–æ—Ç–∞–Ω: {main_question[:50]}...")
            return parsed_item
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —ç–ª–µ–º–µ–Ω—Ç–∞ {item_id}: {e}")
            return None
    
    def _expand_keywords(self, keywords: List[str], variations: List[str], answer: str) -> List[str]:
        """–†–∞—Å—à–∏—Ä—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—Ä–∏–∞—Ü–∏–π –∏ –æ—Ç–≤–µ—Ç–∞"""
        expanded = set(keywords)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–∞—Ä–∏–∞—Ü–∏–π
        for variation in variations:
            words = self._extract_keywords_from_text(variation)
            expanded.update(words)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –æ—Ç–≤–µ—Ç–∞
        answer_words = self._extract_keywords_from_text(answer)
        expanded.update(answer_words)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
        filtered_keywords = [kw for kw in expanded if len(kw) > 2 and kw not in self.stop_words]
        return sorted(list(set(filtered_keywords)))[:20]  # –ú–∞–∫—Å–∏–º—É–º 20 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
    
    def _extract_keywords_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return []
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–µ–∫—Å—Ç
        text = text.lower()
        
        # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        import re
        words = re.findall(r'\b\w+\b', text)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
        keywords = [word for word in words if word not in self.stop_words and len(word) > 2]
        
        # –°—Ç–µ–º–º–∏–Ω–≥ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        if self.stemmer:
            keywords = [self.stemmer.stem(word) for word in keywords]
        
        return keywords
    
    def _categorize_question_advanced(self, question: str, answer: str, keywords: List[str]) -> str:
        """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤"""
        text = (question + " " + answer + " ".join(keywords)).lower()
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
        categories = {
            'pricing': ['–Ω–∞—Ü–µ–Ω–∫–∞', '—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '–¥–æ—Ä–æ–≥–æ', '–¥–µ—à–µ–≤–æ', '—Ç–∞—Ä–∏—Ñ', '–∫–æ–º—Ñ–æ—Ä—Ç', '–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç', '–¥–æ–ø–ª–∞—Ç–∞', '–Ω–∞–¥–±–∞–≤–∫–∞', '—Å–ø—Ä–æ—Å', '–ø–æ–¥–æ—Ä–æ–∂–∞–Ω–∏–µ'],
            'booking': ['–∑–∞–∫–∞–∑', '–ø–æ–µ–∑–¥–∫–∞', '—Ç–∞–∫—Å–∏', '–≤—ã–∑–æ–≤', '–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π', '–æ—Ç–º–µ–Ω–∏—Ç—å', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å', '–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞'],
            'payment': ['–±–∞–ª–∞–Ω—Å', '–ø–æ–ø–æ–ª–Ω–∏—Ç—å', '–æ–ø–ª–∞—Ç–∞', '–ø–ª–∞—Ç–µ–∂', '–∫–∞—Ä—Ç–∞', 'qiwi', 'kaspi', '—Ç–µ—Ä–º–∏–Ω–∞–ª', 'id', '–µ–¥–∏–Ω–∏—Ü–∞', '–∫–∞—Å—Å–∞24'],
            'technical': ['–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–º–æ—Ç–æ—á–∞—Å—ã', 'gps', '–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–æ–±–Ω–æ–≤–∏—Ç—å', '–≤—ã–ª–µ—Ç–∞–µ—Ç', '–∑–∞–≤–∏—Å–∞–µ—Ç', 'google play', 'app store'],
            'delivery': ['–¥–æ—Å—Ç–∞–≤–∫–∞', '–∫—É—Ä—å–µ—Ä', '–ø–æ—Å—ã–ª–∫–∞', '–æ—Ç–ø—Ä–∞–≤–∏—Ç—å', '–ø–æ–ª—É—á–∞—Ç–µ–ª—å', '—Ç–µ–ª–µ—Ñ–æ–Ω', '–æ—Ç–∫—É–¥–∞', '–∫—É–¥–∞'],
            'driver': ['–≤–æ–¥–∏—Ç–µ–ª—å', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '—Å–≤—è–∑—å', '–ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–∫–∞–∑—ã', '—Ä–∞–±–æ—Ç–∞—Ç—å', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è', '–ª–µ–Ω—Ç–∞ –∑–∞–∫–∞–∑–æ–≤', '–∫–ª–∏–µ–Ω—Ç', '–ø—Ä–æ–±–Ω—ã–π'],
            'cancellation': ['–æ—Ç–º–µ–Ω–∏—Ç—å', '–æ—Ç–º–µ–Ω–∞', '–æ—Ç–∫–∞–∑', '–ø—Ä–µ–∫—Ä–∞—Ç–∏—Ç—å'],
            'complaint': ['–∂–∞–ª–æ–±–∞', '–ø—Ä–æ–±–ª–µ–º–∞', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω', '–ø–ª–æ—Ö–æ'],
            'general': ['–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ', '—Å–ø–∞—Å–∏–±–æ', '–≤–æ–ø—Ä–æ—Å', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è', '—Ä–∞—Å—Ü–µ–Ω–∫–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ç–∞–∫—Å–æ–º–µ—Ç—Ä', '–∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä']
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category_scores = {}
        for category, cat_keywords in categories.items():
            score = sum(1 for kw in cat_keywords if kw in text)
            category_scores[category] = score
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
        best_category = max(category_scores, key=category_scores.get)
        return best_category if category_scores[best_category] > 0 else 'general'
    
    def _calculate_advanced_confidence(self, question: str, answer: str, keywords: List[str], variations: List[str]) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"""
        confidence = 0.8  # –ë–∞–∑–æ–≤—ã–π –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–∞
        if '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ' in answer.lower():
            confidence += 0.05
        if '–∫–æ–º–∞–Ω–¥–∞ –∞–ø–∞—Ä—É' in answer.lower():
            confidence += 0.05
        if '—Å —É–≤–∞–∂–µ–Ω–∏–µ–º' in answer.lower():
            confidence += 0.05
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –¥–ª–∏–Ω—É –∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—é –æ—Ç–≤–µ—Ç–∞
        if len(answer) > 200:
            confidence += 0.05
        if len(answer) > 400:
            confidence += 0.05
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞—Ü–∏–π
        if len(variations) > 10:
            confidence += 0.05
        if len(variations) > 15:
            confidence += 0.05
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if len(keywords) > 5:
            confidence += 0.05
        if len(keywords) > 10:
            confidence += 0.05
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∑–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤–æ–ø—Ä–æ—Å–∞
        complexity = self._calculate_complexity_score(question)
        if complexity > 0.7:
            confidence += 0.05
        
        return min(confidence, 1.0)
    
    def _calculate_complexity_score(self, text: str) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"""
        if not text:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        words = text.split()
        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0
        sentence_count = text.count('.') + text.count('!') + text.count('?')
        avg_sentence_length = len(words) / sentence_count if sentence_count > 0 else len(words)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1
        complexity = (avg_word_length / 10.0 + avg_sentence_length / 20.0) / 2.0
        return min(complexity, 1.0)
    
    def build_advanced_search_index(self):
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å"""
        if not self.knowledge_base:
            logger.warning("‚ö†Ô∏è –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞, –∏–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω")
            return
        
        logger.info("üîç –°—Ç—Ä–æ–∏–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å")
        
        # –°—Ç—Ä–æ–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –≤–∞—Ä–∏–∞—Ü–∏–π
        if self.embeddings_model:
            try:
                all_texts = []
                text_to_item = {}
                
                for item in self.knowledge_base:
                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å
                    all_texts.append(item['question'])
                    text_to_item[item['question']] = item
                    
                    # –í–∞—Ä–∏–∞—Ü–∏–∏
                    for variation in item['variations']:
                        all_texts.append(variation)
                        text_to_item[variation] = item
                
                # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
                self.question_embeddings = self.embeddings_model.encode(all_texts)
                
                # –°–æ–∑–¥–∞–µ–º FAISS –∏–Ω–¥–µ–∫—Å
                dimension = self.question_embeddings.shape[1]
                self.embeddings_index = faiss.IndexFlatIP(dimension)
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è cosine similarity
                faiss.normalize_L2(self.question_embeddings)
                self.embeddings_index.add(self.question_embeddings)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
                self.text_to_item = text_to_item
                
                logger.info(f"‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {len(all_texts)} —Ç–µ–∫—Å—Ç–æ–≤")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                self.embeddings_index = None
    
    def save_advanced_knowledge_base(self, output_path: str = "senior_ai_knowledge_base.json"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {output_path}")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(self.knowledge_base, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
            return output_path
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return None
    
    def save_search_index(self, output_path: str = "senior_ai_search_index.pkl"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å"""
        if not self.embeddings_index:
            logger.warning("‚ö†Ô∏è –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω")
            return None
        
        try:
            index_data = {
                'embeddings_index': self.embeddings_index,
                'question_embeddings': self.question_embeddings,
                'text_to_item': self.text_to_item
            }
            
            with open(output_path, 'wb') as f:
                pickle.dump(index_data, f)
            
            logger.info(f"‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
            return output_path
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            return None
    
    def get_statistics(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        if not self.knowledge_base:
            return {'error': '–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞'}
        
        categories = {}
        total_variations = 0
        total_keywords = 0
        total_answer_length = 0
        complexity_scores = []
        
        for item in self.knowledge_base:
            cat = item['category']
            categories[cat] = categories.get(cat, 0) + 1
            total_variations += len(item['variations'])
            total_keywords += len(item['keywords'])
            total_answer_length += len(item['answer'])
            complexity_scores.append(item['metadata']['complexity_score'])
        
        return {
            'total_records': len(self.knowledge_base),
            'categories': categories,
            'total_variations': total_variations,
            'total_keywords': total_keywords,
            'avg_variations_per_record': total_variations / len(self.knowledge_base),
            'avg_keywords_per_record': total_keywords / len(self.knowledge_base),
            'avg_answer_length': total_answer_length / len(self.knowledge_base),
            'avg_complexity_score': sum(complexity_scores) / len(complexity_scores),
            'embeddings_available': self.embeddings_model is not None,
            'fuzzy_available': FUZZY_AVAILABLE,
            'nltk_available': NLTK_AVAILABLE
        }

if __name__ == "__main__":
    parser = SeniorAIParser()
    
    print("üöÄ Senior AI Engineer - –ü–∞—Ä—Å–∏–Ω–≥ JSON –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π APARU")
    print("=" * 70)
    
    # –ü–∞—Ä—Å–∏–º JSON –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    parsed_data = parser.parse_json_knowledge_base("BZ.txt")
    
    if parsed_data:
        # –°—Ç—Ä–æ–∏–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        parser.build_advanced_search_index()
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        json_path = parser.save_advanced_knowledge_base()
        index_path = parser.save_search_index()
        
        print(f"\nüéØ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"üìÅ JSON: {json_path}")
        print(f"üîç –ò–Ω–¥–µ–∫—Å: {index_path}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats = parser.get_statistics()
        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        print(f"   –ó–∞–ø–∏—Å–µ–π: {stats['total_records']}")
        print(f"   –í–∞—Ä–∏–∞—Ü–∏–π: {stats['total_variations']}")
        print(f"   –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {stats['total_keywords']}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –≤–∞—Ä–∏–∞—Ü–∏–π –Ω–∞ –∑–∞–ø–∏—Å—å: {stats['avg_variations_per_record']:.1f}")
        print(f"   –°—Ä–µ–¥–Ω–µ–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞ –∑–∞–ø–∏—Å—å: {stats['avg_keywords_per_record']:.1f}")
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞: {stats['avg_answer_length']:.0f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {stats['avg_complexity_score']:.2f}")
        
        print(f"\nüìã –ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for cat, count in stats['categories'].items():
            print(f"   {cat}: {count} –∑–∞–ø–∏—Å–µ–π")
        
        print(f"\nüîß –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
        print(f"   –≠–º–±–µ–¥–¥–∏–Ω–≥–∏: {'‚úÖ' if stats['embeddings_available'] else '‚ùå'}")
        print(f"   Fuzzy search: {'‚úÖ' if stats['fuzzy_available'] else '‚ùå'}")
        print(f"   NLTK –æ–±—Ä–∞–±–æ—Ç–∫–∞: {'‚úÖ' if stats['nltk_available'] else '‚ùå'}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
        print(f"\nüìù –ü—Ä–∏–º–µ—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π:")
        for i, item in enumerate(parsed_data[:3]):
            print(f"   {i+1}. {item['question']}")
            print(f"      –û—Ç–≤–µ—Ç: {item['answer'][:100]}...")
            print(f"      –ö–∞—Ç–µ–≥–æ—Ä–∏—è: {item['category']}")
            print(f"      –í–∞—Ä–∏–∞—Ü–∏–π: {len(item['variations'])}")
            print(f"      –ö–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {len(item['keywords'])}")
            print(f"      –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {item['confidence']:.3f}")
            print()
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞!")
