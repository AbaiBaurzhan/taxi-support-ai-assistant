#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π APARU
–î–æ–±–∞–≤–ª—è–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –±–æ–ª—å—à–µ–π –≥–∏–±–∫–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
"""

import json
import re
from pathlib import Path

def create_enhanced_knowledge_base():
    """–°–æ–∑–¥–∞–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ –≤–æ–ø—Ä–æ—Å–æ–≤"""
    
    # –ß–∏—Ç–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –±–∞–∑—É
    with open('database_Aparu/BZ.txt', 'r', encoding='utf-8') as f:
        content = f.read()
    
    # –ü–∞—Ä—Å–∏–º –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã
    questions_answers = []
    sections = content.split('- question:')
    
    for section in sections[1:]:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–µ—Ä–≤—ã–π –ø—É—Å—Ç–æ–π —ç–ª–µ–º–µ–Ω—Ç
        lines = section.strip().split('\n')
        if len(lines) >= 2:
            question = lines[0].strip()
            answer = '\n'.join(lines[1:]).strip()
            questions_answers.append((question, answer))
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –±–∞–∑—É —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
    enhanced_kb = []
    
    for question, answer in questions_answers:
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        variations = create_question_variations(question)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        keywords = extract_keywords(question)
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å
        kb_entry = {
            "original_question": question,
            "variations": variations,
            "keywords": keywords,
            "answer": answer,
            "category": categorize_question(question)
        }
        
        enhanced_kb.append(kb_entry)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –±–∞–∑—É
    with open('enhanced_aparu_knowledge_base.json', 'w', encoding='utf-8') as f:
        json.dump(enhanced_kb, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π: {len(enhanced_kb)} –∑–∞–ø–∏—Å–µ–π")
    return enhanced_kb

def create_question_variations(question):
    """–°–æ–∑–¥–∞–µ—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–∞"""
    
    # –ë–∞–∑–æ–≤—ã–µ –≤–∞—Ä–∏–∞—Ü–∏–∏
    variations = [question]
    
    # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
    synonyms = {
        "—á—Ç–æ —Ç–∞–∫–æ–µ": ["—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç", "—á—Ç–æ —ç—Ç–æ", "—á—Ç–æ –∑–Ω–∞—á–∏—Ç", "—á—Ç–æ –∑–∞"],
        "–∫–∞–∫": ["–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–∫–∞–∫ –∏–º–µ–Ω–Ω–æ", "–∫–∞–∫ –º–æ–∂–Ω–æ", "–∫–∞–∫ –Ω—É–∂–Ω–æ"],
        "–≥–¥–µ": ["–≤ –∫–∞–∫–æ–º –º–µ—Å—Ç–µ", "–≥–¥–µ –Ω–∞–π—Ç–∏", "–≥–¥–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å", "–≥–¥–µ –∏—Å–∫–∞—Ç—å"],
        "–ø–æ—á–µ–º—É": ["–∑–∞—á–µ–º", "–æ—Ç–∫—É–¥–∞", "–∏–∑-–∑–∞ —á–µ–≥–æ", "–ø–æ –∫–∞–∫–æ–π –ø—Ä–∏—á–∏–Ω–µ"],
        "–º–æ–∂–Ω–æ –ª–∏": ["–≤–æ–∑–º–æ–∂–Ω–æ –ª–∏", "—Ä–µ–∞–ª—å–Ω–æ –ª–∏", "–ø–æ–ª—É—á–∏—Ç—Å—è –ª–∏", "—É–¥–∞—Å—Ç—Å—è –ª–∏"],
        "–∫–∞–∫ —É–∑–Ω–∞—Ç—å": ["–∫–∞–∫ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å", "–∫–∞–∫ –Ω–∞–π—Ç–∏", "–∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å", "–∫–∞–∫ —É–≤–∏–¥–µ—Ç—å"],
        "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å": ["–∫–∞–∫ –≤—ã–ø–æ–ª–Ω–∏—Ç—å", "–∫–∞–∫ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å", "–∫–∞–∫ –ø—Ä–æ–≤–µ—Å—Ç–∏", "–∫–∞–∫ —Ä–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å"],
        "–∫–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å": ["–∫–∞–∫ –∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å", "–∫–∞–∫ –≤—ã–∑–≤–∞—Ç—å", "–∫–∞–∫ –ø–æ–ø—Ä–æ—Å–∏—Ç—å", "–∫–∞–∫ –∑–∞–ø—Ä–æ—Å–∏—Ç—å"],
        "–∫–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å": ["–∫–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å", "–∫–∞–∫ –≤–Ω–µ—Å—Ç–∏", "–∫–∞–∫ –∑–∞–ø–ª–∞—Ç–∏—Ç—å", "–∫–∞–∫ –æ–ø–ª–∞—Ç–∏—Ç—å"],
        "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å": ["–∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è", "–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å", "–∫–∞–∫ –ø—Ä–∏–º–µ–Ω—è—Ç—å", "–∫–∞–∫ —É–ø—Ä–∞–≤–ª—è—Ç—å"]
    }
    
    # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–∏–Ω–æ–Ω–∏–º–æ–≤
    for original, alternatives in synonyms.items():
        if original in question.lower():
            for alt in alternatives:
                new_question = question.lower().replace(original, alt)
                if new_question not in variations:
                    variations.append(new_question.capitalize())
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º–∏
    if "—á—Ç–æ —Ç–∞–∫–æ–µ" in question.lower():
        # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ "—á—Ç–æ —Ç–∞–∫–æ–µ X"
        subject = question.lower().replace("—á—Ç–æ —Ç–∞–∫–æ–µ", "").strip().strip('"').strip("'")
        if subject:
            variations.extend([
                f"–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç {subject}?",
                f"–ß—Ç–æ —ç—Ç–æ {subject}?",
                f"–ß—Ç–æ –∑–Ω–∞—á–∏—Ç {subject}?",
                f"–ß—Ç–æ –∑–∞ {subject}?",
                f"–ü–æ—á–µ–º—É {subject}?",
                f"–ó–∞—á–µ–º {subject}?"
            ])
    
    elif "–∫–∞–∫" in question.lower():
        # –î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ "–∫–∞–∫ X"
        action = question.lower().replace("–∫–∞–∫", "").strip()
        if action:
            variations.extend([
                f"–ö–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º {action}?",
                f"–ö–∞–∫ –∏–º–µ–Ω–Ω–æ {action}?",
                f"–ö–∞–∫ –º–æ–∂–Ω–æ {action}?",
                f"–ö–∞–∫ –Ω—É–∂–Ω–æ {action}?",
                f"–ì–¥–µ {action}?",
                f"–ö–æ–≥–¥–∞ {action}?"
            ])
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
    return list(set(variations))

def extract_keywords(question):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
    
    # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
    stop_words = {
        "—á—Ç–æ", "–∫–∞–∫", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–º–æ–∂–Ω–æ", "–ª–∏", "—ç—Ç–æ", "—Ç–∞–∫–æ–µ",
        "–æ–∑–Ω–∞—á–∞–µ—Ç", "–∑–Ω–∞—á–∏—Ç", "–µ—Å—Ç—å", "–±—ã—Ç—å", "–¥–µ–ª–∞—Ç—å", "—Å–¥–µ–ª–∞—Ç—å", "—É–∑–Ω–∞—Ç—å", "–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å",
        "–Ω–∞–π—Ç–∏", "–Ω–∞–π—Ç–∏", "–ø–æ–ª—É—á–∏—Ç—å", "–≤–∑—è—Ç—å", "–¥–∞—Ç—å", "—Å–∫–∞–∑–∞—Ç—å", "–æ–±—ä—è—Å–Ω–∏—Ç—å"
    }
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –∏ —É–±–∏—Ä–∞–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
    words = re.findall(r'\b\w+\b', question.lower())
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    keywords = [word for word in words if word not in stop_words and len(word) > 2]
    
    return keywords

def categorize_question(question):
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å"""
    
    question_lower = question.lower()
    
    if any(word in question_lower for word in ["–Ω–∞—Ü–µ–Ω–∫–∞", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–¥–æ—Ä–æ–≥–æ"]):
        return "pricing"
    elif any(word in question_lower for word in ["–ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–±–∞–ª–∞–Ω—Å", "—Å—á–µ—Ç", "–æ–ø–ª–∞—Ç–∏—Ç—å"]):
        return "payment"
    elif any(word in question_lower for word in ["–∫–æ–º—Ñ–æ—Ä—Ç", "—Ç–∞—Ä–∏—Ñ", "–∫–ª–∞—Å—Å"]):
        return "tariff"
    elif any(word in question_lower for word in ["—Ä–∞—Å—Ü–µ–Ω–∫–∞", "—Ç–∞—Ä–∏—Ñ", "—Ü–µ–Ω–∞"]):
        return "pricing"
    elif any(word in question_lower for word in ["–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π", "–∑–∞—Ä–∞–Ω–µ–µ", "–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å"]):
        return "booking"
    elif any(word in question_lower for word in ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–∫—É—Ä—å–µ—Ä", "—Ç–æ–≤–∞—Ä"]):
        return "delivery"
    elif any(word in question_lower for word in ["–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–∫–∞–∑—ã", "—Ä–∞–±–æ—Ç–∞—Ç—å"]):
        return "driver"
    elif any(word in question_lower for word in ["–º–æ—Ç–æ—á–∞—Å—ã", "–≤—Ä–µ–º—è", "–ø–æ–µ–∑–¥–∫–∞"]):
        return "pricing"
    elif any(word in question_lower for word in ["—Ç–∞–∫—Å–æ–º–µ—Ç—Ä", "—Ä–∞–±–æ—Ç–∞—Ç—å", "–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è"]):
        return "driver"
    elif any(word in question_lower for word in ["–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–æ–±–ª–µ–º–∞"]):
        return "technical"
    else:
        return "general"

def test_enhanced_knowledge_base():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É
    with open('enhanced_aparu_knowledge_base.json', 'r', encoding='utf-8') as f:
        kb = json.load(f)
    
    print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(kb)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã
    for i, entry in enumerate(kb[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
        print(f"\nüìù –ó–∞–ø–∏—Å—å {i+1}:")
        print(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å: {entry['original_question']}")
        print(f"–í–∞—Ä–∏–∞—Ü–∏–∏ ({len(entry['variations'])}): {entry['variations'][:3]}...")
        print(f"–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {entry['keywords']}")
        print(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {entry['category']}")
    
    return kb

if __name__ == "__main__":
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π APARU...")
    
    # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –±–∞–∑—É
    enhanced_kb = create_enhanced_knowledge_base()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    test_enhanced_knowledge_base()
    
    print("\n‚úÖ –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≥–æ—Ç–æ–≤–∞!")
    print("üìÅ –§–∞–π–ª: enhanced_aparu_knowledge_base.json")
    print("üîÑ –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Å –Ω–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏!")
