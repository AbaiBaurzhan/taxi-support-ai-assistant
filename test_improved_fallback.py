"""
–¢–µ—Å—Ç —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ fallback –ø–æ–∏—Å–∫–∞
"""

import sys
import os
sys.path.append('backend')

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ–∏—Å–∫–∞ –±–µ–∑ FastAPI
import json
import re

def search_faq_standalone(text: str, kb_data: dict):
    """–°—Ç–æ—è–Ω–æ—á–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ FAQ"""
    if not text or not kb_data:
        return None
    
    # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
    faq_items = kb_data.get("faq", [])
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    slang_replacements = {
        '—á–µ': '—á—Ç–æ',
        '—Ç–∞–º': '',
        '–ø–æ': '',
        '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç': '–∫–∞–∫',
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': '—Ä–∞–±–æ—Ç–∞',
        '–∫–∞–∫': '–∫–∞–∫',
        '–≥–¥–µ': '–≥–¥–µ',
        '—á—Ç–æ': '—á—Ç–æ',
        '–∫–∞–∫–∏–µ': '–∫–∞–∫–∏–µ',
        '–º–æ–∂–Ω–æ': '–º–æ–∂–Ω–æ',
        '–Ω—É–∂–Ω–æ': '–Ω—É–∂–Ω–æ',
        '–µ—Å—Ç—å': '–µ—Å—Ç—å',
        '–±—ã—Ç—å': '–±—ã—Ç—å',
        '–≤—Ä–µ–º—è': '–≤—Ä–µ–º—è',
        '–¥–µ–Ω—å–≥–∏': '–¥–µ–Ω—å–≥–∏',
        '—Ü–µ–Ω–∞': '—Ü–µ–Ω–∞',
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': '—Å—Ç–æ–∏–º–æ—Å—Ç—å'
    }
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    text_cleaned = re.sub(r'[^\w\s]', ' ', text_lower)
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    
    processed_text = text_lower
    for slang, replacement in slang_replacements.items():
        processed_text = processed_text.replace(slang, replacement)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    processed_text = ' '.join(processed_text.split())
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    def calculate_tfidf_score(query_words, doc_words):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not query_words or not doc_words:
            return 0
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        common_words = set(query_words).intersection(set(doc_words))
        if not common_words:
            return 0
        
        # –ü—Ä–æ—Å—Ç–æ–π TF-IDF: log(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤ / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        tf_score = len(common_words) / len(doc_words)
        idf_score = 1 + (len(common_words) / len(query_words))
        
        return tf_score * idf_score
    
    best_match = None
    best_score = 0
    
    for item in faq_items:
        score = 0
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
        keywords = item.get("keywords", [])
        keyword_score = 0
        for keyword in keywords:
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ = –≤—ã—Å—à–∏–π –±–∞–ª–ª
            if keyword in text_cleaned:
                keyword_score += 3
            elif keyword in text_lower or keyword in processed_text:
                keyword_score += 2
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            elif any(keyword in word or word in keyword for word in text_cleaned.split()):
                keyword_score += 1
        
        # –ü–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤ (TF-IDF)
        variations = item.get("question_variations", [])
        variation_score = 0
        query_words = text_cleaned.split()
        for variation in variations:
            variation_words = variation.lower().split()
            tfidf_score = calculate_tfidf_score(query_words, variation_words)
            variation_score += tfidf_score
        
        # –ü–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É (TF-IDF)
        main_question = item.get("question", "").lower()
        main_question_words = main_question.split()
        main_score = calculate_tfidf_score(query_words, main_question_words) * 2  # –£–¥–≤–∞–∏–≤–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–ª–æ–≤ –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
        max_possible_keywords = len(keywords) * 3  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        normalized_keyword_score = keyword_score / max_possible_keywords if max_possible_keywords > 0 else 0
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å –≤–µ—Å–∞–º–∏ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π
        score = (
            normalized_keyword_score * 4.0 +    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ = —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ)
            variation_score * 1.0 +             # –í–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (TF-IDF)
            main_score * 2.0                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å (TF-IDF —É–¥–≤–æ–µ–Ω–Ω—ã–π)
        )
        
        if score > best_score:
            best_score = score
            best_match = item
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–∞–ª–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–π
    if best_score >= 0.5:  # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
        return best_match
    
    return None

def load_kb_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    try:
        with open('backend/kb.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ kb.json: {e}")
        return None

def test_improved_fallback():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π fallback –ø–æ–∏—Å–∫"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ fallback –ø–æ–∏—Å–∫–∞")
    print("=" * 60)
    
    kb_data = load_kb_data()
    if not kb_data:
        return
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–¥–æ—Å—Ç–∞–≤–∫–∞",
        "—á–µ —Ç–∞–º –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ",
        "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–æ—Å—Ç–∞–≤–∫–∞",
        "—Ü–µ–Ω–∞ –ø–æ–µ–∑–¥–∫–∏",
        "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç —Ç–∞–∫—Å–∏",
        "–∫–∞–∫–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è",
        "–±–∞–ª–∞–Ω—Å –Ω–∞ –∫–∞—Ä—Ç–µ",
        "–∫–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å —Å—á–µ—Ç",
        "–≤–æ–¥–∏—Ç–µ–ª—å –Ω–µ –ø—Ä–∏–µ—Ö–∞–ª",
        "–≥–¥–µ –º–æ–π –∑–∞–∫–∞–∑",
        "–∫–æ–º—Ñ–æ—Ä—Ç –∫–ª–∞—Å—Å",
        "–ø—Ä–µ–¥–∑–∞–∫–∞–∑ –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å",
        "–Ω–∞—Ü–µ–Ω–∫–∞ –∑–∞ —á—Ç–æ",
        "–º–æ—Ç–æ—á–∞—Å—ã —á—Ç–æ —ç—Ç–æ",
        "—Ç–∞–∫—Å–æ–º–µ—Ç—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç",
        "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–ª—é—á–∏—Ç"
    ]
    
    print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º {len(test_queries)} –∑–∞–ø—Ä–æ—Å–æ–≤:")
    print()
    
    success_count = 0
    for i, query in enumerate(test_queries, 1):
        print(f"{i:2d}. –ó–∞–ø—Ä–æ—Å: '{query}'")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º fallback –ø–æ–∏—Å–∫
        result = search_faq_standalone(query, kb_data)
        
        if result:
            print(f"    ‚úÖ –ù–ê–ô–î–ï–ù–û!")
            print(f"    üìã Question: {result.get('question', '')[:60]}...")
            success_count += 1
        else:
            print(f"    ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û")
        
        print()
    
    success_rate = (success_count / len(test_queries)) * 100
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"–£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω–æ: {success_count}/{len(test_queries)} ({success_rate:.1f}%)")
    
    if success_rate >= 80:
        print("üéâ –û—Ç–ª–∏—á–Ω–æ! Fallback —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–æ!")
    elif success_rate >= 60:
        print("‚úÖ –•–æ—Ä–æ—à–æ! –ï—Å—Ç—å –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è!")
    else:
        print("‚ùå –ù—É–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è")

if __name__ == "__main__":
    test_improved_fallback()
