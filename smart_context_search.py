#!/usr/bin/env python3
"""
–£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –¥–ª—è APARU LLM
–ü–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–º—ã—Å–ª –≤–æ–ø—Ä–æ—Å–æ–≤, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
"""

import json
import re
from pathlib import Path
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from typing import List, Dict, Tuple

class SmartContextSearch:
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞"""
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
        self.knowledge_base = []
        
        # –ò–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        self.index = None
        self.dimension = 384  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        
        # –°–ª–æ–≤–∞—Ä—å —Å–∏–Ω–æ–Ω–∏–º–æ–≤ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–≤—è–∑–µ–π
        self.context_mappings = self._create_context_mappings()
        
    def _create_context_mappings(self):
        """–°–æ–∑–¥–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö —Å–≤—è–∑–µ–π"""
        
        return {
            # –ì—Ä—É–ø–ø—ã —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ–Ω—è—Ç–∏–π
            "pricing": {
                "keywords": ["—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ä–∞—Å—Ü–µ–Ω–∫–∞", "—Ç–∞—Ä–∏—Ñ", "–Ω–∞—Ü–µ–Ω–∫–∞", "–¥–æ—Ä–æ–≥–æ", "–¥–µ—à–µ–≤–æ", "–æ–ø–ª–∞—Ç–∞", "–ø–ª–∞—Ç–µ–∂"],
                "contexts": ["—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç", "–ø–æ—á–µ–º—É –¥–æ—Ä–æ–≥–æ", "–æ—Ç–∫—É–¥–∞ —Ü–µ–Ω–∞", "—á—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞", "–∫–∞–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Ü–µ–Ω–∞"]
            },
            "payment": {
                "keywords": ["–ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–±–∞–ª–∞–Ω—Å", "—Å—á–µ—Ç", "–¥–µ–Ω—å–≥–∏", "–æ–ø–ª–∞—Ç–∏—Ç—å", "–ø–ª–∞—Ç–µ–∂", "–∫–æ—à–µ–ª–µ–∫", "–∫–∞—Ä—Ç–∞"],
                "contexts": ["–∫–∞–∫ –∑–∞–ø–ª–∞—Ç–∏—Ç—å", "–≥–¥–µ –ø–æ–ø–æ–ª–Ω–∏—Ç—å", "–∫–∞–∫ –¥–æ–±–∞–≤–∏—Ç—å –¥–µ–Ω—å–≥–∏", "–∫–∞–∫ –≤–Ω–µ—Å—Ç–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞"]
            },
            "booking": {
                "keywords": ["–∑–∞–∫–∞–∑–∞—Ç—å", "–≤—ã–∑–≤–∞—Ç—å", "–∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å", "–ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π", "–∑–∞—Ä–∞–Ω–µ–µ", "–≤—Ä–µ–º—è"],
                "contexts": ["–∫–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å —Ç–∞–∫—Å–∏", "–º–æ–∂–Ω–æ –ª–∏ –∑–∞—Ä–∞–Ω–µ–µ", "–∫–∞–∫ –∑–∞–±—Ä–æ–Ω–∏—Ä–æ–≤–∞—Ç—å", "–∫–æ–≥–¥–∞ –∑–∞–∫–∞–∑—ã–≤–∞—Ç—å"]
            },
            "driver": {
                "keywords": ["–≤–æ–¥–∏—Ç–µ–ª—å", "—Ä–∞–±–æ—Ç–∞—Ç—å", "–∑–∞–∫–∞–∑—ã", "–ø–∞—Ä—Ç–Ω–µ—Ä", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è", "—Ç–∞–∫—Å–æ–º–µ—Ç—Ä"],
                "contexts": ["–∫–∞–∫ —Å—Ç–∞—Ç—å –≤–æ–¥–∏—Ç–µ–ª–µ–º", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å", "–∫–∞–∫ –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–∞–∫–∞–∑—ã", "–∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–∞–∫—Å–æ–º–µ—Ç—Ä–æ–º"]
            },
            "delivery": {
                "keywords": ["–¥–æ—Å—Ç–∞–≤–∫–∞", "–∫—É—Ä—å–µ—Ä", "—Ç–æ–≤–∞—Ä", "–≤–µ—â–∏", "–¥–æ–∫—É–º–µ–Ω—Ç—ã", "–ø–æ—Å—ã–ª–∫–∞"],
                "contexts": ["–∫–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É", "–∫–∞–∫ –¥–æ—Å—Ç–∞–≤–∏—Ç—å", "–∫—É—Ä—å–µ—Ä—Å–∫–∞—è —Å–ª—É–∂–±–∞"]
            },
            "technical": {
                "keywords": ["–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–æ–±–ª–µ–º–∞", "–æ—à–∏–±–∫–∞", "–∑–∞–≤–∏—Å–∞–µ—Ç", "–Ω–µ –∑–∞–ø—É—Å–∫–∞–µ—Ç—Å—è"],
                "contexts": ["–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç", "—á—Ç–æ –¥–µ–ª–∞—Ç—å", "–∫–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å", "–ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º"]
            }
        }
    
    def load_knowledge_base(self, kb_file: str = "database_Aparu/BZ.txt"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π"""
        
        print("üìö –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π...")
        
        # –ß–∏—Ç–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –±–∞–∑—É
        with open(kb_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # –ü–∞—Ä—Å–∏–º –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã
        sections = content.split('- question:')
        
        for section in sections[1:]:
            lines = section.strip().split('\n')
            if len(lines) >= 2:
                question = lines[0].strip()
                answer = '\n'.join(lines[1:]).strip()
                
                # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—É—é –∑–∞–ø–∏—Å—å
                kb_entry = {
                    "question": question,
                    "answer": answer,
                    "keywords": self._extract_keywords(question),
                    "category": self._categorize_question(question),
                    "contexts": self._generate_contexts(question),
                    "embeddings": None  # –ë—É–¥–µ—Ç –∑–∞–ø–æ–ª–Ω–µ–Ω–æ –ø–æ–∑–∂–µ
                }
                
                self.knowledge_base.append(kb_entry)
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.knowledge_base)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        self._create_embeddings()
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        self._build_index()
        
        return self.knowledge_base
    
    def _extract_keywords(self, question: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {
            "—á—Ç–æ", "–∫–∞–∫", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º", "–º–æ–∂–Ω–æ", "–ª–∏", "—ç—Ç–æ", "—Ç–∞–∫–æ–µ",
            "–æ–∑–Ω–∞—á–∞–µ—Ç", "–∑–Ω–∞—á–∏—Ç", "–µ—Å—Ç—å", "–±—ã—Ç—å", "–¥–µ–ª–∞—Ç—å", "—Å–¥–µ–ª–∞—Ç—å", "—É–∑–Ω–∞—Ç—å", "–ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å",
            "–Ω–∞–π—Ç–∏", "–ø–æ–ª—É—á–∏—Ç—å", "–≤–∑—è—Ç—å", "–¥–∞—Ç—å", "—Å–∫–∞–∑–∞—Ç—å", "–æ–±—ä—è—Å–Ω–∏—Ç—å", "–º–Ω–µ", "–º–Ω–µ", "—è"
        }
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞
        words = re.findall(r'\b\w+\b', question.lower())
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º
        keywords = [word for word in words if word not in stop_words and len(word) > 2]
        
        return keywords
    
    def _categorize_question(self, question: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        
        question_lower = question.lower()
        
        for category, data in self.context_mappings.items():
            if any(keyword in question_lower for keyword in data["keywords"]):
                return category
        
        return "general"
    
    def _generate_contexts(self, question: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –≤–æ–ø—Ä–æ—Å–∞"""
        
        contexts = [question]
        question_lower = question.lower()
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è
        for category, data in self.context_mappings.items():
            if any(keyword in question_lower for keyword in data["keywords"]):
                contexts.extend(data["contexts"])
        
        # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω–æ–Ω–∏–º–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        synonyms = {
            "—á—Ç–æ —Ç–∞–∫–æ–µ": ["—á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç", "—á—Ç–æ —ç—Ç–æ", "—á—Ç–æ –∑–Ω–∞—á–∏—Ç", "—á—Ç–æ –∑–∞"],
            "–∫–∞–∫": ["–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º", "–∫–∞–∫ –∏–º–µ–Ω–Ω–æ", "–∫–∞–∫ –º–æ–∂–Ω–æ", "–∫–∞–∫ –Ω—É–∂–Ω–æ"],
            "–≥–¥–µ": ["–≤ –∫–∞–∫–æ–º –º–µ—Å—Ç–µ", "–≥–¥–µ –Ω–∞–π—Ç–∏", "–≥–¥–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å", "–≥–¥–µ –∏—Å–∫–∞—Ç—å"],
            "–ø–æ—á–µ–º—É": ["–∑–∞—á–µ–º", "–æ—Ç–∫—É–¥–∞", "–∏–∑-–∑–∞ —á–µ–≥–æ", "–ø–æ –∫–∞–∫–æ–π –ø—Ä–∏—á–∏–Ω–µ"],
            "–º–æ–∂–Ω–æ –ª–∏": ["–≤–æ–∑–º–æ–∂–Ω–æ –ª–∏", "—Ä–µ–∞–ª—å–Ω–æ –ª–∏", "–ø–æ–ª—É—á–∏—Ç—Å—è –ª–∏", "—É–¥–∞—Å—Ç—Å—è –ª–∏"]
        }
        
        for original, alternatives in synonyms.items():
            if original in question_lower:
                for alt in alternatives:
                    new_context = question_lower.replace(original, alt)
                    if new_context not in contexts:
                        contexts.append(new_context.capitalize())
        
        return list(set(contexts))
    
    def _create_embeddings(self):
        """–°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π"""
        
        print("üß† –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
        
        for entry in self.knowledge_base:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            main_embedding = self.model.encode(entry["question"])
            
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            context_embeddings = []
            for context in entry["contexts"]:
                context_embeddings.append(self.model.encode(context))
            
            # –£—Å—Ä–µ–¥–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
            if context_embeddings:
                avg_embedding = np.mean([main_embedding] + context_embeddings, axis=0)
            else:
                avg_embedding = main_embedding
            
            entry["embeddings"] = avg_embedding
        
        print("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã")
    
    def _build_index(self):
        """–°–æ–∑–¥–∞–µ—Ç FAISS –∏–Ω–¥–µ–∫—Å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        
        print("üîç –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å...")
        
        # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
        self.index = faiss.IndexFlatIP(self.dimension)  # Inner Product –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞
        embeddings = np.array([entry["embeddings"] for entry in self.knowledge_base])
        faiss.normalize_L2(embeddings)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏–Ω–¥–µ–∫—Å
        self.index.add(embeddings)
        
        print("‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω")
    
    def search(self, query: str, top_k: int = 3) -> List[Dict]:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.model.encode(query)
        query_embedding = query_embedding.reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏
        scores, indices = self.index.search(query_embedding, top_k)
        
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.knowledge_base):
                entry = self.knowledge_base[idx].copy()
                entry["similarity_score"] = float(score)
                entry["rank"] = i + 1
                results.append(entry)
        
        return results
    
    def get_contextual_answer(self, query: str) -> Dict:
        """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å"""
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø–∏—Å–∏
        results = self.search(query, top_k=3)
        
        if not results:
            return {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à–µ–ª –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –æ—Ç–≤–µ—Ç–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
                "confidence": 0.0,
                "source": "fallback"
            }
        
        # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        best_match = results[0]
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = best_match["similarity_score"]
        
        if confidence > 0.7:
            # –í—ã—Å–æ–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç
            return {
                "answer": best_match["answer"],
                "confidence": confidence,
                "source": "exact_match",
                "matched_question": best_match["question"],
                "category": best_match["category"]
            }
        elif confidence > 0.5:
            # –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
            return {
                "answer": best_match["answer"],
                "confidence": confidence,
                "source": "contextual_match",
                "matched_question": best_match["question"],
                "category": best_match["category"]
            }
        else:
            # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—â–∏–π –æ—Ç–≤–µ—Ç
            return {
                "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ —É–≤–µ—Ä–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ. –í–æ–∑–º–æ–∂–Ω–æ, –≤—ã –∏–º–µ–ª–∏ –≤ –≤–∏–¥—É —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ?",
                "confidence": confidence,
                "source": "low_confidence",
                "suggestions": [r["question"] for r in results[:3]]
            }

def test_smart_search():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞"""
    
    print("üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–º–Ω—É—é —Å–∏—Å—Ç–µ–º—É –ø–æ–∏—Å–∫–∞...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏—Å—Ç–µ–º—É
    smart_search = SmartContextSearch()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
    smart_search.load_knowledge_base()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        "–ü–æ—á–µ–º—É —Ç–∞–∫ –¥–æ—Ä–æ–≥–æ?",
        "–ö–∞–∫ –∑–∞–ø–ª–∞—Ç–∏—Ç—å –∑–∞ –ø–æ–µ–∑–¥–∫—É?",
        "–ú–æ–∂–Ω–æ –ª–∏ –∑–∞–∫–∞–∑–∞—Ç—å –∑–∞—Ä–∞–Ω–µ–µ?",
        "–ö–∞–∫ —Å—Ç–∞—Ç—å –≤–æ–¥–∏—Ç–µ–ª–µ–º?",
        "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç—Å—è",
        "–ß—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –Ω–∞—Ü–µ–Ω–∫–∞?",
        "–ì–¥–µ –Ω–∞–π—Ç–∏ —Ü–µ–Ω—É?",
        "–ö–∞–∫ –¥–æ—Å—Ç–∞–≤–∏—Ç—å –ø–æ—Å—ã–ª–∫—É?"
    ]
    
    print("\nüîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    
    for query in test_queries:
        print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {query}")
        
        result = smart_search.get_contextual_answer(query)
        
        print(f"üìù –û—Ç–≤–µ—Ç: {result['answer'][:100]}...")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")
        print(f"üìÇ –ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']}")
        
        if 'matched_question' in result:
            print(f"üîó –°–æ–≤–ø–∞–≤—à–∏–π –≤–æ–ø—Ä–æ—Å: {result['matched_question']}")
        
        if 'suggestions' in result:
            print(f"üí° –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {result['suggestions']}")

if __name__ == "__main__":
    print("üöÄ –°–æ–∑–¥–∞–Ω–∏–µ —É–º–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞ APARU...")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    test_smart_search()
    
    print("\n‚úÖ –£–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–æ–∏—Å–∫–∞ –≥–æ—Ç–æ–≤–∞!")
    print("üß† –¢–µ–ø–µ—Ä—å –º–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Å–º—ã—Å–ª –≤–æ–ø—Ä–æ—Å–æ–≤!")
