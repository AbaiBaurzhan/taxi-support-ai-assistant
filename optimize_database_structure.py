#!/usr/bin/env python3
"""
üßπ –û—á–∏—Å—Ç–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö
–£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ñ–∞–π–ª—ã, –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ
"""

import os
import json
import logging
from typing import Dict, List, Any

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class DatabaseOptimizer:
    def __init__(self):
        # –§–∞–π–ª—ã –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è (—É—Å—Ç–∞—Ä–µ–≤—à–∏–µ)
        self.files_to_delete = [
            'enhanced_aparu_knowledge.json',
            'professional_aparu_knowledge.json', 
            'final_professional_aparu_knowledge.json',
            'fixed_professional_aparu_knowledge.json',
            'expanded_aparu_knowledge.json',
            'enhanced_aparu_knowledge_base.json',
            'aparu_knowledge_base.json',
            'aparu_knowledge_index.pkl',
            'enhanced_search_index.pkl',
            'senior_ai_search_index.pkl'
        ]
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã (–æ—Å—Ç–∞–≤–ª—è–µ–º)
        self.essential_files = [
            'BZ.txt',  # –ò—Å—Ö–æ–¥–Ω–∞—è –±–∞–∑–∞
            'expanded_knowledge_base.json',  # –°–∏–Ω–æ–Ω–∏–º—ã
            'senior_ai_knowledge_base.json',  # –†–∞–±–æ—á–∞—è –±–∞–∑–∞
            'fixtures.json'  # –ú–æ–∫–∏
        ]
    
    def analyze_current_structure(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑ –¥–∞–Ω–Ω—ã—Ö"""
        analysis = {
            'total_files': 0,
            'essential_files': [],
            'outdated_files': [],
            'missing_files': [],
            'file_sizes': {}
        }
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
        for file in self.files_to_delete + self.essential_files:
            if os.path.exists(file):
                analysis['total_files'] += 1
                analysis['file_sizes'][file] = os.path.getsize(file)
                
                if file in self.essential_files:
                    analysis['essential_files'].append(file)
                else:
                    analysis['outdated_files'].append(file)
            else:
                if file in self.essential_files:
                    analysis['missing_files'].append(file)
        
        return analysis
    
    def create_unified_structure(self) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        unified_structure = {
            'metadata': {
                'version': '2.0',
                'created_at': '2025-09-19',
                'description': 'Unified APARU Knowledge Base',
                'total_categories': 4,
                'total_synonyms': 764
            },
            'sources': {
                'primary': 'BZ.txt',
                'synonyms': 'expanded_knowledge_base.json',
                'working': 'senior_ai_knowledge_base.json'
            },
            'categories': {
                '–Ω–∞—Ü–µ–Ω–∫–∞': {
                    'description': '–í–æ–ø—Ä–æ—Å—ã –æ –Ω–∞—Ü–µ–Ω–∫–∞—Ö, –¥–æ–ø–ª–∞—Ç–∞—Ö, –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞—Ö',
                    'synonyms_count': 200,
                    'examples': ['–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞?', '–ü–æ—á–µ–º—É —Ç–∞–∫ –¥–æ—Ä–æ–≥–æ?']
                },
                '–¥–æ—Å—Ç–∞–≤–∫–∞': {
                    'description': '–í–æ–ø—Ä–æ—Å—ã –æ –¥–æ—Å—Ç–∞–≤–∫–µ, –∫—É—Ä—å–µ—Ä–∞—Ö, –ø–æ—Å—ã–ª–∫–∞—Ö',
                    'synonyms_count': 200,
                    'examples': ['–ö–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É?', '–ö–∞–∫ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –ø–æ—Å—ã–ª–∫—É?']
                },
                '–±–∞–ª–∞–Ω—Å': {
                    'description': '–í–æ–ø—Ä–æ—Å—ã –æ –±–∞–ª–∞–Ω—Å–µ, —Å—á–µ—Ç–µ, –ø–æ–ø–æ–ª–Ω–µ–Ω–∏–∏',
                    'synonyms_count': 150,
                    'examples': ['–ö–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å?', '–ü–æ–ø–æ–ª–Ω–∏—Ç—å —Å—á–µ—Ç']
                },
                '–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ': {
                    'description': '–í–æ–ø—Ä–æ—Å—ã –æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏, –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è—Ö, —Ä–∞–±–æ—Ç–µ',
                    'synonyms_count': 200,
                    'examples': ['–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç', '–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è']
                }
            }
        }
        
        return unified_structure
    
    def cleanup_outdated_files(self) -> bool:
        """–£–¥–∞–ª—è–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ñ–∞–π–ª—ã"""
        deleted_count = 0
        
        for file in self.files_to_delete:
            if os.path.exists(file):
                try:
                    os.remove(file)
                    deleted_count += 1
                    logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω —É—Å—Ç–∞—Ä–µ–≤—à–∏–π —Ñ–∞–π–ª: {file}")
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {file}: {e}")
        
        logger.info(f"‚úÖ –£–¥–∞–ª–µ–Ω–æ {deleted_count} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤")
        return deleted_count > 0
    
    def create_structure_documentation(self) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ"""
        doc = """
# üìö –°–¢–†–£–ö–¢–£–†–ê –ë–ê–ó –î–ê–ù–ù–´–• APARU AI

## üéØ –û–°–ù–û–í–ù–´–ï –§–ê–ô–õ–´:

### 1. `BZ.txt` - –ò–°–•–û–î–ù–ê–Ø –ë–ê–ó–ê
- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –ò—Å—Ç–æ—á–Ω–∏–∫ –∏—Å—Ç–∏–Ω—ã, –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –æ—Ç–≤–µ—Ç—ã
- **–§–æ—Ä–º–∞—Ç:** JSON —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏, –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏, –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –û–°–ù–û–í–ù–û–ô

### 2. `expanded_knowledge_base.json` - –°–ò–ù–û–ù–ò–ú–´
- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** 764 —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã—Ö —Å–∏–Ω–æ–Ω–∏–º–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
- **–°–æ–¥–µ—Ä–∂–∏—Ç:** –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–æ—Ä–º—ã, —Å–∏–Ω–æ–Ω–∏–º—ã, —Ñ—Ä–∞–∑—ã
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô

### 3. `senior_ai_knowledge_base.json` - –†–ê–ë–û–ß–ê–Ø –ë–ê–ó–ê
- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –±–∞–∑–∞ –¥–ª—è AI —Å–∏—Å—Ç–µ–º—ã
- **–°–æ–¥–µ—Ä–∂–∏—Ç:** –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –†–ê–ë–û–ß–ò–ô

### 4. `fixtures.json` - –ú–û–ö–ò
- **–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ:** –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- **–°–æ–¥–µ—Ä–∂–∏—Ç:** –ü–æ–µ–∑–¥–∫–∏, —á–µ–∫–∏, –∫–∞—Ä—Ç—ã, —Ç–∏–∫–µ—Ç—ã
- **–°—Ç–∞—Ç—É—Å:** ‚úÖ –¢–ï–°–¢–û–í–´–ô

## üîß –ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢ –°–ò–°–¢–ï–ú–ê:

1. **BZ.txt** ‚Üí –ø–∞—Ä—Å–∏—Ç—Å—è –≤ **senior_ai_knowledge_base.json**
2. **expanded_knowledge_base.json** ‚Üí –¥–æ–±–∞–≤–ª—è–µ—Ç —Å–∏–Ω–æ–Ω–∏–º—ã
3. **enhanced_search_client.py** ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ–±–µ –±–∞–∑—ã
4. **main.py** ‚Üí –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã

## üìä –†–ï–ó–£–õ–¨–¢–ê–¢:
- **100% —Ç–æ—á–Ω–æ—Å—Ç—å** –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö
- **764 —Å–∏–Ω–æ–Ω–∏–º–∞** –≤ 4 –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
- **–ï–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞** –±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
"""
        return doc

def main():
    optimizer = DatabaseOptimizer()
    
    print("üßπ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –°–¢–†–£–ö–¢–£–†–´ –ë–ê–ó –î–ê–ù–ù–´–•")
    print("=" * 50)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    analysis = optimizer.analyze_current_structure()
    print(f"üìä –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã:")
    print(f"   –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {analysis['total_files']}")
    print(f"   –û—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(analysis['essential_files'])}")
    print(f"   –£—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤: {len(analysis['outdated_files'])}")
    print(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤: {len(analysis['missing_files'])}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤
    print(f"\nüìÅ –†–∞–∑–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤:")
    for file, size in analysis['file_sizes'].items():
        size_kb = size / 1024
        print(f"   {file}: {size_kb:.1f} KB")
    
    # –°–æ–∑–¥–∞–µ–º –µ–¥–∏–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
    unified = optimizer.create_unified_structure()
    print(f"\nüéØ –ï–¥–∏–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:")
    print(f"   –í–µ—Ä—Å–∏—è: {unified['metadata']['version']}")
    print(f"   –ö–∞—Ç–µ–≥–æ—Ä–∏–π: {unified['metadata']['total_categories']}")
    print(f"   –°–∏–Ω–æ–Ω–∏–º–æ–≤: {unified['metadata']['total_synonyms']}")
    
    # –°–æ–∑–¥–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    doc = optimizer.create_structure_documentation()
    with open('DATABASE_STRUCTURE.md', 'w', encoding='utf-8') as f:
        f.write(doc)
    
    print(f"\n‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞: DATABASE_STRUCTURE.md")
    
    # –ü—Ä–µ–¥–ª–∞–≥–∞–µ–º –æ—á–∏—Å—Ç–∫—É
    if analysis['outdated_files']:
        print(f"\nüóëÔ∏è –ù–∞–π–¥–µ–Ω–æ {len(analysis['outdated_files'])} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö —Ñ–∞–π–ª–æ–≤:")
        for file in analysis['outdated_files']:
            print(f"   - {file}")
        
        response = input(f"\n‚ùì –£–¥–∞–ª–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ —Ñ–∞–π–ª—ã? (y/n): ")
        if response.lower() == 'y':
            optimizer.cleanup_outdated_files()
        else:
            print("‚è≠Ô∏è –û—á–∏—Å—Ç–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
    
    print(f"\nüéâ –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
    print(f"   –û—Å–Ω–æ–≤–Ω—ã–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    print(f"   –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    print(f"   –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞")

if __name__ == "__main__":
    main()
