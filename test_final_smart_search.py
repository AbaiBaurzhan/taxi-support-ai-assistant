"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–æ—Ä–Ω–µ–π
"""

import sys
import os
sys.path.append('backend')

import json
import re

def load_kb_data():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π"""
    try:
        with open('backend/kb.json', 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ kb.json: {e}")
        return None

def extract_word_root(word):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ—Ä–µ–Ω—å —Å–ª–æ–≤–∞, —É–±–∏—Ä–∞—è –æ–∫–æ–Ω—á–∞–Ω–∏—è, —Å—É—Ñ—Ñ–∏–∫—Å—ã –∏ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏"""
    if len(word) < 4:
        return word
    
    root = word.lower()
    
    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–ª–æ–≤
    special_cases = {
        '–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–∏': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫—É': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–∫–µ': '–¥–æ—Å—Ç–∞–≤',
        '–¥–æ—Å—Ç–∞–≤—â–∏–∫': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤—â–∏—Ü–∞': '–¥–æ—Å—Ç–∞–≤', '–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å': '–¥–æ—Å—Ç–∞–≤',
        '–ø–µ—Ä–µ–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤', '–ø–æ–¥–¥–æ—Å—Ç–∞–≤–∫–∞': '–¥–æ—Å—Ç–∞–≤',
        '–≤–æ–¥–∏—Ç–µ–ª—å': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª–∏': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏—Ç—å': '–≤–æ–¥–∏',
        '–≤–æ–¥–∏—Ç': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∞': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–æ': '–≤–æ–¥–∏', '–≤–æ–¥–∏–ª–∏': '–≤–æ–¥–∏',
        '–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å': '–∑–∞–∫–∞–∑', '–∑–∞–∫–∞–∑—á–∏–∫': '–∑–∞–∫–∞–∑',
        '–∑–∞–∫–∞–∑—á–∏—Ü–∞': '–∑–∞–∫–∞–∑', '–ø–µ—Ä–µ–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑', '–ø–æ–¥–∑–∞–∫–∞–∑': '–∑–∞–∫–∞–∑',
        '—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ': '—Ü–µ–Ω', '–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω', '–æ—Ü–µ–Ω–∫–∞': '—Ü–µ–Ω',
        '–∫–∞—Ä—Ç–æ—á–∫–∞': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π': '–∫–∞—Ä—Ç', '–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ': '–∫–∞—Ä—Ç',
        '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞': '–±–∞–ª–∞–Ω—Å', '–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å': '–±–∞–ª–∞–Ω—Å',
        '—Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è': '—Ç–∞—Ä–∏—Ñ', '—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å': '—Ç–∞—Ä–∏—Ñ'
    }
    
    if root in special_cases:
        return special_cases[root]
    
    # –†—É—Å—Å–∫–∏–µ –æ–∫–æ–Ω—á–∞–Ω–∏—è (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    russian_endings = [
        '–æ–≤–∞—Ç—å—Å—è', '–∏–≤–∞—Ç—å—Å—è', '–µ–≤–∞—Ç—å—Å—è', '–∞—Ç—å—Å—è', '–∏—Ç—å—Å—è', '–µ—Ç—å—Å—è', '—É—Ç—å—Å—è',
        '–æ–≤–∞–Ω–∏–µ', '–∏—Ä–æ–≤–∞–Ω–∏–µ', '–µ–≤–∞–Ω–∏–µ', '–∞–Ω–∏–µ', '–µ–Ω–∏–µ', '–µ–Ω–∏–µ',
        '—Å–∫–∏–π', '—Å–∫–∞—è', '—Å–∫–æ–µ', '—Å–∫–∏–µ', '—Å–∫–æ–≥–æ', '—Å–∫–æ–π', '—Å–∫—É—é', '—Å–∫–∏–º', '—Å–∫–æ–º', '—Å–∫–∏—Ö', '—Å–∫–∏–º–∏',
        '–æ—Å—Ç—å', '–µ—Å—Ç—å', '—Å—Ç–≤–æ', '—Ç–µ–ª—å', '—Ç–µ–ª—å–Ω–∏—Ü–∞',
        '—ã–π', '–∞—è', '–æ–µ', '—ã–µ', '–æ–≥–æ', '–æ–π', '—É—é', '—ã–º', '–æ–º', '–∏—Ö', '—ã–º–∏',
        '–∏—è', '–∏–π', '–∏–µ', '–∏—é', '–∏–µ–º', '–∏–∏', '–∏—è–º–∏', '–∏—è—Ö',
        '–∏–∫', '–∏—Ü', '–∏—á', '–∏—â', '–Ω–∏–∫', '–Ω–∏—Ü', '—â–∏–∫', '—â–∏—Ü',
        '–∞', '–æ', '—É', '—ã', '–∏', '–µ', '—é', '–µ–º', '–æ–º', '–∞—Ö', '—è–º–∏', '—è—Ö',
        '—Ç—å', '—Ç–∏', '–ª', '–ª–∞', '–ª–æ', '–ª–∏', '–Ω', '–Ω–∞', '–Ω–æ', '–Ω—ã'
    ]
    
    # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏ (–ø–æ –¥–ª–∏–Ω–µ –æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    prefixes = [
        '–ø–µ—Ä–µ', '–ø—Ä–µ–¥', '–ø–æ–¥', '–Ω–∞–¥', '–ø—Ä–∏', '—Ä–∞–∑', '—Ä–∞—Å', '–∏–∑', '–∏—Å',
        '–æ—Ç', '–æ–±', '–≤', '–≤–æ', '–∑–∞', '–Ω–∞', '–¥–æ', '–ø–æ', '—Å–æ', '–≤—ã', '—É'
    ]
    
    # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    for ending in russian_endings:
        if root.endswith(ending) and len(root) > len(ending) + 2:
            root = root[:-len(ending)]
            break
    
    # –£–±–∏—Ä–∞–µ–º –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ (–æ—Ç –¥–ª–∏–Ω–Ω—ã—Ö –∫ –∫–æ—Ä–æ—Ç–∫–∏–º)
    for prefix in prefixes:
        if root.startswith(prefix) and len(root) > len(prefix) + 3:
            root = root[len(prefix):]
            break
    
    # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ—Ä–Ω—è
    return root if len(root) >= 3 else word.lower()

def search_faq_smart(text: str, kb_data: dict):
    """–£–º–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ FAQ —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∫–æ—Ä–Ω–µ–π"""
    if not text or not kb_data:
        return None
    
    # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
    faq_items = kb_data.get("faq", [])
    text_lower = text.lower()
    
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    slang_replacements = {
        '—á–µ': '—á—Ç–æ',
        '—Ç–∞–º': '',
        '–ø–æ': '',
        '–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç': '–∫–∞–∫',
        '—Ä–∞–±–æ—Ç–∞–µ—Ç': '—Ä–∞–±–æ—Ç–∞',
        '–∫–∞–∫': '–∫–∞–∫',
        '–≥–¥–µ': '–≥–¥–µ',
        '—á—Ç–æ': '—á—Ç–æ',
        '–∫–∞–∫–∏–µ': '–∫–∞–∫–∏–µ',
        '–º–æ–∂–Ω–æ': '–º–æ–∂–Ω–æ',
        '–Ω—É–∂–Ω–æ': '–Ω—É–∂–Ω–æ',
        '–µ—Å—Ç—å': '–µ—Å—Ç—å',
        '–±—ã—Ç—å': '–±—ã—Ç—å',
        '–≤—Ä–µ–º—è': '–≤—Ä–µ–º—è',
        '–¥–µ–Ω—å–≥–∏': '–¥–µ–Ω—å–≥–∏',
        '—Ü–µ–Ω–∞': '—Ü–µ–Ω–∞',
        '—Å—Ç–æ–∏–º–æ—Å—Ç—å': '—Å—Ç–æ–∏–º–æ—Å—Ç—å'
    }
    
    # –£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
    text_cleaned = re.sub(r'[^\w\s]', ' ', text_lower)
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    
    processed_text = text_lower
    for slang, replacement in slang_replacements.items():
        processed_text = processed_text.replace(slang, replacement)
    
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    processed_text = ' '.join(processed_text.split())
    
    # TF-IDF –ø–æ–¥—Å—á–µ—Ç –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
    def calculate_tfidf_score(query_words, doc_words):
        """–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç TF-IDF score –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–æ–º –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–º"""
        if not query_words or not doc_words:
            return 0
        
        # –ü–æ–¥—Å—á–µ—Ç –æ–±—â–∏—Ö —Å–ª–æ–≤
        common_words = set(query_words).intersection(set(doc_words))
        if not common_words:
            return 0
        
        # –ü—Ä–æ—Å—Ç–æ–π TF-IDF: log(–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—â–∏—Ö —Å–ª–æ–≤ / –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤)
        tf_score = len(common_words) / len(doc_words)
        idf_score = 1 + (len(common_words) / len(query_words))
        
        return tf_score * idf_score
    
    best_match = None
    best_score = 0
    
    for item in faq_items:
        score = 0
        
        # –£–õ–¨–¢–†–ê-–ì–ò–ë–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —É–º–Ω—ã–º stemming
        keywords = item.get("keywords", [])
        keyword_score = 0
        
        for keyword in keywords:
            keyword_lower = keyword.lower()
            keyword_root = extract_word_root(keyword_lower)
            
            # 1. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–≤—ã—Å—à–∏–π –±–∞–ª–ª)
            if keyword_lower in text_cleaned:
                keyword_score += 6
                continue
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–æ—Ä–Ω—é —Å–ª–æ–≤–∞ (stemming) - –ì–õ–ê–í–ù–´–ô –ê–õ–ì–û–†–ò–¢–ú
            found_by_root = False
            for word in text_cleaned.split():
                word_root = extract_word_root(word)
                
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π
                if keyword_root == word_root:
                    keyword_score += 5
                    found_by_root = True
                    break
                
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–æ—Ä–Ω–µ–π (–º–∏–Ω–∏–º—É–º 4 —Å–∏–º–≤–æ–ª–∞)
                if len(keyword_root) >= 4 and len(word_root) >= 4:
                    min_len = min(len(keyword_root), len(word_root))
                    if keyword_root[:min_len] == word_root[:min_len] and min_len >= 4:
                        keyword_score += 4
                        found_by_root = True
                        break
            
            if found_by_root:
                continue
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º —Ñ–æ—Ä–º–∞–º —Å–ª–æ–≤–∞ (–º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –≤–∞—Ä–∏–∞—Ü–∏–∏)
            keyword_forms = [
                keyword_lower,
                keyword_lower + '–∞', keyword_lower + '–æ', keyword_lower + '—É', keyword_lower + '—ã', keyword_lower + '–∏',
                keyword_lower + '–∞–º', keyword_lower + '–∞–º–∏', keyword_lower + '–∞—Ö',
                keyword_lower + '–æ–π', keyword_lower + '—É—é', keyword_lower + '—ã–º', keyword_lower + '–æ–º',
                keyword_lower + '—ã–π', keyword_lower + '–∞—è', keyword_lower + '–æ–µ', keyword_lower + '—ã–µ',
                keyword_lower + '–æ–≥–æ', keyword_lower + '–æ–π', keyword_lower + '–∏—Ö', keyword_lower + '—ã–º–∏',
                keyword_lower[:-1] if keyword_lower.endswith('–∞') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞'
                keyword_lower[:-1] if keyword_lower.endswith('–æ') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–æ'
                keyword_lower[:-1] if keyword_lower.endswith('–∏') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∏'
                keyword_lower[:-2] if keyword_lower.endswith('—ã–π') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '—ã–π'
                keyword_lower[:-2] if keyword_lower.endswith('–∞—è') else keyword_lower,  # —É–±–∏—Ä–∞–µ–º '–∞—è'
            ]
            
            for form in keyword_forms:
                if form in text_cleaned:
                    keyword_score += 3
                    found_by_root = True
                    break
            
            if found_by_root:
                continue
            
            # 4. –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞)
            if keyword_lower in text_lower or keyword_lower in processed_text:
                keyword_score += 2
                continue
            
            # 5. –û–±—Ä–∞—Ç–Ω–æ–µ —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å –≤ –∫–ª—é—á–µ–≤–æ–º —Å–ª–æ–≤–µ)
            for word in text_cleaned.split():
                if len(word) > 3 and word in keyword_lower:
                    keyword_score += 2
                    break
            
            # 6. –§–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–ø–æ—Ö–æ–∂–∏–µ –∑–≤—É–∫–∏)
            phonetic_map = {
                '–∫': ['–≥', '—Ö'],
                '–ø': ['–±'],
                '—Ç': ['–¥'],
                '—Å': ['–∑', '—Ü'],
                '—Ñ': ['–≤'],
                '—à': ['—â', '–∂'],
                '—á': ['—â']
            }
            
            for word in text_cleaned.split():
                word_root = extract_word_root(word)
                if len(word_root) >= 4 and len(keyword_root) >= 4:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –∫–æ—Ä–Ω–µ–π
                    similarity_count = 0
                    min_len = min(len(word_root), len(keyword_root))
                    
                    for i in range(min_len):
                        if word_root[i] == keyword_root[i]:
                            similarity_count += 1
                        elif word_root[i] in phonetic_map.get(keyword_root[i], []):
                            similarity_count += 0.5
                    
                    if similarity_count >= min_len * 0.7:  # 70% —Å—Ö–æ–¥—Å—Ç–≤–∞
                        keyword_score += 1.5
                        break
        
        # –ì–õ–£–ë–û–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏—è–º –≤–æ–ø—Ä–æ—Å–æ–≤
        variations = item.get("question_variations", [])
        variation_score = 0
        query_words = text_cleaned.split()
        
        for variation in variations:
            variation_words = variation.lower().split()
            
            # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
            tfidf_score = calculate_tfidf_score(query_words, variation_words)
            variation_score += tfidf_score
            
            # 2. –ü–æ–∏—Å–∫ –ø–æ –±–∏–≥—Ä–∞–º–º–∞–º (–¥–≤–∞ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            query_bigrams = set()
            for i in range(len(query_words) - 1):
                query_bigrams.add(f"{query_words[i]} {query_words[i+1]}")
            
            variation_bigrams = set()
            for i in range(len(variation_words) - 1):
                variation_bigrams.add(f"{variation_words[i]} {variation_words[i+1]}")
            
            bigram_matches = len(query_bigrams.intersection(variation_bigrams))
            variation_score += bigram_matches * 0.5
            
            # 3. –ü–æ–∏—Å–∫ –ø–æ —Ç—Ä–∏–≥—Ä–∞–º–º–∞–º (—Ç—Ä–∏ —Å–ª–æ–≤–∞ –ø–æ–¥—Ä—è–¥)
            if len(query_words) >= 3 and len(variation_words) >= 3:
                query_trigrams = set()
                for i in range(len(query_words) - 2):
                    query_trigrams.add(f"{query_words[i]} {query_words[i+1]} {query_words[i+2]}")
                
                variation_trigrams = set()
                for i in range(len(variation_words) - 2):
                    variation_trigrams.add(f"{variation_words[i]} {variation_words[i+1]} {variation_words[i+2]}")
                
                trigram_matches = len(query_trigrams.intersection(variation_trigrams))
                variation_score += trigram_matches * 1.0
        
        # –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–ò–ô –ø–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É
        main_question = item.get("question", "").lower()
        main_question_words = main_question.split()
        
        # 1. TF-IDF –±–∞–∑–æ–≤—ã–π –ø–æ–¥—Å—á–µ—Ç
        main_score = calculate_tfidf_score(query_words, main_question_words) * 2
        
        # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
        question_phrases = [
            "–∫–∞–∫", "—á—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞", "–ø–æ—á–µ–º—É", "–∑–∞—á–µ–º",
            "–º–æ–∂–Ω–æ", "–Ω—É–∂–Ω–æ", "–µ—Å—Ç—å", "—Ä–∞–±–æ—Ç–∞–µ—Ç", "–¥–µ–ª–∞—Ç—å"
        ]
        
        for phrase in question_phrases:
            if phrase in text_cleaned and phrase in main_question:
                main_score += 0.5
        
        # 3. –ü–æ–∏—Å–∫ –ø–æ –ø–æ—Ä—è–¥–∫—É —Å–ª–æ–≤ (–≤–∞–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
        query_words_set = set(query_words)
        question_words_set = set(main_question_words)
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ —Å —É—á–µ—Ç–æ–º –ø–æ—Ä—è–¥–∫–∞
        ordered_matches = 0
        for i, word in enumerate(query_words):
            if word in main_question_words:
                # –ò—â–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –≤–æ–ø—Ä–æ—Å–µ
                for j, q_word in enumerate(main_question_words):
                    if word == q_word:
                        # –ë–ª–∏–∑–æ—Å—Ç—å –ø–æ–∑–∏—Ü–∏–π —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –±–∞–ª–ª
                        position_bonus = 1.0 - (abs(i - j) * 0.1)
                        ordered_matches += max(0.1, position_bonus)
                        break
        
        main_score += ordered_matches * 0.3
        
        # 4. –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤
        important_words = ["–¥–æ—Å—Ç–∞–≤–∫–∞", "—Ü–µ–Ω–∞", "—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ç–∞—Ä–∏—Ñ", "–∫–∞—Ä—Ç–∞", "–±–∞–ª–∞–Ω—Å", "–≤–æ–¥–∏—Ç–µ–ª—å", "–∑–∞–∫–∞–∑"]
        for word in important_words:
            if word in text_cleaned and word in main_question:
                main_score += 1.0  # –í—ã—Å–æ–∫–∏–π –±–æ–Ω—É—Å –∑–∞ –≤–∞–∂–Ω—ã–µ —Å–ª–æ–≤–∞
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–ª–ª–æ–≤ –¥–ª—è –Ω–æ–≤–æ–≥–æ –∞–ª–≥–æ—Ä–∏—Ç–º–∞
        max_possible_keywords = len(keywords) * 5  # –ú–∞–∫—Å–∏–º—É–º –±–∞–ª–ª–æ–≤ –∑–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ (–Ω–æ–≤—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        normalized_keyword_score = keyword_score / max_possible_keywords if max_possible_keywords > 0 else 0
        
        # –û–±—â–∏–π –±–∞–ª–ª —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏
        score = (
            normalized_keyword_score * 6.0 +    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ = —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            variation_score * 1.5 +             # –í–∞—Ä–∏–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ (—É–≤–µ–ª–∏—á–µ–Ω–æ)
            main_score * 3.0                    # –û—Å–Ω–æ–≤–Ω–æ–π –≤–æ–ø—Ä–æ—Å (—É–≤–µ–ª–∏—á–µ–Ω–æ)
        )
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if keyword_score > 0 and variation_score > 0 and main_score > 0:
            score *= 1.2  # 20% –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –≤—Å–µ–º —Ç—Ä–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º
        
        if score > best_score:
            best_score = score
            best_match = item
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –±–∞–ª–ª –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–∏–π
    if best_score >= 0.3:  # –ï—â–µ –±–æ–ª—å—à–µ –ø–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥
        return best_match
    
    return None

def test_comprehensive_search():
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π —Ç–µ—Å—Ç —É–º–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
    print("üß™ –§–ò–ù–ê–õ–¨–ù–´–ô –¢–ï–°–¢ –£–ú–ù–û–ì–û –ü–û–ò–°–ö–ê")
    print("=" * 60)
    
    kb_data = load_kb_data()
    if not kb_data:
        return
    
    # –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    test_queries = [
        # –î–æ—Å—Ç–∞–≤–∫–∞ - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "–¥–æ—Å—Ç–∞–≤–∫–∞",
        "–¥–æ—Å—Ç–∞–≤–∫–∏", 
        "–¥–æ—Å—Ç–∞–≤–ª—è—Ç—å",
        "–¥–æ—Å—Ç–∞–≤—â–∏–∫",
        "–ø–µ—Ä–µ–¥–æ—Å—Ç–∞–≤–∫–∞",
        "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É",
        "–≥–¥–µ –∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É",
        "—á–µ —Ç–∞–º –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ",
        "–¥–æ—Å—Ç–∞–≤–∫–∞ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        
        # –¶–µ–Ω–∞ - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "—Ü–µ–Ω–∞",
        "—Ü–µ–Ω—ã",
        "—Ü–µ–Ω—É",
        "—Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ",
        "–ø–µ—Ä–µ–æ—Ü–µ–Ω–∫–∞",
        "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –ø–æ–µ–∑–¥–∫–∞",
        "–∫–∞–∫–∞—è —Ü–µ–Ω–∞ –∑–∞ –∫–∏–ª–æ–º–µ—Ç—Ä",
        "–∫–∞–∫ —Å—á–∏—Ç–∞–µ—Ç—Å—è —Å—Ç–æ–∏–º–æ—Å—Ç—å",
        
        # –ö–∞—Ä—Ç–∞ - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "–∫–∞—Ä—Ç–∞",
        "–∫–∞—Ä—Ç—ã",
        "–∫–∞—Ä—Ç–æ—á–∫–∞",
        "–∫–∞—Ä—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–π",
        "–∫–∞–∫–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è",
        "–∫–∞–∫ –ø—Ä–∏–≤—è–∑–∞—Ç—å –∫–∞—Ä—Ç—É",
        "–∫–∞—Ä—Ç–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç",
        
        # –ë–∞–ª–∞–Ω—Å - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "–±–∞–ª–∞–Ω—Å",
        "–±–∞–ª–∞–Ω—Å—ã",
        "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞",
        "–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å",
        "–∫–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å",
        "–≥–¥–µ –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –±–∞–ª–∞–Ω—Å",
        "–±–∞–ª–∞–Ω—Å –∫–∞—Ä—Ç—ã",
        
        # –¢–∞—Ä–∏—Ñ - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "—Ç–∞—Ä–∏—Ñ",
        "—Ç–∞—Ä–∏—Ñ—ã",
        "—Ç–∞—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è",
        "—Ç–∞—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –∫–æ–º—Ñ–æ—Ä—Ç —Ç–∞—Ä–∏—Ñ",
        "–∫–∞–∫–∏–µ –µ—Å—Ç—å —Ç–∞—Ä–∏—Ñ—ã",
        "—Ç–∞—Ä–∏—Ñ –∫–æ–º—Ñ–æ—Ä—Ç",
        
        # –í–æ–¥–∏—Ç–µ–ª—å - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "–≤–æ–¥–∏—Ç–µ–ª—å",
        "–≤–æ–¥–∏—Ç–µ–ª–∏",
        "–≤–æ–¥–∏—Ç–µ–ª—å–Ω–∏—Ü–∞",
        "–≤–æ–¥–∏—Ç—å",
        "–≥–¥–µ –º–æ–π –≤–æ–¥–∏—Ç–µ–ª—å",
        "–≤–æ–¥–∏—Ç–µ–ª—å –Ω–µ –ø—Ä–∏–µ—Ö–∞–ª",
        "–∫–∞–∫ —Å–≤—è–∑–∞—Ç—å—Å—è —Å –≤–æ–¥–∏—Ç–µ–ª–µ–º",
        
        # –ó–∞–∫–∞–∑ - —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        "–∑–∞–∫–∞–∑",
        "–∑–∞–∫–∞–∑—ã",
        "–∑–∞–∫–∞–∑—ã–≤–∞—Ç—å",
        "–∑–∞–∫–∞–∑—á–∏–∫",
        "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å –∑–∞–∫–∞–∑",
        "–≥–¥–µ –º–æ–π –∑–∞–∫–∞–∑",
        "–∑–∞–∫–∞–∑ –Ω–µ –ø—Ä–∏—à–µ–ª",
        
        # –°–ª–æ–∂–Ω—ã–µ –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
        "–∫–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å –∫–∞—Ä—Ç—ã",
        "–≥–¥–µ –≤—ã–±—Ä–∞—Ç—å —Ç–∏–ø –∑–∞–∫–∞–∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∞",
        "—Å–∫–æ–ª—å–∫–æ —Å—Ç–æ–∏—Ç –∫–æ–º—Ñ–æ—Ä—Ç –∫–ª–∞—Å—Å",
        "–º–æ–∂–Ω–æ –ª–∏ —Å–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥–∑–∞–∫–∞–∑",
        "—á—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞ –∑–∞ —Ç–∞—Ä–∏—Ñ",
        "–≤–æ–¥–∏—Ç–µ–ª—å –Ω–µ –ø—Ä–∏–µ—Ö–∞–ª –Ω–∞ –∑–∞–∫–∞–∑",
        "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –∫–∞—Ä—Ç–æ–π",
        "–∫–∞–∫–∏–µ –∫–∞—Ä—Ç—ã –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –¥–ª—è –æ–ø–ª–∞—Ç—ã",
        "–∫–∞–∫ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞–∫–∞–∑ –¥–æ—Å—Ç–∞–≤–∫–∏",
        "–≥–¥–µ –≤–≤–æ–¥–∏—Ç—å –ø—Ä–æ–º–æ–∫–æ–¥ –Ω–∞ —Å–∫–∏–¥–∫—É"
    ]
    
    print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º {len(test_queries)} –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤:")
    print()
    
    success_count = 0
    detailed_results = []
    
    for i, query in enumerate(test_queries, 1):
        print(f"{i:2d}. –ó–∞–ø—Ä–æ—Å: '{query}'")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —É–º–Ω—ã–π –ø–æ–∏—Å–∫
        result = search_faq_smart(query, kb_data)
        
        if result:
            question = result.get('question', '')
            keywords = result.get('keywords', [])
            print(f"    ‚úÖ –ù–ê–ô–î–ï–ù–û!")
            print(f"    üìã Question: {question[:60]}...")
            print(f"    üîë Keywords: {keywords[:3]}...")
            success_count += 1
            detailed_results.append({
                'query': query,
                'found': True,
                'question': question,
                'keywords': keywords
            })
        else:
            print(f"    ‚ùå –ù–ï –ù–ê–ô–î–ï–ù–û")
            detailed_results.append({
                'query': query,
                'found': False,
                'question': None,
                'keywords': None
            })
        
        print()
    
    success_rate = (success_count / len(test_queries)) * 100
    print("üìä –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"–£—Å–ø–µ—à–Ω–æ –Ω–∞–π–¥–µ–Ω–æ: {success_count}/{len(test_queries)} ({success_rate:.1f}%)")
    
    if success_rate >= 95:
        print("üéâ –ü–†–ï–í–û–°–•–û–î–ù–û! –£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–¥–µ–∞–ª—å–Ω–æ!")
    elif success_rate >= 90:
        print("üöÄ –û–¢–õ–ò–ß–ù–û! –û—á–µ–Ω—å –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞!")
    elif success_rate >= 80:
        print("‚úÖ –•–û–†–û–®–û! –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è!")
    elif success_rate >= 70:
        print("‚ö†Ô∏è –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û! –ù—É–∂–Ω—ã –Ω–µ–±–æ–ª—å—à–∏–µ —É–ª—É—á—à–µ–Ω–∏—è!")
    else:
        print("‚ùå –ù—É–∂–Ω—ã —Å–µ—Ä—å–µ–∑–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è!")
    
    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    print("\nüìà –ê–ù–ê–õ–ò–ó –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
    categories = {
        '–¥–æ—Å—Ç–∞–≤–∫–∞': [r for r in detailed_results if '–¥–æ—Å—Ç–∞–≤' in r['query']],
        '—Ü–µ–Ω–∞': [r for r in detailed_results if any(word in r['query'] for word in ['—Ü–µ–Ω–∞', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ç–∞—Ä–∏—Ñ'])],
        '–∫–∞—Ä—Ç–∞': [r for r in detailed_results if '–∫–∞—Ä—Ç' in r['query']],
        '–±–∞–ª–∞–Ω—Å': [r for r in detailed_results if '–±–∞–ª–∞–Ω—Å' in r['query']],
        '–≤–æ–¥–∏—Ç–µ–ª—å': [r for r in detailed_results if '–≤–æ–¥–∏—Ç–µ–ª' in r['query']],
        '–∑–∞–∫–∞–∑': [r for r in detailed_results if '–∑–∞–∫–∞–∑' in r['query']],
        '—Å–ª–æ–∂–Ω—ã–µ': [r for r in detailed_results if len(r['query'].split()) > 3]
    }
    
    for category, results in categories.items():
        if results:
            found = sum(1 for r in results if r['found'])
            rate = (found / len(results)) * 100
            print(f"  {category.capitalize()}: {found}/{len(results)} ({rate:.1f}%)")
    
    return detailed_results

if __name__ == "__main__":
    test_comprehensive_search()
