#!/usr/bin/env python3
"""
üöÄ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø LLM –ú–û–î–ï–õ–¨ APARU
–ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
"""

import requests
import json
import logging
from typing import Dict, Any, Optional
from datetime import datetime

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizedLLMClient:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.model_name = "aparu-senior-ai"
        self.ollama_available = False
        self._check_ollama()
        
        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.optimized_params = {
            "temperature": 0.3,  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            "num_predict": 200,  # –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (–±—ã—Å—Ç—Ä–µ–µ)
            "num_ctx": 512,      # –ú–µ–Ω—å—à–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            "repeat_penalty": 1.1,
            "top_k": 20,         # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –≤—ã–±–æ—Ä
            "top_p": 0.9,        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
            "stop": ["\n\n", "–í–æ–ø—Ä–æ—Å:", "–û—Ç–≤–µ—Ç:"]  # –°—Ç–æ–ø-—Å–ª–æ–≤–∞
        }
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –±—ã—Å—Ç—Ä—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        self.system_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏ APARU. 
–û—Ç–≤–µ—á–∞–π –ö–†–ê–¢–ö–û –∏ –¢–û–ß–ù–û –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ:
- –ù–∞—Ü–µ–Ω–∫–∞—Ö –∏ —Ç–∞—Ä–∏—Ñ–∞—Ö
- –î–æ—Å—Ç–∞–≤–∫–µ –∏ –∫—É—Ä—å–µ—Ä—Å–∫–∏—Ö —É—Å–ª—É–≥–∞—Ö  
- –ë–∞–ª–∞–Ω—Å–µ –∏ –ø–ª–∞—Ç–µ–∂–∞—Ö
- –ü—Ä–æ–±–ª–µ–º–∞—Ö —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º

–ü—Ä–∞–≤–∏–ª–∞:
1. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –ø–æ —Å—É—â–µ—Å—Ç–≤—É
2. –ú–∞–∫—Å–∏–º—É–º 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞
4. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å - —Å–∫–∞–∂–∏ "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É"

–ü—Ä–∏–º–µ—Ä—ã —Ö–æ—Ä–æ—à–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤:
- "–ù–∞—Ü–µ–Ω–∫–∞ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç–∞ –∑–∞ –≤—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å"
- "–î–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏: –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ‚Üí –î–æ—Å—Ç–∞–≤–∫–∞ ‚Üí –∞–¥—Ä–µ—Å–∞ ‚Üí –∑–∞–∫–∞–∑"
- "–ü–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å: –ü—Ä–æ—Ñ–∏–ª—å ‚Üí –ü–æ–ø–æ–ª–Ω–∏—Ç—å ‚Üí —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã"
- "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç? –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –∏ –æ–±–Ω–æ–≤–∏—Ç–µ" """

    def _check_ollama(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get("models", [])
                if any(m["name"].startswith(self.model_name) for m in models):
                    self.ollama_available = True
                    logger.info(f"‚úÖ Ollama –∏ –º–æ–¥–µ–ª—å '{self.model_name}' –¥–æ—Å—Ç—É–ø–Ω—ã")
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å '{self.model_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            else:
                logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")

    def get_fast_llm_response(self, question: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM –º–æ–¥–µ–ª–∏"""
        if not self.ollama_available:
            logger.error("‚ùå Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return None

        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            prompt = f"{self.system_prompt}\n\n–í–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç:"
            
            payload = {
                "model": self.model_name,
                "prompt": prompt,
                "stream": False,
                "options": self.optimized_params
            }
            
            logger.info(f"üß† –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –±—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM...")
            start_time = datetime.now()
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=45  # –£–º–µ–Ω—å—à–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–æ 45 —Å–µ–∫—É–Ω–¥
            )
            
            processing_time = (datetime.now() - start_time).total_seconds()
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', '').strip()
                
                if answer:
                    logger.info(f"‚úÖ LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {processing_time:.2f}—Å")
                    return {
                        "answer": answer,
                        "category": "llm_generated",
                        "confidence": 0.9,
                        "source": "optimized_llm",
                        "processing_time": processing_time
                    }
            
            logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {response.status_code}")
            return None
            
        except requests.exceptions.Timeout:
            logger.error(f"‚ùå LLM —Ç–∞–π–º–∞—É—Ç (>45—Å)")
            return None
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
            return None

    def get_hybrid_response(self, question: str) -> Dict[str, Any]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –æ—Ç–≤–µ—Ç: LLM + fallback"""
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å LLM –æ—Ç–≤–µ—Ç
        llm_result = self.get_fast_llm_response(question)
        
        if llm_result:
            return llm_result
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
        logger.info("üîÑ Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É...")
        return self._simple_search(question)
    
    def _simple_search(self, question: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (fallback)"""
        question_lower = question.lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
        simple_kb = {
            "–Ω–∞—Ü–µ–Ω–∫–∞": "–ù–∞—Ü–µ–Ω–∫–∞ - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç–∞ –∑–∞ –≤—ã—Å–æ–∫–∏–π —Å–ø—Ä–æ—Å. –ü–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏–≤–ª–µ—á—å –±–æ–ª—å—à–µ –≤–æ–¥–∏—Ç–µ–ª–µ–π.",
            "–¥–æ—Å—Ç–∞–≤–∫–∞": "–î–ª—è –¥–æ—Å—Ç–∞–≤–∫–∏: –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ‚Üí –î–æ—Å—Ç–∞–≤–∫–∞ ‚Üí —É–∫–∞–∂–∏—Ç–µ –∞–¥—Ä–µ—Å–∞ ‚Üí –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –∑–∞–∫–∞–∑.",
            "–±–∞–ª–∞–Ω—Å": "–ü–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å: –ü—Ä–æ—Ñ–∏–ª—å ‚Üí –ü–æ–ø–æ–ª–Ω–∏—Ç—å ‚Üí –≤—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã.",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç? –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –æ–±–Ω–æ–≤–∏—Ç–µ –≤–µ—Ä—Å–∏—é."
        }
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword, answer in simple_kb.items():
            if keyword in question_lower:
                return {
                    "answer": answer,
                    "category": keyword,
                    "confidence": 0.8,
                    "source": "simple_search",
                    "processing_time": 0.1
                }
        
        # Fallback
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
            "category": "unknown",
            "confidence": 0.0,
            "source": "fallback",
            "processing_time": 0.1
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
optimized_llm_client = OptimizedLLMClient()

def test_optimized_llm():
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é LLM –º–æ–¥–µ–ª—å"""
    print("üß™ –¢–ï–°–¢–ò–†–£–Æ –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–£–Æ LLM –ú–û–î–ï–õ–¨")
    print("=" * 50)
    
    test_questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –Ω–∞—Ü–µ–Ω–∫–∞?",
        "–ö–∞–∫ –∑–∞–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç–∞–≤–∫—É?",
        "–ö–∞–∫ –ø–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å?",
        "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"
    ]
    
    for i, question in enumerate(test_questions, 1):
        print(f"\nüìù –í–æ–ø—Ä–æ—Å {i}: {question}")
        
        result = optimized_llm_client.get_hybrid_response(question)
        
        print(f"‚úÖ –û—Ç–≤–µ—Ç: {result['answer']}")
        print(f"üìä –ò—Å—Ç–æ—á–Ω–∏–∫: {result['source']}")
        print(f"‚è±Ô∏è –í—Ä–µ–º—è: {result['processing_time']:.2f}—Å")
        print(f"üéØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f}")

if __name__ == "__main__":
    test_optimized_llm()
