#!/usr/bin/env python3
"""
üöÄ –õ–û–ö–ê–õ–¨–ù–´–ô –°–ï–†–í–ï–† –° LLM –ú–û–î–ï–õ–¨–Æ
–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–∞–ø—Ä–æ—Å—ã –æ—Ç Railway –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Ö —á–µ—Ä–µ–∑ LLM
"""

import json
import re
import logging
import requests
from typing import Dict, Any, List, Optional
from datetime import datetime
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="APARU Local LLM Server", version="3.1.0")

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ú–æ–¥–µ–ª–∏
class ChatRequest(BaseModel):
    text: str
    user_id: str
    locale: str = "ru"

class ChatResponse(BaseModel):
    response: str
    intent: str
    confidence: float
    source: str
    timestamp: str
    suggestions: List[str] = []

class HealthResponse(BaseModel):
    status: str
    architecture: str = "local_llm"
    timestamp: str

class LocalLLMClient:
    def __init__(self):
        self.ollama_url = "http://localhost:11434"
        self.model_name = "aparu-senior-ai"
        self.llm_available = False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama
        self._check_ollama()
        
    def _check_ollama(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                self.llm_available = True
                logger.info("‚úÖ Ollama –¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                logger.warning("‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama: {e}")
    
    def get_answer(self, question: str) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM –º–æ–¥–µ–ª–∏"""
        start_time = datetime.now()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM –º–æ–¥–µ–ª—å
        if self.llm_available:
            try:
                logger.info("üß† –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç LLM –º–æ–¥–µ–ª–∏...")
                response = self._query_llm(question)
                if response and response.get('response'):
                    processing_time = (datetime.now() - start_time).total_seconds()
                    logger.info(f"‚úÖ LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {processing_time:.2f}—Å")
                    return response
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ LLM: {e}")
        
        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É
        logger.info("üîÑ Fallback –∫ –ø—Ä–æ—Å—Ç–æ–º—É –ø–æ–∏—Å–∫—É...")
        return self._simple_search(question)
    
    def _query_llm(self, question: str) -> Dict[str, Any]:
        """–ó–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM –º–æ–¥–µ–ª–∏"""
        try:
            # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è APARU
            system_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ —Ç–∞–∫—Å–∏-–∞–≥—Ä–µ–≥–∞—Ç–æ—Ä–∞ APARU. 
–û—Ç–≤–µ—á–∞–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –æ:
- –ù–∞—Ü–µ–Ω–∫–∞—Ö –∏ —Ç–∞—Ä–∏—Ñ–∞—Ö
- –î–æ—Å—Ç–∞–≤–∫–µ –∏ –∫—É—Ä—å–µ—Ä—Å–∫–∏—Ö —É—Å–ª—É–≥–∞—Ö  
- –ë–∞–ª–∞–Ω—Å–µ –∏ –ø–ª–∞—Ç–µ–∂–∞—Ö
- –ü—Ä–æ–±–ª–µ–º–∞—Ö —Å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ–º

–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –≤–µ–∂–ª–∏–≤–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç, –ø—Ä–µ–¥–ª–æ–∂–∏ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏."""
            
            payload = {
                "model": self.model_name,
                "prompt": f"{system_prompt}\n\n–í–æ–ø—Ä–æ—Å: {question}",
                "stream": False,
                "options": {
                    "temperature": 0.7,
                    "max_tokens": 500
                }
            }
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=120
            )
            
            if response.status_code == 200:
                data = response.json()
                answer = data.get('response', '').strip()
                
                if answer:
                    return {
                        "answer": answer,
                        "category": "llm_generated",
                        "confidence": 0.9,
                        "source": "local_llm"
                    }
            
            logger.warning(f"‚ö†Ô∏è LLM –≤–µ—Ä–Ω—É–ª –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç: {response.status_code}")
            return None
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM: {e}")
            return None
    
    def _simple_search(self, question: str) -> Dict[str, Any]:
        """–ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (fallback)"""
        question_lower = question.lower()
        
        # –ü—Ä–æ—Å—Ç–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
        simple_kb = {
            "–Ω–∞—Ü–µ–Ω–∫–∞": "–ù–∞—Ü–µ–Ω–∫–∞ - —ç—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–ª–∞—Ç–∞ –∑–∞ –ø–æ–≤—ã—à–µ–Ω–Ω—ã–π —Å–ø—Ä–æ—Å. –û–Ω–∞ –ø–æ–º–æ–≥–∞–µ—Ç –ø—Ä–∏–≤–ª–µ—á—å –±–æ–ª—å—à–µ –≤–æ–¥–∏—Ç–µ–ª–µ–π –∏ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è.",
            "–¥–æ—Å—Ç–∞–≤–∫–∞": "–î–ª—è –∑–∞–∫–∞–∑–∞ –¥–æ—Å—Ç–∞–≤–∫–∏: –æ—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ‚Üí –≤—ã–±–µ—Ä–∏—Ç–µ '–î–æ—Å—Ç–∞–≤–∫–∞' ‚Üí —É–∫–∞–∂–∏—Ç–µ –∞–¥—Ä–µ—Å–∞ ‚Üí –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –∑–∞–∫–∞–∑.",
            "–±–∞–ª–∞–Ω—Å": "–î–ª—è –ø–æ–ø–æ–ª–Ω–µ–Ω–∏—è –±–∞–ª–∞–Ω—Å–∞: –æ—Ç–∫—Ä–æ–π—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ ‚Üí '–ü—Ä–æ—Ñ–∏–ª—å' ‚Üí '–ü–æ–ø–æ–ª–Ω–∏—Ç—å –±–∞–ª–∞–Ω—Å' ‚Üí –≤—ã–±–µ—Ä–∏—Ç–µ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã.",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ": "–ï—Å–ª–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç: –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç, –æ–±–Ω–æ–≤–∏—Ç–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏."
        }
        
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
        for keyword, answer in simple_kb.items():
            if keyword in question_lower:
                return {
                    "answer": answer,
                    "category": keyword,
                    "confidence": 0.8,
                    "source": "simple_search"
                }
        
        # Fallback
        return {
            "answer": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å. –û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏.",
            "category": "unknown",
            "confidence": 0.0,
            "source": "fallback"
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
local_llm_client = LocalLLMClient()

@app.get("/")
async def root():
    return {
        "message": "APARU Local LLM Server", 
        "status": "running", 
        "version": "3.1.0",
        "architecture": "local_llm",
        "llm_available": local_llm_client.llm_available
    }

@app.get("/health", response_model=HealthResponse)
async def health():
    return HealthResponse(
        status="healthy",
        architecture="local_llm",
        timestamp=datetime.now().isoformat()
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest):
    """–û—Å–Ω–æ–≤–Ω–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è —á–∞—Ç–∞"""
    try:
        result = local_llm_client.get_answer(request.text)
        
        return ChatResponse(
            response=result["answer"],
            intent=result["category"],
            confidence=result["confidence"],
            source=result["source"],
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )
    
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ /chat: {e}")
        return ChatResponse(
            response="–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.",
            intent="error",
            confidence=0.0,
            source="error",
            timestamp=datetime.now().isoformat(),
            suggestions=[]
        )

if __name__ == "__main__":
    import uvicorn
    
    # –õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –Ω–∞ –ø–æ—Ä—Ç—É 8001
    port = int(os.environ.get("LOCAL_PORT", 8001))
    uvicorn.run(app, host="0.0.0.0", port=port)